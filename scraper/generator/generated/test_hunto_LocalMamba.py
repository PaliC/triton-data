import sys
_module = sys.modules[__name__]
del sys
causal_conv1d = _module
causal_conv1d_interface = _module
setup = _module
test_causal_conv1d = _module
lib = _module
dataset = _module
augment_ops = _module
builder = _module
dataloader = _module
file_io = _module
mixup = _module
transform = _module
models = _module
ShuffleNetv1 = _module
ShuffleNetv2 = _module
cifar = _module
classifier = _module
mobilenetv2 = _module
resnet = _module
resnetv2 = _module
util = _module
vgg = _module
wrn = _module
darts_model = _module
lightvit = _module
local_vim = _module
local_vmamba = _module
losses = _module
cross_entropy = _module
diffkd = _module
diffkd_modules = _module
scheduling_ddim = _module
dist_kd = _module
kd_loss = _module
kl_div = _module
mamba = _module
local_scan = _module
multi_mamba = _module
rope = _module
mdconv = _module
mobilenet_v1 = _module
nas_model = _module
operations = _module
operations_resnet = _module
utils = _module
dbb = _module
dbb_block = _module
dbb_transforms = _module
dbb_converter = _module
dyrep = _module
recal_bn = _module
args = _module
dist_utils = _module
gen_network = _module
measure = _module
misc = _module
model_ema = _module
optim = _module
scheduler = _module
convert = _module
convert_ckpt = _module
speed_test = _module
test = _module
train = _module
vis_search_prob = _module
ade20k_instance = _module
ade20k_panoptic = _module
ade20k_semantic = _module
cityscapes_detection = _module
cityscapes_instance = _module
coco_caption = _module
coco_detection = _module
coco_instance = _module
coco_instance_semantic = _module
coco_panoptic = _module
coco_semantic = _module
deepfashion = _module
dsdl = _module
isaid_instance = _module
lvis_v1_instance = _module
mot_challenge = _module
mot_challenge_det = _module
mot_challenge_reid = _module
objects365v1_detection = _module
objects365v2_detection = _module
openimages_detection = _module
refcoco = _module
refcocog = _module
semi_coco_detection = _module
v3det = _module
voc0712 = _module
wider_face = _module
youtube_vis = _module
default_runtime = _module
retinanet_r50_fpn = _module
rpn_r50_fpn = _module
ssd300 = _module
schedule_1x = _module
schedule_20e = _module
schedule_2x = _module
mask_rcnn_local_vssm_small_fpn_coco = _module
mask_rcnn_local_vssm_small_fpn_coco_ms_3x = _module
mask_rcnn_local_vssm_tiny_fpn_coco = _module
mask_rcnn_local_vssm_tiny_fpn_coco_ms_3x = _module
model = _module
analyze_logs = _module
analyze_results = _module
benchmark = _module
browse_dataset = _module
coco_error_analysis = _module
coco_occluded_separated_recall = _module
confusion_matrix = _module
eval_metric = _module
fuse_results = _module
get_flops = _module
mamba_get_flops = _module
mot_error_visualize = _module
mot_param_search = _module
optimize_anchors = _module
robustness_eval = _module
test_robustness = _module
ade20k2coco = _module
cityscapes = _module
coco_stuff164k = _module
crowdhuman2coco = _module
images2coco = _module
mot2coco = _module
mot2reid = _module
pascal_voc = _module
prepare_coco_semantic_annos_from_panoptic_annos = _module
youtubevis2coco = _module
mmdet2torchserve = _module
mmdet_handler = _module
test_torchserver = _module
download_dataset = _module
gen_coco_panoptic_test_info = _module
get_crowdhuman_id_hw = _module
get_image_metas = _module
print_config = _module
split_coco = _module
detectron2_to_mmdet = _module
detectron2pytorch = _module
detic_to_mmdet = _module
glip_to_mmdet = _module
groundingdino_to_mmdet = _module
publish_model = _module
regnet2mmdet = _module
selfsup2mmdet = _module
swinv1_to_mmdet = _module
upgrade_model_version = _module
upgrade_ssd_version = _module
test_tracking = _module
vitdet = _module
fp16_compression_hook = _module
layer_decay_optimizer_constructor = _module
simple_fpn = _module
benchmark_generation_mamba_simple = _module
lm_harness_eval = _module
mamba_ssm = _module
config_mamba = _module
mixer_seq_simple = _module
modules = _module
mamba_simple = _module
ops = _module
selective_scan_interface = _module
triton = _module
layernorm = _module
selective_state_update = _module
generation = _module
hf = _module
setup = _module
test_selective_scan = _module
test_selective_state_update = _module
ade20k = _module
ade20k_640x640 = _module
bdd100k = _module
chase_db1 = _module
cityscapes_1024x1024 = _module
cityscapes_768x768 = _module
cityscapes_769x769 = _module
cityscapes_832x832 = _module
drive = _module
hrf = _module
isaid = _module
levir_256x256 = _module
loveda = _module
mapillary_v1 = _module
mapillary_v1_65 = _module
mapillary_v2 = _module
nyu = _module
nyu_512x512 = _module
pascal_context = _module
pascal_context_59 = _module
pascal_voc12 = _module
pascal_voc12_aug = _module
potsdam = _module
refuge = _module
stare = _module
synapse = _module
vaihingen = _module
bisenetv2 = _module
cgnet = _module
erfnet_fcn = _module
fast_scnn = _module
fcn_hr18 = _module
fpn_poolformer_s12 = _module
fpn_r50 = _module
ocrnet_hr18 = _module
pointrend_r50 = _module
setr_mla = _module
setr_naive = _module
setr_pup = _module
stdc = _module
upernet_beit = _module
upernet_convnext = _module
upernet_mae = _module
upernet_r50 = _module
upernet_swin = _module
vpd_sd = _module
schedule_160k = _module
schedule_20k = _module
schedule_240k = _module
schedule_25k = _module
schedule_320k = _module
schedule_40k = _module
schedule_80k = _module
visualization_cam = _module
coco_stuff10k = _module
levircd = _module
voc_aug = _module
pytorch2torchscript = _module
beit2mmseg = _module
clip2mmseg = _module
mit2mmseg = _module
san2mmseg = _module
stdc2mmseg = _module
swin2mmseg = _module
twins2mmseg = _module
vit2mmseg = _module
vitjax2mmseg = _module
mmseg2torchserve = _module
mmseg_handler = _module
test_torchserve = _module

from _paritybench_helpers import _mock_config, patch_functional
from unittest.mock import mock_open, MagicMock
from torch.autograd import Function
from torch.nn import Module
import abc, collections, copy, enum, functools, inspect, itertools, logging, math, matplotlib, numbers, numpy, pandas, queue, random, re, scipy, sklearn, string, time, torch, torchaudio, torchvision, triton, types, typing, uuid, warnings
import operator as op
from dataclasses import dataclass
import numpy as np
from torch import Tensor
patch_functional()
open = mock_open()
yaml = logging = sys = argparse = MagicMock()
ArgumentParser = argparse.ArgumentParser
_global_config = args = argv = cfg = config = params = _mock_config()
argparse.ArgumentParser.return_value.parse_args.return_value = _global_config
yaml.load.return_value = _global_config
sys.argv = _global_config
__version__ = '1.0.0'
xrange = range
wraps = functools.wraps


import torch


import torch.nn as nn


from functools import partial


from torch import Tensor


from typing import Optional


import math


import torch.nn.functional as F


import triton


import triton.language as tl


from collections import namedtuple


from torch.cuda.amp import custom_fwd


from torch.cuda.amp import custom_bwd


import warnings


import re


from torch.utils.cpp_extension import BuildExtension


from torch.utils.cpp_extension import CppExtension


from torch.utils.cpp_extension import CUDAExtension


from torch.utils.cpp_extension import CUDA_HOME


@triton.jit
def triton_local_scan(x, y, K: 'tl.constexpr', flip: 'tl.constexpr', BC: 'tl.constexpr', BH: 'tl.constexpr', BW: 'tl.constexpr', DC: 'tl.constexpr', DH: 'tl.constexpr', DW: 'tl.constexpr', NH: 'tl.constexpr', NW: 'tl.constexpr'):
    i_hw, i_c, i_b = tl.program_id(0), tl.program_id(1), tl.program_id(2)
    i_h, i_w = i_hw // NW, i_hw % NW
    _mask_h = i_h * BH + tl.arange(0, BH) < DH
    _mask_w = i_w * BW + tl.arange(0, BW) < DW
    _mask_hw = _mask_h[:, None] & _mask_w[None, :]
    _for_C = min(DC - i_c * BC, BC)
    _tmp0 = i_c * BC * DH * DW
    _tmp1 = DC * DH * DW
    _tmp2 = _tmp0 + i_h * BH * DW + tl.arange(0, BH)[:, None] * DW + i_w * BW + tl.arange(0, BW)[None, :]
    p_x = x + i_b * _tmp1 + _tmp2
    _i = (tl.arange(0, BH) + BH * i_h)[:, None]
    _j = (tl.arange(0, BW) + BW * i_w)[None, :]
    _c_offset = (DW // K * (_i // K) + _j // K) * K * K + _i % K * K + _j % K
    if flip:
        _c_offset = DH * DW - _c_offset - 1
    p_y = y + i_b * _tmp1 + _tmp0 + _c_offset
    for idxc in range(_for_C):
        _idx = idxc * DH * DW
        _x = tl.load(p_x + _idx, mask=_mask_hw)
        tl.store(p_y + _idx, _x, mask=_mask_hw)
    tl.debug_barrier()


@triton.jit
def triton_local_reverse(x, y, K: 'tl.constexpr', flip: 'tl.constexpr', BC: 'tl.constexpr', BH: 'tl.constexpr', BW: 'tl.constexpr', DC: 'tl.constexpr', DH: 'tl.constexpr', DW: 'tl.constexpr', NH: 'tl.constexpr', NW: 'tl.constexpr'):
    i_hw, i_c, i_b = tl.program_id(0), tl.program_id(1), tl.program_id(2)
    i_h, i_w = i_hw // NW, i_hw % NW
    _mask_h = i_h * BH + tl.arange(0, BH) < DH
    _mask_w = i_w * BW + tl.arange(0, BW) < DW
    _mask_hw = _mask_h[:, None] & _mask_w[None, :]
    _for_C = min(DC - i_c * BC, BC)
    _tmp0 = i_c * BC * DH * DW
    _tmp1 = DC * DH * DW
    _tmp2 = _tmp0 + i_h * BH * DW + tl.arange(0, BH)[:, None] * DW + i_w * BW + tl.arange(0, BW)[None, :]
    p_x = x + i_b * _tmp1 + _tmp2
    _i = (tl.arange(0, BH) + BH * i_h)[:, None]
    _j = (tl.arange(0, BW) + BW * i_w)[None, :]
    _o = _i * DW + _j
    _i = _o // (K * K) // (DW // K) * K + _o % (K * K) // K
    _j = _o // (K * K) % (DW // K) * K + _o % (K * K) % K
    _c_offset = _i * DW + _j
    if flip:
        _c_offset = DH * DW - _c_offset - 1
    p_y = y + i_b * _tmp1 + _tmp0 + _c_offset
    for idxc in range(_for_C):
        _idx = idxc * DH * DW
        _x = tl.load(p_x + _idx, mask=_mask_hw)
        tl.store(p_y + _idx, _x, mask=_mask_hw)
    tl.debug_barrier()


@triton.autotune(configs=[triton.Config({}, num_warps=1), triton.Config({}, num_warps=2), triton.Config({}, num_warps=4), triton.Config({}, num_warps=8), triton.Config({}, num_warps=16), triton.Config({}, num_warps=32)], key=['N', 'HAS_RESIDUAL', 'STORE_RESIDUAL_OUT', 'IS_RMS_NORM', 'HAS_BIAS'])
@triton.jit
def _layer_norm_fwd_1pass_kernel(X, Y, W, B, RESIDUAL, RESIDUAL_OUT, Mean, Rstd, stride_x_row, stride_y_row, stride_res_row, stride_res_out_row, N, eps, IS_RMS_NORM: 'tl.constexpr', BLOCK_N: 'tl.constexpr', HAS_RESIDUAL: 'tl.constexpr', STORE_RESIDUAL_OUT: 'tl.constexpr', HAS_BIAS: 'tl.constexpr'):
    row = tl.program_id(0)
    X += row * stride_x_row
    Y += row * stride_y_row
    if HAS_RESIDUAL:
        RESIDUAL += row * stride_res_row
    if STORE_RESIDUAL_OUT:
        RESIDUAL_OUT += row * stride_res_out_row
    cols = tl.arange(0, BLOCK_N)
    x = tl.load(X + cols, mask=cols < N, other=0.0)
    if HAS_RESIDUAL:
        residual = tl.load(RESIDUAL + cols, mask=cols < N, other=0.0)
        x += residual
    if STORE_RESIDUAL_OUT:
        tl.store(RESIDUAL_OUT + cols, x, mask=cols < N)
    if not IS_RMS_NORM:
        mean = tl.sum(x, axis=0) / N
        tl.store(Mean + row, mean)
        xbar = tl.where(cols < N, x - mean, 0.0)
        var = tl.sum(xbar * xbar, axis=0) / N
    else:
        xbar = tl.where(cols < N, x, 0.0)
        var = tl.sum(xbar * xbar, axis=0) / N
    rstd = 1 / tl.sqrt(var + eps)
    tl.store(Rstd + row, rstd)
    mask = cols < N
    w = tl.load(W + cols, mask=mask)
    if HAS_BIAS:
        b = tl.load(B + cols, mask=mask)
    x_hat = (x - mean) * rstd if not IS_RMS_NORM else x * rstd
    y = x_hat * w + b if HAS_BIAS else x_hat * w
    tl.store(Y + cols, y, mask=mask)


@triton.autotune(configs=[triton.Config({}, num_warps=1), triton.Config({}, num_warps=2), triton.Config({}, num_warps=4), triton.Config({}, num_warps=8), triton.Config({}, num_warps=16), triton.Config({}, num_warps=32)], key=['N', 'HAS_DRESIDUAL', 'STORE_DRESIDUAL', 'IS_RMS_NORM', 'HAS_BIAS'])
@triton.heuristics({'RECOMPUTE_OUTPUT': lambda args: args['Y'] is not None})
@triton.jit
def _layer_norm_bwd_kernel(X, W, B, Y, DY, DX, DW, DB, DRESIDUAL, DRESIDUAL_IN, Mean, Rstd, stride_x_row, stride_y_row, stride_dy_row, stride_dx_row, stride_dres_row, stride_dres_in_row, M, N, eps, rows_per_program, IS_RMS_NORM: 'tl.constexpr', BLOCK_N: 'tl.constexpr', HAS_DRESIDUAL: 'tl.constexpr', STORE_DRESIDUAL: 'tl.constexpr', HAS_BIAS: 'tl.constexpr', RECOMPUTE_OUTPUT: 'tl.constexpr'):
    row_block_id = tl.program_id(0)
    row_start = row_block_id * rows_per_program
    cols = tl.arange(0, BLOCK_N)
    mask = cols < N
    X += row_start * stride_x_row
    if HAS_DRESIDUAL:
        DRESIDUAL += row_start * stride_dres_row
    if STORE_DRESIDUAL:
        DRESIDUAL_IN += row_start * stride_dres_in_row
    DY += row_start * stride_dy_row
    DX += row_start * stride_dx_row
    if RECOMPUTE_OUTPUT:
        Y += row_start * stride_y_row
    w = tl.load(W + cols, mask=mask)
    if RECOMPUTE_OUTPUT and HAS_BIAS:
        b = tl.load(B + cols, mask=mask, other=0.0)
    dw = tl.zeros((BLOCK_N,), dtype=tl.float32)
    if HAS_BIAS:
        db = tl.zeros((BLOCK_N,), dtype=tl.float32)
    row_end = min((row_block_id + 1) * rows_per_program, M)
    for row in range(row_start, row_end):
        x = tl.load(X + cols, mask=mask, other=0)
        dy = tl.load(DY + cols, mask=mask, other=0)
        if not IS_RMS_NORM:
            mean = tl.load(Mean + row)
        rstd = tl.load(Rstd + row)
        xhat = (x - mean) * rstd if not IS_RMS_NORM else x * rstd
        xhat = tl.where(mask, xhat, 0.0)
        if RECOMPUTE_OUTPUT:
            y = xhat * w + b if HAS_BIAS else xhat * w
            tl.store(Y + cols, y, mask=mask)
        wdy = w * dy
        dw += dy * xhat
        if HAS_BIAS:
            db += dy
        if not IS_RMS_NORM:
            c1 = tl.sum(xhat * wdy, axis=0) / N
            c2 = tl.sum(wdy, axis=0) / N
            dx = (wdy - (xhat * c1 + c2)) * rstd
        else:
            c1 = tl.sum(xhat * wdy, axis=0) / N
            dx = (wdy - xhat * c1) * rstd
        if HAS_DRESIDUAL:
            dres = tl.load(DRESIDUAL + cols, mask=mask, other=0)
            dx += dres
        if STORE_DRESIDUAL:
            tl.store(DRESIDUAL_IN + cols, dx, mask=mask)
        tl.store(DX + cols, dx, mask=mask)
        X += stride_x_row
        if HAS_DRESIDUAL:
            DRESIDUAL += stride_dres_row
        if STORE_DRESIDUAL:
            DRESIDUAL_IN += stride_dres_in_row
        if RECOMPUTE_OUTPUT:
            Y += stride_y_row
        DY += stride_dy_row
        DX += stride_dx_row
    tl.store(DW + row_block_id * N + cols, dw, mask=mask)
    if HAS_BIAS:
        tl.store(DB + row_block_id * N + cols, db, mask=mask)


@triton.heuristics({'HAS_DT_BIAS': lambda args: args['dt_bias_ptr'] is not None})
@triton.heuristics({'HAS_D': lambda args: args['D_ptr'] is not None})
@triton.heuristics({'HAS_Z': lambda args: args['z_ptr'] is not None})
@triton.heuristics({'BLOCK_SIZE_DSTATE': lambda args: triton.next_power_of_2(args['dstate'])})
@triton.jit
def _selective_scan_update_kernel(state_ptr, x_ptr, dt_ptr, dt_bias_ptr, A_ptr, B_ptr, C_ptr, D_ptr, z_ptr, out_ptr, batch, dim, dstate, stride_state_batch, stride_state_dim, stride_state_dstate, stride_x_batch, stride_x_dim, stride_dt_batch, stride_dt_dim, stride_dt_bias_dim, stride_A_dim, stride_A_dstate, stride_B_batch, stride_B_dstate, stride_C_batch, stride_C_dstate, stride_D_dim, stride_z_batch, stride_z_dim, stride_out_batch, stride_out_dim, DT_SOFTPLUS: 'tl.constexpr', BLOCK_SIZE_M: 'tl.constexpr', HAS_DT_BIAS: 'tl.constexpr', HAS_D: 'tl.constexpr', HAS_Z: 'tl.constexpr', BLOCK_SIZE_DSTATE: 'tl.constexpr'):
    pid_m = tl.program_id(axis=0)
    pid_b = tl.program_id(axis=1)
    state_ptr += pid_b * stride_state_batch
    x_ptr += pid_b * stride_x_batch
    dt_ptr += pid_b * stride_dt_batch
    B_ptr += pid_b * stride_B_batch
    C_ptr += pid_b * stride_C_batch
    if HAS_Z:
        z_ptr += pid_b * stride_z_batch
    out_ptr += pid_b * stride_out_batch
    offs_m = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)
    offs_n = tl.arange(0, BLOCK_SIZE_DSTATE)
    state_ptrs = state_ptr + (offs_m[:, None] * stride_state_dim + offs_n[None, :] * stride_state_dstate)
    x_ptrs = x_ptr + offs_m * stride_x_dim
    dt_ptrs = dt_ptr + offs_m * stride_dt_dim
    if HAS_DT_BIAS:
        dt_bias_ptrs = dt_bias_ptr + offs_m * stride_dt_bias_dim
    A_ptrs = A_ptr + (offs_m[:, None] * stride_A_dim + offs_n[None, :] * stride_A_dstate)
    B_ptrs = B_ptr + offs_n * stride_B_dstate
    C_ptrs = C_ptr + offs_n * stride_C_dstate
    if HAS_D:
        D_ptrs = D_ptr + offs_m * stride_D_dim
    if HAS_Z:
        z_ptrs = z_ptr + offs_m * stride_z_dim
    out_ptrs = out_ptr + offs_m * stride_out_dim
    state = tl.load(state_ptrs, mask=(offs_m[:, None] < dim) & (offs_n[None, :] < dstate), other=0.0)
    x = tl.load(x_ptrs, mask=offs_m < dim, other=0.0)
    dt = tl.load(dt_ptrs, mask=offs_m < dim, other=0.0)
    if HAS_DT_BIAS:
        dt += tl.load(dt_bias_ptrs, mask=offs_m < dim, other=0.0)
    if DT_SOFTPLUS:
        dt = tl.log(1.0 + tl.exp(dt))
    A = tl.load(A_ptrs, mask=(offs_m[:, None] < dim) & (offs_n[None, :] < dstate), other=0.0)
    dA = tl.exp(A * dt[:, None])
    B = tl.load(B_ptrs, mask=offs_n < dstate, other=0.0)
    C = tl.load(C_ptrs, mask=offs_n < dstate, other=0.0)
    if HAS_D:
        D = tl.load(D_ptrs, mask=offs_m < dim, other=0.0)
    if HAS_Z:
        z = tl.load(z_ptrs, mask=offs_m < dim, other=0.0)
    dB = B[None, :] * dt[:, None]
    state = state * dA + dB * x[:, None]
    tl.store(state_ptrs, state, mask=(offs_m[:, None] < dim) & (offs_n[None, :] < dstate))
    out = tl.sum(state * C[None, :], axis=1)
    if HAS_D:
        out += x * D
    if HAS_Z:
        out *= z * tl.sigmoid(z)
    tl.store(out_ptrs, out, mask=offs_m < dim)

