import sys
_module = sys.modules[__name__]
del sys
conf = _module
config = _module
map_fn = _module
train_hf = _module
train_lora_hf = _module
train_qlora_hf = _module
setup = _module
xtuner = _module
apis = _module
datasets = _module
alpaca = _module
arxiv = _module
code_alpaca = _module
colorist = _module
lawyer = _module
medical = _module
moss_003_sft = _module
oasst1 = _module
open_orca = _module
sql = _module
tiny_codes = _module
wizardlm = _module
model = _module
training_args = _module
configs = _module
baichuan2_13b_base_qlora_alpaca_e3 = _module
baichuan2_13b_base_qlora_alpaca_enzh_e3 = _module
baichuan2_13b_base_qlora_alpaca_enzh_oasst1_e3 = _module
baichuan2_13b_base_qlora_alpaca_zh_e3 = _module
baichuan2_13b_base_qlora_arxiv_gentitle_e3 = _module
baichuan2_13b_base_qlora_code_alpaca_e3 = _module
baichuan2_13b_base_qlora_colorist_e5 = _module
baichuan2_13b_base_qlora_lawyer_e3 = _module
baichuan2_13b_base_qlora_oasst1_512_e3 = _module
baichuan2_13b_base_qlora_oasst1_e3 = _module
baichuan2_13b_base_qlora_open_platypus_e3 = _module
baichuan2_13b_base_qlora_sql_e3 = _module
baichuan2_13b_chat_qlora_alpaca_e3 = _module
baichuan2_13b_chat_qlora_alpaca_enzh_e3 = _module
baichuan2_13b_chat_qlora_alpaca_enzh_oasst1_e3 = _module
baichuan2_13b_chat_qlora_alpaca_zh_e3 = _module
baichuan2_13b_chat_qlora_code_alpaca_e3 = _module
baichuan2_13b_chat_qlora_lawyer_e3 = _module
baichuan2_13b_chat_qlora_oasst1_512_e3 = _module
baichuan2_13b_chat_qlora_oasst1_e3 = _module
baichuan2_13b_chat_qlora_open_platypus_e3 = _module
baichuan2_7b_base_qlora_alpaca_e3 = _module
baichuan2_7b_base_qlora_alpaca_enzh_e3 = _module
baichuan2_7b_base_qlora_alpaca_enzh_oasst1_e3 = _module
baichuan2_7b_base_qlora_alpaca_zh_e3 = _module
baichuan2_7b_base_qlora_arxiv_gentitle_e3 = _module
baichuan2_7b_base_qlora_code_alpaca_e3 = _module
baichuan2_7b_base_qlora_colorist_e5 = _module
baichuan2_7b_base_qlora_lawyer_e3 = _module
baichuan2_7b_base_qlora_oasst1_512_e3 = _module
baichuan2_7b_base_qlora_oasst1_e3 = _module
baichuan2_7b_base_qlora_open_platypus_e3 = _module
baichuan2_7b_base_qlora_sql_e3 = _module
baichuan2_7b_chat_qlora_alpaca_e3 = _module
baichuan2_7b_chat_qlora_alpaca_enzh_e3 = _module
baichuan2_7b_chat_qlora_alpaca_enzh_oasst1_e3 = _module
baichuan2_7b_chat_qlora_alpaca_zh_e3 = _module
baichuan2_7b_chat_qlora_code_alpaca_e3 = _module
baichuan2_7b_chat_qlora_lawyer_e3 = _module
baichuan2_7b_chat_qlora_oasst1_512_e3 = _module
baichuan2_7b_chat_qlora_oasst1_e3 = _module
baichuan2_7b_chat_qlora_open_platypus_e3 = _module
baichuan_13b_base_qlora_alpaca_e3 = _module
baichuan_13b_base_qlora_alpaca_enzh_e3 = _module
baichuan_13b_base_qlora_alpaca_enzh_oasst1_e3 = _module
baichuan_13b_base_qlora_alpaca_zh_e3 = _module
baichuan_13b_base_qlora_arxiv_gentitle_e3 = _module
baichuan_13b_base_qlora_code_alpaca_e3 = _module
baichuan_13b_base_qlora_colorist_e5 = _module
baichuan_13b_base_qlora_lawyer_e3 = _module
baichuan_13b_base_qlora_medical_e1 = _module
baichuan_13b_base_qlora_moss_sft_all_e1 = _module
baichuan_13b_base_qlora_moss_sft_all_e2_gpu8 = _module
baichuan_13b_base_qlora_moss_sft_plugins_e1 = _module
baichuan_13b_base_qlora_oasst1_512_e3 = _module
baichuan_13b_base_qlora_oasst1_e3 = _module
baichuan_13b_base_qlora_open_platypus_e3 = _module
baichuan_13b_base_qlora_openorca_e1 = _module
baichuan_13b_base_qlora_sql_e3 = _module
baichuan_13b_base_qlora_tiny_codes_e1 = _module
baichuan_13b_chat_qlora_alpaca_e3 = _module
baichuan_13b_chat_qlora_alpaca_enzh_e3 = _module
baichuan_13b_chat_qlora_alpaca_enzh_oasst1_e3 = _module
baichuan_13b_chat_qlora_alpaca_zh_e3 = _module
baichuan_13b_chat_qlora_arxiv_gentitle_e3 = _module
baichuan_13b_chat_qlora_code_alpaca_e3 = _module
baichuan_13b_chat_qlora_colorist_e5 = _module
baichuan_13b_chat_qlora_lawyer_e3 = _module
baichuan_13b_chat_qlora_medical_e1 = _module
baichuan_13b_chat_qlora_oasst1_512_e3 = _module
baichuan_13b_chat_qlora_oasst1_e3 = _module
baichuan_13b_chat_qlora_open_platypus_e3 = _module
baichuan_13b_chat_qlora_openorca_e1 = _module
baichuan_13b_chat_qlora_sql_e3 = _module
baichuan_13b_chat_qlora_tiny_codes_e1 = _module
baichuan_7b_qlora_alpaca_e3 = _module
baichuan_7b_qlora_alpaca_enzh_e3 = _module
baichuan_7b_qlora_alpaca_enzh_oasst1_e3 = _module
baichuan_7b_qlora_alpaca_zh_e3 = _module
baichuan_7b_qlora_arxiv_gentitle_e3 = _module
baichuan_7b_qlora_code_alpaca_e3 = _module
baichuan_7b_qlora_colorist_e5 = _module
baichuan_7b_qlora_lawyer_e3 = _module
baichuan_7b_qlora_medical_e1 = _module
baichuan_7b_qlora_moss_sft_all_e1 = _module
baichuan_7b_qlora_moss_sft_all_e2_gpu8 = _module
baichuan_7b_qlora_moss_sft_plugins_e1 = _module
baichuan_7b_qlora_oasst1_512_e3 = _module
baichuan_7b_qlora_oasst1_e3 = _module
baichuan_7b_qlora_open_platypus_e3 = _module
baichuan_7b_qlora_openorca_e1 = _module
baichuan_7b_qlora_sql_e3 = _module
baichuan_7b_qlora_tiny_codes_e1 = _module
chatglm2_6b_qlora_alpaca_e3 = _module
chatglm2_6b_qlora_alpaca_enzh_e3 = _module
chatglm2_6b_qlora_alpaca_enzh_oasst1_e3 = _module
chatglm2_6b_qlora_alpaca_zh_e3 = _module
chatglm2_6b_qlora_arxiv_gentitle_e3 = _module
chatglm2_6b_qlora_code_alpaca_e3 = _module
chatglm2_6b_qlora_colorist_e5 = _module
chatglm2_6b_qlora_lawyer_e3 = _module
chatglm2_6b_qlora_medical_e1 = _module
chatglm2_6b_qlora_oasst1_512_e3 = _module
chatglm2_6b_qlora_oasst1_e3 = _module
chatglm2_6b_qlora_open_platypus_e3 = _module
chatglm2_6b_qlora_openorca_e1 = _module
chatglm2_6b_qlora_sql_e3 = _module
chatglm2_6b_qlora_tiny_codes_e1 = _module
chatglm3_6b_qlora_alpaca_e3 = _module
chatglm3_6b_qlora_alpaca_enzh_e3 = _module
chatglm3_6b_qlora_alpaca_enzh_oasst1_e3 = _module
chatglm3_6b_qlora_alpaca_zh_e3 = _module
chatglm3_6b_qlora_arxiv_gentitle_e3 = _module
chatglm3_6b_qlora_code_alpaca_e3 = _module
chatglm3_6b_qlora_colorist_e5 = _module
chatglm3_6b_qlora_lawyer_e3 = _module
chatglm3_6b_qlora_medical_e1 = _module
chatglm3_6b_qlora_oasst1_512_e3 = _module
chatglm3_6b_qlora_oasst1_e3 = _module
chatglm3_6b_qlora_open_platypus_e3 = _module
chatglm3_6b_qlora_openorca_e1 = _module
chatglm3_6b_qlora_sql_e3 = _module
chatglm3_6b_qlora_tiny_codes_e1 = _module
chatglm3_6b_base_qlora_alpaca_e3 = _module
chatglm3_6b_base_qlora_alpaca_enzh_e3 = _module
chatglm3_6b_base_qlora_alpaca_enzh_oasst1_e3 = _module
chatglm3_6b_base_qlora_alpaca_zh_e3 = _module
chatglm3_6b_base_qlora_arxiv_gentitle_e3 = _module
chatglm3_6b_base_qlora_code_alpaca_e3 = _module
chatglm3_6b_base_qlora_colorist_e5 = _module
chatglm3_6b_base_qlora_lawyer_e3 = _module
chatglm3_6b_base_qlora_medical_e1 = _module
chatglm3_6b_base_qlora_oasst1_512_e3 = _module
chatglm3_6b_base_qlora_oasst1_e3 = _module
chatglm3_6b_base_qlora_open_platypus_e3 = _module
chatglm3_6b_base_qlora_openorca_e1 = _module
chatglm3_6b_base_qlora_sql_e3 = _module
chatglm3_6b_base_qlora_tiny_codes_e1 = _module
cohere_100b_128k_sp32 = _module
baichuan2_13b_base_full_custom_pretrain_e1 = _module
baichuan2_7b_base_full_custom_pretrain_e1 = _module
chatglm2_6b_full_custom_pretrain_e1 = _module
chatglm3_6b_full_custom_pretrain_e1 = _module
deepseek_moe_16b_base_full_custom_pretrain_e1 = _module
gemma_2b_full_custom_pretrain_e1 = _module
gemma_7b_full_custom_pretrain_e1 = _module
internlm2_1_8b_full_custom_pretrain_e1 = _module
internlm2_20b_full_custom_pretrain_e1 = _module
internlm2_7b_full_custom_pretrain_e1 = _module
llama2_70b_full_custom_pretrain_e1 = _module
llama2_7b_full_custom_pretrain_e1 = _module
minicpm3_4b_full_custom_pretrain_e1 = _module
minicpm_1b_full_custom_pretrain_e1 = _module
minicpm_2b_full_custom_pretrain_e1 = _module
mistral_7b_full_custom_pretrain_e1 = _module
mixtral_8x7b_full_custom_pretrain_e1 = _module
qwen1_5_0_5b_full_custom_pretrain_e1 = _module
qwen1_5_14b_full_custom_pretrain_e1 = _module
qwen1_5_1_8b_full_custom_pretrain_e1 = _module
qwen1_5_4b_full_custom_pretrain_e1 = _module
qwen1_5_72b_full_custom_pretrain_e1 = _module
qwen1_5_7b_full_custom_pretrain_e1 = _module
qwen_1_8b_full_custom_pretrain_e1 = _module
qwen_72b_full_custom_pretrain_e1 = _module
qwen_7b_full_custom_pretrain_e1 = _module
starcoder_full_custom_pretrain_e1 = _module
yi_34b_full_custom_pretrain_e1 = _module
yi_6b_full_custom_pretrain_e1 = _module
zephyr_7b_beta_full_custom_pretrain_e1 = _module
baichuan2_13b_chat_qlora_custom_sft_e1 = _module
baichuan2_7b_chat_qlora_custom_sft_e1 = _module
baichuan_13b_chat_qlora_custom_sft_e1 = _module
baichuan_7b_qlora_custom_sft_e1 = _module
chatglm2_6b_qlora_custom_sft_e1 = _module
chatglm3_6b_qlora_custom_sft_e1 = _module
deepseek_moe_16b_chat_qlora_custom_sft_e1 = _module
deepseekcoder_6_7b_instruct_qlora_custom_sft_e1 = _module
gemma_2b_it_qlora_custom_sft_e1 = _module
gemma_2b_qlora_custom_sft_e1 = _module
gemma_7b_it_qlora_custom_sft_e1 = _module
gemma_7b_qlora_custom_sft_e1 = _module
internlm2_chat_1_8b_qlora_custom_sft_e1 = _module
internlm2_chat_20b_qlora_custom_sft_e1 = _module
internlm2_chat_7b_qlora_custom_sft_e1 = _module
llama2_70b_qlora_custom_sft_e1 = _module
llama2_7b_chat_qlora_custom_sft_e1 = _module
minicpm3_4b_chat_qlora_custom_sft_e1 = _module
mistral_7b_full_finetune_custom_sft_e1 = _module
mixtral_8x7b_instruct_qlora_custom_sft_e1 = _module
qwen1_5_0_5b_chat_qlora_custom_sft_e1 = _module
qwen1_5_14b_chat_qlora_custom_sft_e1 = _module
qwen1_5_1_8b_chat_qlora_custom_sft_e1 = _module
qwen1_5_4b_chat_qlora_custom_sft_e1 = _module
qwen1_5_72b_chat_qlora_custom_sft_e1 = _module
qwen1_5_7b_chat_qlora_custom_sft_e1 = _module
qwen_1_8b_chat_qlora_custom_sft_e1 = _module
qwen_72b_qlora_custom_sft_e1 = _module
qwen_7b_chat_qlora_custom_sft_e1 = _module
starcoder_qlora_custom_sft_e1 = _module
yi_34b_qlora_custom_sft_e1 = _module
yi_6b_qlora_custom_sft_e1 = _module
zephyr_7b_beta_qlora_custom_sft_e1 = _module
deepseek_coder_6_7b_base_qlora_code_alpaca_e3 = _module
deepseekcoder_6_7b_instruct_qlora_code_alpaca_e3 = _module
deepseek_moe_16b_base_full_oasst1_e3 = _module
deepseek_moe_16b_base_qlora_oasst1_e3 = _module
deepseek_moe_16b_chat_full_oasst1_e3 = _module
deepseek_moe_16b_chat_qlora_oasst1_e3 = _module
deepseek_v2_chat_full_alpaca_e3 = _module
deepseek_v2_lite_chat_full_alpaca_e3 = _module
deepseek_v2_lite_chat_full_alpaca_e3_32k_varlen = _module
internlm2_chat_1_8b_dpo_full = _module
internlm2_chat_1_8b_dpo_full_varlenattn = _module
internlm2_chat_1_8b_dpo_full_varlenattn_jsonl_dataset = _module
internlm2_chat_7b_dpo_qlora_varlenattn = _module
llama3_8b_instruct_dpo_qlora_varlenattn = _module
gemma_2b_full_alpaca_e3 = _module
gemma_2b_qlora_alpaca_e3 = _module
gemma_2b_it_full_alpaca_e3 = _module
gemma_2b_it_qlora_alpaca_e3 = _module
gemma_7b_full_alpaca_e3 = _module
gemma_7b_qlora_alpaca_e3 = _module
gemma_7b_it_full_alpaca_e3 = _module
gemma_7b_it_qlora_alpaca_e3 = _module
internlm2_1_8b_full_alpaca_e3 = _module
internlm2_1_8b_qlora_alpaca_e3 = _module
internlm2_20b_full_finetune_custom_dataset_e1 = _module
internlm2_20b_qlora_alpaca_e3 = _module
internlm2_20b_qlora_arxiv_gentitle_e3 = _module
internlm2_20b_qlora_code_alpaca_e3 = _module
internlm2_20b_qlora_colorist_e5 = _module
internlm2_20b_qlora_lawyer_e3 = _module
internlm2_20b_qlora_msagent_react_e3_gpu8 = _module
internlm2_20b_qlora_oasst1_512_e3 = _module
internlm2_20b_qlora_oasst1_e3 = _module
internlm2_20b_qlora_sql_e3 = _module
internlm2_5_chat_20b_alpaca_e3 = _module
internlm2_5_chat_20b_qlora_alpaca_e3 = _module
internlm2_5_chat_7b_full_finetune_custom_dataset_e1 = _module
internlm2_5_chat_7b_qlora_alpaca_e3 = _module
internlm2_5_chat_7b_qlora_oasst1_e3 = _module
internlm2_7b_full_finetune_custom_dataset_e1 = _module
internlm2_7b_full_finetune_custom_dataset_e1_sequence_parallel_4 = _module
internlm2_7b_qlora_alpaca_e3 = _module
internlm2_7b_qlora_arxiv_gentitle_e3 = _module
internlm2_7b_qlora_code_alpaca_e3 = _module
internlm2_7b_qlora_colorist_e5 = _module
internlm2_7b_qlora_json_e3 = _module
internlm2_7b_qlora_lawyer_e3 = _module
internlm2_7b_qlora_msagent_react_e3_gpu8 = _module
internlm2_7b_qlora_oasst1_512_e3 = _module
internlm2_7b_qlora_oasst1_e3 = _module
internlm2_7b_qlora_sql_e3 = _module
internlm2_7b_w_internevo_dataset = _module
internlm2_7b_w_tokenized_dataset = _module
internlm2_7b_w_untokenized_dataset = _module
internlm2_chat_1_8b_full_alpaca_e3 = _module
internlm2_chat_1_8b_qlora_alpaca_e3 = _module
internlm2_chat_20b_full_finetune_custom_dataset_e1 = _module
internlm2_chat_20b_qlora_alpaca_e3 = _module
internlm2_chat_20b_qlora_code_alpaca_e3 = _module
internlm2_chat_20b_qlora_lawyer_e3 = _module
internlm2_chat_20b_qlora_oasst1_512_e3 = _module
internlm2_chat_20b_qlora_oasst1_e3 = _module
internlm2_chat_7b_full_finetune_custom_dataset_e1 = _module
internlm2_chat_7b_qlora_alpaca_e3 = _module
internlm2_chat_7b_qlora_code_alpaca_e3 = _module
internlm2_chat_7b_qlora_lawyer_e3 = _module
internlm2_chat_7b_qlora_oasst1_512_e3 = _module
internlm2_chat_7b_qlora_oasst1_e3 = _module
internlm_20b_qlora_alpaca_e3 = _module
internlm_20b_qlora_alpaca_enzh_e3 = _module
internlm_20b_qlora_alpaca_enzh_oasst1_e3 = _module
internlm_20b_qlora_alpaca_zh_e3 = _module
internlm_20b_qlora_arxiv_gentitle_e3 = _module
internlm_20b_qlora_code_alpaca_e3 = _module
internlm_20b_qlora_colorist_e5 = _module
internlm_20b_qlora_lawyer_e3 = _module
internlm_20b_qlora_msagent_react_e3_gpu8 = _module
internlm_20b_qlora_oasst1_512_e3 = _module
internlm_20b_qlora_oasst1_e3 = _module
internlm_20b_qlora_open_platypus_e3 = _module
internlm_20b_qlora_sql_e3 = _module
internlm_7b_full_alpaca_e3 = _module
internlm_7b_full_alpaca_enzh_e3 = _module
internlm_7b_full_alpaca_enzh_oasst1_e3 = _module
internlm_7b_full_alpaca_zh_e3 = _module
internlm_7b_full_intern_repo_dataset_template = _module
internlm_7b_full_oasst1_e3 = _module
internlm_7b_qlora_alpaca_e3 = _module
internlm_7b_qlora_alpaca_enzh_e3 = _module
internlm_7b_qlora_alpaca_enzh_oasst1_e3 = _module
internlm_7b_qlora_alpaca_zh_e3 = _module
internlm_7b_qlora_arxiv_gentitle_e3 = _module
internlm_7b_qlora_code_alpaca_e3 = _module
internlm_7b_qlora_colorist_e5 = _module
internlm_7b_qlora_json_e3 = _module
internlm_7b_qlora_lawyer_e3 = _module
internlm_7b_qlora_medical_e1 = _module
internlm_7b_qlora_moss_sft_all_e1 = _module
internlm_7b_qlora_moss_sft_all_e2_gpu8 = _module
internlm_7b_qlora_moss_sft_plugins_e1 = _module
internlm_7b_qlora_msagent_react_e3_gpu8 = _module
internlm_7b_qlora_oasst1_512_e3 = _module
internlm_7b_qlora_oasst1_e3 = _module
internlm_7b_qlora_oasst1_e3_hf = _module
internlm_7b_qlora_oasst1_mmlu_e3 = _module
internlm_7b_qlora_open_platypus_e3 = _module
internlm_7b_qlora_openorca_e1 = _module
internlm_7b_qlora_sql_e3 = _module
internlm_7b_qlora_tiny_codes_e1 = _module
internlm_chat_20b_qlora_alpaca_e3 = _module
internlm_chat_20b_qlora_alpaca_enzh_e3 = _module
internlm_chat_20b_qlora_alpaca_enzh_oasst1_e3 = _module
internlm_chat_20b_qlora_alpaca_zh_e3 = _module
internlm_chat_20b_qlora_code_alpaca_e3 = _module
internlm_chat_20b_qlora_lawyer_e3 = _module
internlm_chat_20b_qlora_oasst1_512_e3 = _module
internlm_chat_20b_qlora_oasst1_e3 = _module
internlm_chat_20b_qlora_open_platypus_e3 = _module
internlm_chat_7b_qlora_alpaca_e3 = _module
internlm_chat_7b_qlora_alpaca_enzh_e3 = _module
internlm_chat_7b_qlora_alpaca_enzh_oasst1_e3 = _module
internlm_chat_7b_qlora_alpaca_zh_e3 = _module
internlm_chat_7b_qlora_arxiv_gentitle_e3 = _module
internlm_chat_7b_qlora_code_alpaca_e3 = _module
internlm_chat_7b_qlora_colorist_e5 = _module
internlm_chat_7b_qlora_lawyer_e3 = _module
internlm_chat_7b_qlora_medical_e1 = _module
internlm_chat_7b_qlora_oasst1_512_e3 = _module
internlm_chat_7b_qlora_oasst1_e3 = _module
internlm_chat_7b_qlora_open_platypus_e3 = _module
internlm_chat_7b_qlora_openorca_e1 = _module
internlm_chat_7b_qlora_sql_e3 = _module
internlm_chat_7b_qlora_tiny_codes_e1 = _module
convert_to_official = _module
internvl_v1_5_internlm2_26b_finetune = _module
internvl_v1_5_internlm2_26b_lora_finetune = _module
internvl_v1_5_internlm2_26b_qlora_finetune = _module
internvl_v1_5_internlm2_2b_finetune = _module
internvl_v1_5_internlm2_2b_lora_finetune = _module
internvl_v1_5_internlm2_2b_qlora_finetune = _module
internvl_v1_5_phi3_4b_finetune = _module
internvl_v1_5_phi3_4b_lora_finetune = _module
internvl_v1_5_phi3_4b_qlora_finetune = _module
internvl_v2_internlm2_26b_finetune = _module
internvl_v2_internlm2_26b_lora_finetune = _module
internvl_v2_internlm2_26b_qlora_finetune = _module
internvl_v2_internlm2_2b_finetune = _module
internvl_v2_internlm2_2b_lora_finetune = _module
internvl_v2_internlm2_2b_qlora_finetune = _module
internvl_v2_internlm2_5_8b_finetune = _module
internvl_v2_internlm2_5_8b_lora_finetune = _module
internvl_v2_internlm2_5_8b_qlora_finetune = _module
internvl_v2_phi3_4b_finetune = _module
internvl_v2_phi3_4b_lora_finetune = _module
internvl_v2_phi3_4b_qlora_finetune = _module
llama2_70b_full_wizardlm_e1 = _module
llama2_70b_int8_lora_open_platypus_e1 = _module
llama2_70b_int8_lora_open_platypus_e1_hf = _module
llama2_70b_qlora_open_platypus_e1 = _module
llama2_70b_qlora_open_platypus_e1_hf = _module
llama2_7b_full_pgbooks_400iters_sp1 = _module
llama2_7b_full_pgbooks_400iters_sp4 = _module
llama2_7b_full_wizardlm_e1 = _module
llama2_7b_qlora_alpaca_e3 = _module
llama2_7b_qlora_alpaca_enzh_e3 = _module
llama2_7b_qlora_alpaca_enzh_oasst1_e3 = _module
llama2_7b_qlora_alpaca_zh_e3 = _module
llama2_7b_qlora_arxiv_gentitle_e3 = _module
llama2_7b_qlora_code_alpaca_e3 = _module
llama2_7b_qlora_colorist_e5 = _module
llama2_7b_qlora_lawyer_e3 = _module
llama2_7b_qlora_medical_e1 = _module
llama2_7b_qlora_moss_sft_all_e1 = _module
llama2_7b_qlora_moss_sft_all_e2_gpu8 = _module
llama2_7b_qlora_moss_sft_plugins_e1 = _module
llama2_7b_qlora_msagent_react_e3_gpu8 = _module
llama2_7b_qlora_oasst1_512_e3 = _module
llama2_7b_qlora_oasst1_e3 = _module
llama2_7b_qlora_open_platypus_e3 = _module
llama2_7b_qlora_openorca_e1 = _module
llama2_7b_qlora_sql_e3 = _module
llama2_7b_qlora_tiny_codes_e1 = _module
llama2_7b_chat_qlora_alpaca_e3 = _module
llama2_7b_chat_qlora_alpaca_enzh_e3 = _module
llama2_7b_chat_qlora_alpaca_enzh_oasst1_e3 = _module
llama2_7b_chat_qlora_alpaca_zh_e3 = _module
llama2_7b_chat_qlora_arxiv_gentitle_e3 = _module
llama2_7b_chat_qlora_code_alpaca_e3 = _module
llama2_7b_chat_qlora_colorist_e5 = _module
llama2_7b_chat_qlora_lawyer_e3 = _module
llama2_7b_chat_qlora_medical_e1 = _module
llama2_7b_chat_qlora_oasst1_512_e3 = _module
llama2_7b_chat_qlora_oasst1_e3 = _module
llama2_7b_chat_qlora_open_platypus_e3 = _module
llama2_7b_chat_qlora_openorca_e1 = _module
llama2_7b_chat_qlora_sql_e3 = _module
llama2_7b_chat_qlora_tiny_codes_e1 = _module
llama3_70b_instruct_qlora_alpaca_e3_2k_gpu8 = _module
llama3_8b_full_alpaca_e3 = _module
llama3_8b_instruct_full_alpaca_e3 = _module
llama3_8b_instruct_qlora_alpaca_e3 = _module
llama_7b_qlora_alpaca_e3 = _module
llama_7b_qlora_alpaca_enzh_e3 = _module
llama_7b_qlora_alpaca_enzh_oasst1_e3 = _module
llama_7b_qlora_alpaca_zh_e3 = _module
llama_7b_qlora_arxiv_gentitle_e3 = _module
llama_7b_qlora_code_alpaca_e3 = _module
llama_7b_qlora_colorist_e5 = _module
llama_7b_qlora_lawyer_e3 = _module
llama_7b_qlora_medical_e1 = _module
llama_7b_qlora_moss_sft_all_e1 = _module
llama_7b_qlora_moss_sft_all_e2_gpu8 = _module
llama_7b_qlora_moss_sft_plugins_e1 = _module
llama_7b_qlora_oasst1_512_e3 = _module
llama_7b_qlora_oasst1_e3 = _module
llama_7b_qlora_open_platypus_e3 = _module
llama_7b_qlora_openorca_e1 = _module
llama_7b_qlora_sql_e3 = _module
llama_7b_qlora_tiny_codes_e1 = _module
llama2_70b_full_alpaca_enzh_128k_sp8 = _module
llama2_70b_full_alpaca_enzh_256k_sp16 = _module
llama2_70b_full_alpaca_enzh_32k_sp4 = _module
llama2_70b_full_alpaca_enzh_8k_sp1 = _module
llama2_7b_full_alpaca_enzh_128k_sp8 = _module
llama2_7b_full_alpaca_enzh_1M_sp16 = _module
llama2_7b_full_alpaca_enzh_256k_sp8 = _module
llama2_7b_full_alpaca_enzh_32k_sp1 = _module
llama2_7b_full_alpaca_enzh_8k_sp1 = _module
yi_34b_200k_full_alpaca_enzh_128k_sp8 = _module
yi_34b_200k_full_alpaca_enzh_256k_sp8 = _module
yi_34b_200k_full_alpaca_enzh_32k_sp2 = _module
yi_34b_200k_full_alpaca_enzh_8k_sp1 = _module
llava_internlm2_chat_1_8b_qlora_clip_vit_large_p14_336_lora_e1_gpu8_finetune = _module
llava_internlm2_chat_1_8b_clip_vit_large_p14_336_e1_gpu8_pretrain = _module
llava_internlm2_chat_20b_clip_vit_large_p14_336_e1_gpu8_finetune = _module
llava_internlm2_chat_20b_qlora_clip_vit_large_p14_336_lora_e1_gpu8_finetune = _module
llava_internlm2_chat_20b_clip_vit_large_p14_336_e1_gpu8_pretrain = _module
llava_internlm2_chat_7b_clip_vit_large_p14_336_e1_gpu8_finetune = _module
llava_internlm2_chat_7b_qlora_clip_vit_large_p14_336_lora_e1_gpu8_finetune = _module
llava_internlm2_chat_7b_clip_vit_large_p14_336_e1_gpu8_pretrain = _module
llava_internlm_chat_7b_qlora_clip_vit_large_p14_336_lora_e1_gpu8_finetune = _module
llava_internlm_chat_7b_clip_vit_large_p14_336_e1_gpu8_pretrain = _module
llava_llama3_70b_instruct_quant_clip_vit_large_p14_336_e1_gpu8_pretrain = _module
convert_xtuner_weights_to_hf = _module
convert_xtuner_weights_to_llava = _module
llava_llama3_8b_instruct_full_clip_vit_large_p14_336_e1_gpu8_finetune = _module
llava_llama3_8b_instruct_full_clip_vit_large_p14_336_lora_e1_gpu8_finetune = _module
llava_llama3_8b_instruct_full_clip_vit_large_p14_336_lora_e1_gpu8_internvl_finetune = _module
llava_llama3_8b_instruct_qlora_clip_vit_large_p14_336_e1_gpu1_finetune = _module
llava_llama3_8b_instruct_clip_vit_large_p14_336_e1_gpu8_pretrain = _module
llava_llama3_8b_instruct_clip_vit_large_p14_336_e1_gpu8_sharegpt4v_pretrain = _module
llava_llama3_8b_instruct_quant_clip_vit_large_p14_336_e1_gpu1_pretrain = _module
llava_v15_13b_finetune = _module
llava_v15_13b_finetune_lora = _module
llava_v15_13b_pretrain = _module
llava_v15_7b_finetune = _module
llava_v15_7b_finetune_lora = _module
llava_v15_7b_pretrain = _module
convert_phi_to_llama = _module
llava_phi3_mini_4k_instruct_full_clip_vit_large_p14_336_e1_gpu8_finetune = _module
llava_phi3_mini_4k_instruct_full_clip_vit_large_p14_336_full_e2_gpu8_internvl_finetune = _module
llava_phi3_mini_4k_instruct_clip_vit_large_p14_336_e1_gpu8_pretrain = _module
llava_phi3_mini_4k_instruct_clip_vit_large_p14_336_e1_gpu8_sharegpt4v_pretrain = _module
llava_vicuna_13b_v15_qlora_clip_vit_large_p14_336_lora_e1_gpu8_finetune = _module
llava_vicuna_13b_v15_clip_vit_large_p14_336_e1_gpu8_pretrain = _module
llava_vicuna_7b_v15_qlora_clip_vit_large_p14_336_lora_e1_gpu8_finetune = _module
llava_vicuna_7b_v15_qlora_clip_vit_large_p14_336_lora_e1_gpu8_finetune_refcoco = _module
llava_vicuna_7b_v15_clip_vit_large_p14_336_e1_gpu8_pretrain = _module
minicpm_1b_dpo_qlora = _module
minicpm_1b_full_alpaca_zh_e3 = _module
minicpm_1b_lora_alpaca_zh_e3 = _module
minicpm_1b_qlora_alpaca_enzh_e3 = _module
minicpm_1b_qlora_alpaca_zh_e3 = _module
minicpm_2b_dpo_qlora = _module
minicpm_2b_full_alpaca_zh_e3 = _module
minicpm_2b_lora_alpaca_zh_e3 = _module
minicpm_2b_qlora_alpaca_enzh_e3 = _module
minicpm_2b_qlora_alpaca_zh_e3 = _module
minicpm3_4b_dpo_qlora = _module
minicpm3_4b_full_alpaca_zh_e3 = _module
mistral_7b_full_finetune_custom_dataset_e1 = _module
mistral_7b_qlora_skypile_pretrain_e1 = _module
mistral_7b_w_tokenized_dataset = _module
mistral_7b_w_untokenized_dataset = _module
mixtral_8x7b_full_oasst1_e3 = _module
mixtral_8x7b_qlora_oasst1_e3 = _module
mixtral_8x7b_instruct_full_oasst1_e3 = _module
mixtral_8x7b_instruct_qlora_oasst1_e3 = _module
internlm2_chat_1_8b_orpo_full = _module
internlm2_chat_1_8b_orpo_full_varlenattn = _module
internlm2_chat_1_8b_orpo_full_varlenattn_jsonl_dataset = _module
internlm2_chat_7b_orpo_qlora_varlenattn_ultrafeedback_e5 = _module
llama3_8b_instruct_orpo_qlora_varlenattn_ultrafeedback_e5 = _module
phi3_mini_128k_instruct_full_alpaca_e3 = _module
phi3_mini_128k_instruct_qlora_alpaca_e3 = _module
phi3_mini_4k_instruct_full_alpaca_e3 = _module
phi3_mini_4k_instruct_qlora_alpaca_e3 = _module
qwen_1_8b_qlora_alpaca_e3 = _module
qwen_1_8b_qlora_alpaca_enzh_e3 = _module
qwen_1_8b_qlora_alpaca_enzh_oasst1_e3 = _module
qwen_1_8b_qlora_alpaca_zh_e3 = _module
qwen_1_8b_qlora_code_alpaca_e3 = _module
qwen_1_8b_chat_qlora_alpaca_e3 = _module
qwen_1_8b_chat_qlora_alpaca_enzh_e3 = _module
qwen_1_8b_chat_qlora_alpaca_enzh_oasst1_e3 = _module
qwen_1_8b_chat_qlora_alpaca_zh_e3 = _module
qwen_1_8b_chat_qlora_code_alpaca_e3 = _module
qwen_72b_qlora_alpaca_e3 = _module
qwen_72b_qlora_alpaca_enzh_e3 = _module
qwen_72b_qlora_alpaca_enzh_oasst1_e3 = _module
qwen_72b_qlora_alpaca_zh_e3 = _module
qwen_72b_qlora_code_alpaca_e3 = _module
qwen_7b_qlora_alpaca_e3 = _module
qwen_7b_qlora_alpaca_enzh_e3 = _module
qwen_7b_qlora_alpaca_enzh_oasst1_e3 = _module
qwen_7b_qlora_alpaca_zh_e3 = _module
qwen_7b_qlora_arxiv_gentitle_e3 = _module
qwen_7b_qlora_code_alpaca_e3 = _module
qwen_7b_qlora_colorist_e5 = _module
qwen_7b_qlora_lawyer_e3 = _module
qwen_7b_qlora_medical_e1 = _module
qwen_7b_qlora_moss_sft_all_e1 = _module
qwen_7b_qlora_moss_sft_all_e2_gpu8 = _module
qwen_7b_qlora_moss_sft_plugins_e1 = _module
qwen_7b_qlora_oasst1_512_e3 = _module
qwen_7b_qlora_oasst1_e3 = _module
qwen_7b_qlora_open_platypus_e3 = _module
qwen_7b_qlora_openorca_e1 = _module
qwen_7b_qlora_sql_e3 = _module
qwen_7b_qlora_tiny_codes_e1 = _module
qwen_7b_chat_qlora_alpaca_e3 = _module
qwen_7b_chat_qlora_alpaca_enzh_e3 = _module
qwen_7b_chat_qlora_alpaca_enzh_oasst1_e3 = _module
qwen_7b_chat_qlora_alpaca_zh_e3 = _module
qwen_7b_chat_qlora_arxiv_gentitle_e3 = _module
qwen_7b_chat_qlora_code_alpaca_e3 = _module
qwen_7b_chat_qlora_colorist_e5 = _module
qwen_7b_chat_qlora_lawyer_e3 = _module
qwen_7b_chat_qlora_medical_e1 = _module
qwen_7b_chat_qlora_oasst1_512_e3 = _module
qwen_7b_chat_qlora_oasst1_e3 = _module
qwen_7b_chat_qlora_open_platypus_e3 = _module
qwen_7b_chat_qlora_openorca_e1 = _module
qwen_7b_chat_qlora_sql_e3 = _module
qwen_7b_chat_qlora_tiny_codes_e1 = _module
qwen1_5_0_5b_full_alpaca_e3 = _module
qwen1_5_0_5b_qlora_alpaca_e3 = _module
qwen1_5_0_5b_chat_full_alpaca_e3 = _module
qwen1_5_0_5b_chat_qlora_alpaca_e3 = _module
qwen1_5_110b_full_alpaca_e3 = _module
qwen1_5_110b_qlora_alpaca_e3 = _module
qwen1_5_110b_chat_full_alpaca_e3 = _module
qwen1_5_110b_chat_qlora_alpaca_e3 = _module
qwen1_5_110b_chat_qlora_alpaca_e3_16k_2gpus = _module
qwen1_5_14b_full_alpaca_e3 = _module
qwen1_5_14b_qlora_alpaca_e3 = _module
qwen1_5_14b_chat_full_alpaca_e3 = _module
qwen1_5_14b_chat_qlora_alpaca_e3 = _module
qwen1_5_1_8b_full_alpaca_e3 = _module
qwen1_5_1_8b_qlora_alpaca_e3 = _module
qwen1_5_1_8b_chat_full_alpaca_e3 = _module
qwen1_5_1_8b_chat_qlora_alpaca_e3 = _module
qwen1_5_4b_full_alpaca_e3 = _module
qwen1_5_4b_qlora_alpaca_e3 = _module
qwen1_5_4b_chat_full_alpaca_e3 = _module
qwen1_5_4b_chat_qlora_alpaca_e3 = _module
qwen1_5_72b_full_alpaca_e3 = _module
qwen1_5_72b_qlora_alpaca_e3 = _module
qwen1_5_72b_chat_full_alpaca_e3 = _module
qwen1_5_72b_chat_qlora_alpaca_e3 = _module
qwen1_5_7b_full_alpaca_e3 = _module
qwen1_5_7b_qlora_alpaca_e3 = _module
qwen1_5_7b_chat_full_alpaca_e3 = _module
qwen1_5_7b_chat_qlora_alpaca_e3 = _module
qwen1_5_moe_a2_7_b_chat_full_alpaca_e3 = _module
internlm2_chat_1_8b_reward_full_ultrafeedback = _module
internlm2_chat_1_8b_reward_full_varlenattn_jsonl_dataset = _module
internlm2_chat_1_8b_reward_full_varlenattn_ultrafeedback = _module
internlm2_chat_1_8b_reward_qlora_varlenattn_ultrafeedback = _module
llama3_8b_instruct_reward_full_varlenattn_ultrafeedback = _module
starcoder_qlora_stack_exchange_example = _module
yi_34b_qlora_alpaca_enzh_e3 = _module
yi_6b_qlora_alpaca_enzh_e3 = _module
zephyr_7b_beta_qlora_alpaca_e3 = _module
dataset = _module
collate_fns = _module
default_collate_fn = _module
mmlu_collate_fn = _module
preference_collate_fn = _module
concat_dataset = _module
huggingface = _module
intern_repo = _module
internvl_dataset = _module
json_dataset = _module
llava = _module
map_fns = _module
dataset_map_fns = _module
alpaca_map_fn = _module
alpaca_zh_map_fn = _module
arxiv_map_fn = _module
code_alpaca_map_fn = _module
colors_map_fn = _module
crime_kg_assitant_map_fn = _module
default_map_fn = _module
law_reference_map_fn = _module
llava_map_fn = _module
medical_map_fn = _module
msagent_map_fn = _module
oasst1_map_fn = _module
openai_map_fn = _module
openorca_map_fn = _module
pretrain_map_fn = _module
sql_map_fn = _module
stack_exchange_map_fn = _module
tiny_codes_map_fn = _module
wizardlm_map_fn = _module
template_map_fn = _module
modelscope = _module
moss_sft = _module
preference_dataset = _module
refcoco_json = _module
samplers = _module
length_grouped = _module
utils = _module
engine = _module
_strategy = _module
deepspeed = _module
hooks = _module
dataset_info_hook = _module
evaluate_chat_hook = _module
hf_checkpoint_hook = _module
throughput_hook = _module
varlen_attn_args_to_messagehub_hook = _module
runner = _module
loops = _module
entry_point = _module
evaluation = _module
metrics = _module
mmlu_metric = _module
reward_metric = _module
dpo = _module
internvl = _module
modules = _module
dispatch = _module
attention = _module
baichuan = _module
cohere = _module
deepseek_v2 = _module
internlm = _module
internlm2 = _module
llama = _module
mistral = _module
phi3 = _module
qwen2 = _module
triton_kernels = _module
layer_norm = _module
rms_norm = _module
rotary = _module
yi = _module
projector = _module
configuration_projector = _module
modeling_projector = _module
orpo = _module
reward = _module
sft = _module
transformers_models = _module
configuration_deepseek = _module
modeling_deepseek = _module
tokenization_deepseek_fast = _module
mixtral = _module
configuration_mixtral = _module
modeling_mixtral = _module
parallel = _module
sequence = _module
comm = _module
data_collate = _module
reduce_loss = _module
sampler = _module
setup_distributed = _module
registry = _module
chat = _module
check_custom_dataset = _module
copy_cfg = _module
convert_refcoco = _module
eval_refcoco = _module
get_data_order = _module
list_cfg = _module
list_dataset_format = _module
log_dataset = _module
mmbench = _module
merge = _module
modeling_internlm2_reward = _module
configuration_internlm2 = _module
modeling_internlm2 = _module
pth_to_hf = _module
split = _module
plugins = _module
api = _module
calculate = _module
search = _module
solve = _module
process_untokenized_datasets = _module
process_untokenized_datasets_legacy = _module
process_untokenized_llava_data = _module
test = _module
tokenize_ftdp_datasets = _module
train = _module
constants = _module
fileio = _module
handle_moe_load_and_save = _module
stop_criteria = _module
templates = _module
zero_to_any_dtype = _module
version = _module

from _paritybench_helpers import _mock_config, patch_functional
from unittest.mock import mock_open, MagicMock
from torch.autograd import Function
from torch.nn import Module
import abc, collections, copy, enum, functools, inspect, itertools, logging, math, matplotlib, numbers, numpy, pandas, queue, random, re, scipy, sklearn, string, time, torch, torchaudio, torchvision, triton, types, typing, uuid, warnings
import operator as op
from dataclasses import dataclass
import numpy as np
from torch import Tensor
patch_functional()
open = mock_open()
yaml = logging = sys = argparse = MagicMock()
ArgumentParser = argparse.ArgumentParser
_global_config = args = argv = cfg = config = params = _mock_config()
argparse.ArgumentParser.return_value.parse_args.return_value = _global_config
yaml.load.return_value = _global_config
sys.argv = _global_config
__version__ = '1.0.0'
xrange = range
wraps = functools.wraps


import types


import torch


import triton


import triton.language as tl


from typing import Optional


from typing import Union


@triton.jit
def _rms_norm_fwd_fused(X, Y, W, Rstd, stride, N, eps, BLOCK_SIZE: 'tl.constexpr'):
    row = tl.program_id(0)
    Y += row * stride
    X += row * stride
    _var = tl.zeros([BLOCK_SIZE], dtype=tl.float32)
    for off in range(0, N, BLOCK_SIZE):
        cols = off + tl.arange(0, BLOCK_SIZE)
        x = tl.load(X + cols, mask=cols < N, other=0.0)
        _var += x * x
    var = tl.sum(_var, axis=0) / N
    rstd = 1 / tl.sqrt(var + eps)
    tl.store(Rstd + row, rstd)
    for off in range(0, N, BLOCK_SIZE):
        cols = off + tl.arange(0, BLOCK_SIZE)
        mask = cols < N
        w = tl.load(W + cols, mask=mask)
        x = tl.load(X + cols, mask=mask, other=0.0)
        x_hat = x * rstd
        y = x_hat * w
        tl.store(Y + cols, y, mask=mask)


@triton.jit
def _rms_norm_bwd_dx_fused(DX, DY, DW, X, W, Rstd, Lock, stride, N, eps, GROUP_SIZE_M: 'tl.constexpr', BLOCK_SIZE_N: 'tl.constexpr'):
    row = tl.program_id(0)
    cols = tl.arange(0, BLOCK_SIZE_N)
    mask = cols < N
    X += row * stride
    DY += row * stride
    DX += row * stride
    lock_id = row % GROUP_SIZE_M
    Lock += lock_id
    Count = Lock + GROUP_SIZE_M
    DW = DW + lock_id * N + cols
    x = tl.load(X + cols, mask=mask, other=0)
    dy = tl.load(DY + cols, mask=mask, other=0)
    w = tl.load(W + cols, mask=mask)
    rstd = tl.load(Rstd + row)
    xhat = x * rstd
    wdy = w * dy
    xhat = tl.where(mask, xhat, 0.0)
    wdy = tl.where(mask, wdy, 0.0)
    c1 = tl.sum(xhat * wdy, axis=0) / N
    dx = (wdy - xhat * c1) * rstd
    tl.store(DX + cols, dx, mask=mask)
    partial_dw = dy * xhat
    while tl.atomic_cas(Lock, 0, 1) == 1:
        pass
    count = tl.load(Count)
    if count == 0:
        tl.atomic_xchg(Count, 1)
    else:
        partial_dw += tl.load(DW, mask=mask)
    tl.store(DW, partial_dw, mask=mask)
    tl.atomic_xchg(Lock, 0)


@triton.jit
def _rms_norm_bwd_dwdb(DW, FINAL_DW, M, N, BLOCK_SIZE_M: 'tl.constexpr', BLOCK_SIZE_N: 'tl.constexpr'):
    pid = tl.program_id(0)
    cols = pid * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)
    dw = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)
    for i in range(0, M, BLOCK_SIZE_M):
        rows = i + tl.arange(0, BLOCK_SIZE_M)
        mask = (rows[:, None] < M) & (cols[None, :] < N)
        offs = rows[:, None] * N + cols[None, :]
        dw += tl.load(DW + offs, mask=mask, other=0.0)
    sum_dw = tl.sum(dw, axis=0)
    tl.store(FINAL_DW + cols, sum_dw, mask=cols < N)


@triton.jit
def rotary_kernel(OUT, X, COS, SIN, CU_SEQLENS, SEQLEN_OFFSETS, seqlen, rotary_dim, seqlen_ro, stride_out_batch, stride_out_seqlen, stride_out_nheads, stride_out_headdim, stride_x_batch, stride_x_seqlen, stride_x_nheads, stride_x_headdim, BLOCK_K: 'tl.constexpr', IS_SEQLEN_OFFSETS_TENSOR: 'tl.constexpr', IS_VARLEN: 'tl.constexpr', INTERLEAVED: 'tl.constexpr', CONJUGATE: 'tl.constexpr', BLOCK_M: 'tl.constexpr'):
    pid_m = tl.program_id(axis=0)
    pid_batch = tl.program_id(axis=1)
    pid_head = tl.program_id(axis=2)
    rotary_dim_half = rotary_dim // 2
    if not IS_VARLEN:
        X = X + pid_batch * stride_x_batch + pid_head * stride_x_nheads
        OUT = OUT + pid_batch * stride_out_batch + pid_head * stride_out_nheads
    else:
        start_idx = tl.load(CU_SEQLENS + pid_batch)
        seqlen = tl.load(CU_SEQLENS + pid_batch + 1) - start_idx
        X = X + start_idx * stride_x_seqlen + pid_head * stride_x_nheads
        OUT = OUT + start_idx * stride_out_seqlen + pid_head * stride_out_nheads
    if pid_m * BLOCK_M >= seqlen:
        return
    rm = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)
    if not IS_SEQLEN_OFFSETS_TENSOR:
        rm_cs = rm + SEQLEN_OFFSETS
    else:
        rm_cs = rm + tl.load(SEQLEN_OFFSETS + pid_batch)
    rk = tl.arange(0, BLOCK_K)
    rk_half = tl.arange(0, BLOCK_K // 2)
    if not INTERLEAVED:
        X = X + (rm[:, None] * stride_x_seqlen + rk_half[None, :] * stride_x_headdim)
        COS = COS + (rm_cs[:, None] * rotary_dim + rk_half[None, :])
        SIN = SIN + (rm_cs[:, None] * rotary_dim + rk_half[None, :])
        cos = tl.load(COS, mask=(rm_cs[:, None] < seqlen_ro) & (rk_half[None, :] < rotary_dim_half), other=1.0)
        sin = tl.load(SIN, mask=(rm_cs[:, None] < seqlen_ro) & (rk_half[None, :] < rotary_dim_half), other=0.0)
        x0 = tl.load(X, mask=(rm[:, None] < seqlen) & (rk_half[None, :] < rotary_dim_half), other=0.0)
        x1 = tl.load(X + rotary_dim_half * stride_x_headdim, mask=(rm[:, None] < seqlen) & (rk_half[None, :] < rotary_dim_half), other=0.0)
        if CONJUGATE:
            sin = -sin
        o0 = x0 * cos - x1 * sin
        o1 = x0 * sin + x1 * cos
        OUT = OUT + (rm[:, None] * stride_out_seqlen + rk_half[None, :] * stride_out_headdim)
        tl.store(OUT, o0, mask=(rm[:, None] < seqlen) & (rk_half[None, :] < rotary_dim_half))
        tl.store(OUT + rotary_dim_half * stride_out_headdim, o1, mask=(rm[:, None] < seqlen) & (rk_half[None, :] < rotary_dim_half))
    else:
        rk_swap = rk + (rk + 1) % 2 * 2 - 1
        rk_repeat = tl.arange(0, BLOCK_K) // 2
        X0 = X + (rm[:, None] * stride_x_seqlen + rk[None, :] * stride_x_headdim)
        X1 = X + (rm[:, None] * stride_x_seqlen + rk_swap[None, :] * stride_x_headdim)
        COS = COS + (rm_cs[:, None] * rotary_dim + rk_repeat[None, :])
        SIN = SIN + (rm_cs[:, None] * rotary_dim + rk_repeat[None, :])
        cos = tl.load(COS, mask=(rm_cs[:, None] < seqlen_ro) & (rk_repeat[None, :] < rotary_dim_half), other=1.0)
        sin = tl.load(SIN, mask=(rm_cs[:, None] < seqlen_ro) & (rk_repeat[None, :] < rotary_dim_half), other=0.0)
        x0 = tl.load(X0, mask=(rm[:, None] < seqlen) & (rk[None, :] < rotary_dim), other=0.0)
        x1 = tl.load(X1, mask=(rm[:, None] < seqlen) & (rk_swap[None, :] < rotary_dim), other=0.0)
        if CONJUGATE:
            sin = -sin
        x0_cos = x0 * cos
        x1_sin = x1 * sin
        out = tl.where(rk[None, :] % 2 == 0, x0_cos - x1_sin, x0_cos + x1_sin)
        OUT = OUT + (rm[:, None] * stride_out_seqlen + rk[None, :] * stride_out_headdim)
        tl.store(OUT, out, mask=(rm[:, None] < seqlen) & (rk[None, :] < rotary_dim))

