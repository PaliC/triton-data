class <lambda>(torch.nn.Module):
    def forward(self, arg0_1: "f32[8, 3, 224, 224]", arg1_1: "f32[768, 3, 16, 16]", arg2_1: "f32[768]", arg3_1: "f32[1, 196, 768]", arg4_1: "f32[1, 1, 768]", arg5_1: "f32[768]", arg6_1: "f32[768]", arg7_1: "f32[1536, 768]", arg8_1: "f32[16, 3]", arg9_1: "f32[16]", arg10_1: "f32[16]", arg11_1: "f32[768, 768]", arg12_1: "f32[768, 768]", arg13_1: "f32[768]", arg14_1: "f32[768]", arg15_1: "f32[768]", arg16_1: "f32[3072, 768]", arg17_1: "f32[3072]", arg18_1: "f32[768, 3072]", arg19_1: "f32[768]", arg20_1: "f32[768]", arg21_1: "f32[768]", arg22_1: "f32[1536, 768]", arg23_1: "f32[16, 3]", arg24_1: "f32[16]", arg25_1: "f32[16]", arg26_1: "f32[768, 768]", arg27_1: "f32[768, 768]", arg28_1: "f32[768]", arg29_1: "f32[768]", arg30_1: "f32[768]", arg31_1: "f32[3072, 768]", arg32_1: "f32[3072]", arg33_1: "f32[768, 3072]", arg34_1: "f32[768]", arg35_1: "f32[768]", arg36_1: "f32[768]", arg37_1: "f32[1536, 768]", arg38_1: "f32[16, 3]", arg39_1: "f32[16]", arg40_1: "f32[16]", arg41_1: "f32[768, 768]", arg42_1: "f32[768, 768]", arg43_1: "f32[768]", arg44_1: "f32[768]", arg45_1: "f32[768]", arg46_1: "f32[3072, 768]", arg47_1: "f32[3072]", arg48_1: "f32[768, 3072]", arg49_1: "f32[768]", arg50_1: "f32[768]", arg51_1: "f32[768]", arg52_1: "f32[1536, 768]", arg53_1: "f32[16, 3]", arg54_1: "f32[16]", arg55_1: "f32[16]", arg56_1: "f32[768, 768]", arg57_1: "f32[768, 768]", arg58_1: "f32[768]", arg59_1: "f32[768]", arg60_1: "f32[768]", arg61_1: "f32[3072, 768]", arg62_1: "f32[3072]", arg63_1: "f32[768, 3072]", arg64_1: "f32[768]", arg65_1: "f32[768]", arg66_1: "f32[768]", arg67_1: "f32[1536, 768]", arg68_1: "f32[16, 3]", arg69_1: "f32[16]", arg70_1: "f32[16]", arg71_1: "f32[768, 768]", arg72_1: "f32[768, 768]", arg73_1: "f32[768]", arg74_1: "f32[768]", arg75_1: "f32[768]", arg76_1: "f32[3072, 768]", arg77_1: "f32[3072]", arg78_1: "f32[768, 3072]", arg79_1: "f32[768]", arg80_1: "f32[768]", arg81_1: "f32[768]", arg82_1: "f32[1536, 768]", arg83_1: "f32[16, 3]", arg84_1: "f32[16]", arg85_1: "f32[16]", arg86_1: "f32[768, 768]", arg87_1: "f32[768, 768]", arg88_1: "f32[768]", arg89_1: "f32[768]", arg90_1: "f32[768]", arg91_1: "f32[3072, 768]", arg92_1: "f32[3072]", arg93_1: "f32[768, 3072]", arg94_1: "f32[768]", arg95_1: "f32[768]", arg96_1: "f32[768]", arg97_1: "f32[1536, 768]", arg98_1: "f32[16, 3]", arg99_1: "f32[16]", arg100_1: "f32[16]", arg101_1: "f32[768, 768]", arg102_1: "f32[768, 768]", arg103_1: "f32[768]", arg104_1: "f32[768]", arg105_1: "f32[768]", arg106_1: "f32[3072, 768]", arg107_1: "f32[3072]", arg108_1: "f32[768, 3072]", arg109_1: "f32[768]", arg110_1: "f32[768]", arg111_1: "f32[768]", arg112_1: "f32[1536, 768]", arg113_1: "f32[16, 3]", arg114_1: "f32[16]", arg115_1: "f32[16]", arg116_1: "f32[768, 768]", arg117_1: "f32[768, 768]", arg118_1: "f32[768]", arg119_1: "f32[768]", arg120_1: "f32[768]", arg121_1: "f32[3072, 768]", arg122_1: "f32[3072]", arg123_1: "f32[768, 3072]", arg124_1: "f32[768]", arg125_1: "f32[768]", arg126_1: "f32[768]", arg127_1: "f32[1536, 768]", arg128_1: "f32[16, 3]", arg129_1: "f32[16]", arg130_1: "f32[16]", arg131_1: "f32[768, 768]", arg132_1: "f32[768, 768]", arg133_1: "f32[768]", arg134_1: "f32[768]", arg135_1: "f32[768]", arg136_1: "f32[3072, 768]", arg137_1: "f32[3072]", arg138_1: "f32[768, 3072]", arg139_1: "f32[768]", arg140_1: "f32[768]", arg141_1: "f32[768]", arg142_1: "f32[1536, 768]", arg143_1: "f32[16, 3]", arg144_1: "f32[16]", arg145_1: "f32[16]", arg146_1: "f32[768, 768]", arg147_1: "f32[768, 768]", arg148_1: "f32[768]", arg149_1: "f32[768]", arg150_1: "f32[768]", arg151_1: "f32[3072, 768]", arg152_1: "f32[3072]", arg153_1: "f32[768, 3072]", arg154_1: "f32[768]", arg155_1: "f32[768]", arg156_1: "f32[768]", arg157_1: "f32[2304, 768]", arg158_1: "f32[768, 768]", arg159_1: "f32[768]", arg160_1: "f32[768]", arg161_1: "f32[768]", arg162_1: "f32[3072, 768]", arg163_1: "f32[3072]", arg164_1: "f32[768, 3072]", arg165_1: "f32[768]", arg166_1: "f32[768]", arg167_1: "f32[768]", arg168_1: "f32[2304, 768]", arg169_1: "f32[768, 768]", arg170_1: "f32[768]", arg171_1: "f32[768]", arg172_1: "f32[768]", arg173_1: "f32[3072, 768]", arg174_1: "f32[3072]", arg175_1: "f32[768, 3072]", arg176_1: "f32[768]", arg177_1: "f32[768]", arg178_1: "f32[768]", arg179_1: "f32[1000, 768]", arg180_1: "f32[1000]"):
         # File: /home/sahanp/.conda/envs/pytorch-benchmarks/lib/python3.12/site-packages/timm/layers/patch_embed.py:131 in forward, code: x = self.proj(x)
        convolution_1: "f32[8, 768, 14, 14]" = torch.ops.aten.convolution.default(arg0_1, arg1_1, arg2_1, [16, 16], [0, 0], [1, 1], False, [0, 0], 1);  arg0_1 = arg1_1 = arg2_1 = None
        
         # File: /home/sahanp/.conda/envs/pytorch-benchmarks/lib/python3.12/site-packages/timm/layers/patch_embed.py:133 in forward, code: x = x.flatten(2).transpose(1, 2)  # NCHW -> NLC
        view_293: "f32[8, 768, 196]" = torch.ops.aten.reshape.default(convolution_1, [8, 768, 196]);  convolution_1 = None
        permute_126: "f32[8, 196, 768]" = torch.ops.aten.permute.default(view_293, [0, 2, 1]);  view_293 = None
        
         # File: /home/sahanp/.conda/envs/pytorch-benchmarks/lib/python3.12/site-packages/timm/models/convit.py:360 in forward_features, code: x = x + self.pos_embed
        add_117: "f32[8, 196, 768]" = torch.ops.aten.add.Tensor(permute_126, arg3_1);  permute_126 = arg3_1 = None
        
         # File: /home/sahanp/.conda/envs/pytorch-benchmarks/lib/python3.12/site-packages/timm/layers/norm.py:57 in forward, code: x = F.layer_norm(x, self.normalized_shape, self.weight, self.bias, self.eps)
        clone_159: "f32[8, 196, 768]" = torch.ops.aten.clone.default(add_117, memory_format = torch.contiguous_format)
        var_mean_25 = torch.ops.aten.var_mean.correction(clone_159, [2], correction = 0, keepdim = True)
        getitem_56: "f32[8, 196, 1]" = var_mean_25[0]
        getitem_57: "f32[8, 196, 1]" = var_mean_25[1];  var_mean_25 = None
        
         # File: /home/sahanp/.conda/envs/pytorch-benchmarks/lib/python3.12/site-packages/timm/models/convit.py:88 in get_attention, code: gating = self.gating_param.view(1, -1, 1, 1)
        view_302: "f32[1, 16, 1, 1]" = torch.ops.aten.reshape.default(arg10_1, [1, -1, 1, 1]);  arg10_1 = None
        
         # File: /home/sahanp/.conda/envs/pytorch-benchmarks/lib/python3.12/site-packages/timm/models/convit.py:89 in get_attention, code: attn = (1. - torch.sigmoid(gating)) * patch_score + torch.sigmoid(gating) * pos_score
        sigmoid_20: "f32[1, 16, 1, 1]" = torch.ops.aten.sigmoid.default(view_302)
        sub_70: "f32[1, 16, 1, 1]" = torch.ops.aten.sub.Tensor(1.0, sigmoid_20);  sigmoid_20 = None
        
         # File: /home/sahanp/.conda/envs/pytorch-benchmarks/lib/python3.12/site-packages/timm/layers/norm.py:57 in forward, code: x = F.layer_norm(x, self.normalized_shape, self.weight, self.bias, self.eps)
        sub_67: "f32[8, 196, 768]" = torch.ops.aten.sub.Tensor(clone_159, getitem_57);  clone_159 = getitem_57 = None
        add_118: "f32[8, 196, 1]" = torch.ops.aten.add.Tensor(getitem_56, 1e-06);  getitem_56 = None
        rsqrt_25: "f32[8, 196, 1]" = torch.ops.aten.rsqrt.default(add_118);  add_118 = None
        mul_118: "f32[8, 196, 768]" = torch.ops.aten.mul.Tensor(sub_67, rsqrt_25);  sub_67 = rsqrt_25 = None
        mul_119: "f32[8, 196, 768]" = torch.ops.aten.mul.Tensor(mul_118, arg5_1);  mul_118 = arg5_1 = None
        add_119: "f32[8, 196, 768]" = torch.ops.aten.add.Tensor(mul_119, arg6_1);  mul_119 = arg6_1 = None
        
         # File: /home/sahanp/.conda/envs/pytorch-benchmarks/lib/python3.12/site-packages/timm/models/convit.py:80 in get_attention, code: qk = self.qk(x).reshape(B, N, 2, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)
        view_294: "f32[1568, 768]" = torch.ops.aten.reshape.default(add_119, [1568, 768])
        permute_127: "f32[768, 1536]" = torch.ops.aten.permute.default(arg7_1, [1, 0]);  arg7_1 = None
        mm_32: "f32[1568, 1536]" = torch.ops.aten.mm.default(view_294, permute_127);  view_294 = permute_127 = None
        view_295: "f32[8, 196, 1536]" = torch.ops.aten.reshape.default(mm_32, [8, 196, 1536]);  mm_32 = None
        view_296: "f32[8, 196, 2, 16, 48]" = torch.ops.aten.reshape.default(view_295, [8, 196, 2, 16, 48]);  view_295 = None
        permute_128: "f32[2, 8, 16, 196, 48]" = torch.ops.aten.permute.default(view_296, [2, 0, 3, 1, 4]);  view_296 = None
        
         # File: /home/sahanp/.conda/envs/pytorch-benchmarks/lib/python3.12/site-packages/timm/models/convit.py:81 in get_attention, code: q, k = qk[0], qk[1]
        select_101: "f32[8, 16, 196, 48]" = torch.ops.aten.select.int(permute_128, 0, 0)
        
         # File: /home/sahanp/.conda/envs/pytorch-benchmarks/lib/python3.12/site-packages/timm/models/convit.py:84 in get_attention, code: patch_score = (q @ k.transpose(-2, -1)) * self.scale
        expand_81: "f32[8, 16, 196, 48]" = torch.ops.aten.expand.default(select_101, [8, 16, 196, 48]);  select_101 = None
        clone_161: "f32[8, 16, 196, 48]" = torch.ops.aten.clone.default(expand_81, memory_format = torch.contiguous_format);  expand_81 = None
        view_299: "f32[128, 196, 48]" = torch.ops.aten.reshape.default(clone_161, [128, 196, 48]);  clone_161 = None
        
         # File: /home/sahanp/.conda/envs/pytorch-benchmarks/lib/python3.12/site-packages/timm/models/convit.py:81 in get_attention, code: q, k = qk[0], qk[1]
        select_102: "f32[8, 16, 196, 48]" = torch.ops.aten.select.int(permute_128, 0, 1);  permute_128 = None
        
         # File: /home/sahanp/.conda/envs/pytorch-benchmarks/lib/python3.12/site-packages/timm/models/convit.py:84 in get_attention, code: patch_score = (q @ k.transpose(-2, -1)) * self.scale
        permute_131: "f32[8, 16, 48, 196]" = torch.ops.aten.permute.default(select_102, [0, 1, 3, 2]);  select_102 = None
        expand_82: "f32[8, 16, 48, 196]" = torch.ops.aten.expand.default(permute_131, [8, 16, 48, 196]);  permute_131 = None
        clone_162: "f32[8, 16, 48, 196]" = torch.ops.aten.clone.default(expand_82, memory_format = torch.contiguous_format);  expand_82 = None
        view_300: "f32[128, 48, 196]" = torch.ops.aten.reshape.default(clone_162, [128, 48, 196]);  clone_162 = None
        bmm_24: "f32[128, 196, 196]" = torch.ops.aten.bmm.default(view_299, view_300);  view_299 = view_300 = None
        view_301: "f32[8, 16, 196, 196]" = torch.ops.aten.reshape.default(bmm_24, [8, 16, 196, 196]);  bmm_24 = None
        
        # No stacktrace found for following nodes
        mul_tensor_22: "f32[8, 16, 196, 196]" = torch.ops.aten.mul.Tensor(view_301, 1);  view_301 = None
        amax_default_11: "f32[8, 16, 196, 1]" = torch.ops.aten.amax.default(mul_tensor_22, [-1], True)
        sub_tensor_11: "f32[8, 16, 196, 196]" = torch.ops.aten.sub.Tensor(mul_tensor_22, amax_default_11);  mul_tensor_22 = amax_default_11 = None
        mul_tensor_23: "f32[8, 16, 196, 196]" = torch.ops.aten.mul.Tensor(sub_tensor_11, 0.14433756729740643);  sub_tensor_11 = None
        
         # File: /home/sahanp/.conda/envs/pytorch-benchmarks/lib/python3.12/site-packages/timm/models/convit.py:85 in get_attention, code: patch_score = patch_score.softmax(dim=-1)
        exp_22: "f32[8, 16, 196, 196]" = torch.ops.aten.exp.default(mul_tensor_23);  mul_tensor_23 = None
        sum_33: "f32[8, 16, 196, 1]" = torch.ops.aten.sum.dim_IntList(exp_22, [-1], True)
        div_32: "f32[8, 16, 196, 196]" = torch.ops.aten.div.Tensor(exp_22, sum_33);  exp_22 = sum_33 = None
        
         # File: /home/sahanp/.conda/envs/pytorch-benchmarks/lib/python3.12/site-packages/timm/models/convit.py:89 in get_attention, code: attn = (1. - torch.sigmoid(gating)) * patch_score + torch.sigmoid(gating) * pos_score
        mul_121: "f32[8, 16, 196, 196]" = torch.ops.aten.mul.Tensor(sub_70, div_32);  sub_70 = div_32 = None
        sigmoid_21: "f32[1, 16, 1, 1]" = torch.ops.aten.sigmoid.default(view_302);  view_302 = None
        
         # File: /home/sahanp/.conda/envs/pytorch-benchmarks/lib/python3.12/site-packages/timm/models/convit.py:119 in get_rel_indices, code: rel_indices = torch.zeros(1, num_patches, num_patches, 3)
        full_default: "f32[1, 196, 196, 3]" = torch.ops.aten.full.default([1, 196, 196, 3], 0, dtype = torch.float32, layout = torch.strided, device = device(type='cpu'), pin_memory = False)
        
         # File: /home/sahanp/.conda/envs/pytorch-benchmarks/lib/python3.12/site-packages/timm/models/convit.py:124 in get_rel_indices, code: rel_indices[:, :, :, 2] = indd.unsqueeze(0)
        select: "f32[1, 196, 196]" = torch.ops.aten.select.int(full_default, 3, 2)
        
         # File: /home/sahanp/.conda/envs/pytorch-benchmarks/lib/python3.12/site-packages/timm/models/convit.py:120 in get_rel_indices, code: ind = torch.arange(img_size).view(1, -1) - torch.arange(img_size).view(-1, 1)
        iota: "i64[14]" = torch.ops.prims.iota.default(14, start = 0, step = 1, dtype = torch.int64, device = device(type='cpu'), requires_grad = False)
        view_1: "i64[1, 14]" = torch.ops.aten.reshape.default(iota, [1, -1]);  iota = None
        iota_1: "i64[14]" = torch.ops.prims.iota.default(14, start = 0, step = 1, dtype = torch.int64, device = device(type='cpu'), requires_grad = False)
        view_2: "i64[14, 1]" = torch.ops.aten.reshape.default(iota_1, [-1, 1]);  iota_1 = None
        sub_1: "i64[14, 14]" = torch.ops.aten.sub.Tensor(view_1, view_2);  view_1 = view_2 = None
        
         # File: /home/sahanp/.conda/envs/pytorch-benchmarks/lib/python3.12/site-packages/timm/models/convit.py:121 in get_rel_indices, code: indx = ind.repeat(img_size, img_size)
        repeat: "i64[196, 196]" = torch.ops.aten.repeat.default(sub_1, [14, 14])
        
         # File: /home/sahanp/.conda/envs/pytorch-benchmarks/lib/python3.12/site-packages/timm/models/convit.py:123 in get_rel_indices, code: indd = indx ** 2 + indy ** 2
        pow_1: "i64[196, 196]" = torch.ops.aten.pow.Tensor_Scalar(repeat, 2)
        
         # File: /home/sahanp/.conda/envs/pytorch-benchmarks/lib/python3.12/site-packages/timm/models/convit.py:122 in get_rel_indices, code: indy = ind.repeat_interleave(img_size, dim=0).repeat_interleave(img_size, dim=1)
        unsqueeze: "i64[14, 1, 14]" = torch.ops.aten.unsqueeze.default(sub_1, 1);  sub_1 = None
        expand_1: "i64[14, 14, 14]" = torch.ops.aten.expand.default(unsqueeze, [14, 14, 14]);  unsqueeze = None
        clone_2: "i64[14, 14, 14]" = torch.ops.aten.clone.default(expand_1, memory_format = torch.contiguous_format);  expand_1 = None
        view_3: "i64[196, 14]" = torch.ops.aten.reshape.default(clone_2, [196, 14]);  clone_2 = None
        unsqueeze_1: "i64[196, 14, 1]" = torch.ops.aten.unsqueeze.default(view_3, 2);  view_3 = None
        expand_2: "i64[196, 14, 14]" = torch.ops.aten.expand.default(unsqueeze_1, [196, 14, 14]);  unsqueeze_1 = None
        clone_3: "i64[196, 14, 14]" = torch.ops.aten.clone.default(expand_2, memory_format = torch.contiguous_format);  expand_2 = None
        view_4: "i64[196, 196]" = torch.ops.aten.reshape.default(clone_3, [196, 196]);  clone_3 = None
        
         # File: /home/sahanp/.conda/envs/pytorch-benchmarks/lib/python3.12/site-packages/timm/models/convit.py:123 in get_rel_indices, code: indd = indx ** 2 + indy ** 2
        pow_2: "i64[196, 196]" = torch.ops.aten.pow.Tensor_Scalar(view_4, 2)
        add_3: "i64[196, 196]" = torch.ops.aten.add.Tensor(pow_1, pow_2);  pow_1 = pow_2 = None
        
         # File: /home/sahanp/.conda/envs/pytorch-benchmarks/lib/python3.12/site-packages/timm/models/convit.py:124 in get_rel_indices, code: rel_indices[:, :, :, 2] = indd.unsqueeze(0)
        unsqueeze_2: "i64[1, 196, 196]" = torch.ops.aten.unsqueeze.default(add_3, 0);  add_3 = None
        copy: "f32[1, 196, 196]" = torch.ops.aten.copy.default(select, unsqueeze_2);  select = unsqueeze_2 = None
        
        # No stacktrace found for following nodes
        select_scatter_default: "f32[1, 196, 196, 3]" = torch.ops.aten.select_scatter.default(full_default, copy, 3, 2);  full_default = copy = None
        
         # File: /home/sahanp/.conda/envs/pytorch-benchmarks/lib/python3.12/site-packages/timm/models/convit.py:125 in get_rel_indices, code: rel_indices[:, :, :, 1] = indy.unsqueeze(0)
        select_3: "f32[1, 196, 196]" = torch.ops.aten.select.int(select_scatter_default, 3, 1)
        unsqueeze_3: "i64[1, 196, 196]" = torch.ops.aten.unsqueeze.default(view_4, 0);  view_4 = None
        copy_1: "f32[1, 196, 196]" = torch.ops.aten.copy.default(select_3, unsqueeze_3);  select_3 = unsqueeze_3 = None
        
        # No stacktrace found for following nodes
        select_scatter_default_1: "f32[1, 196, 196, 3]" = torch.ops.aten.select_scatter.default(select_scatter_default, copy_1, 3, 1);  select_scatter_default = copy_1 = None
        
         # File: /home/sahanp/.conda/envs/pytorch-benchmarks/lib/python3.12/site-packages/timm/models/convit.py:126 in get_rel_indices, code: rel_indices[:, :, :, 0] = indx.unsqueeze(0)
        select_6: "f32[1, 196, 196]" = torch.ops.aten.select.int(select_scatter_default_1, 3, 0)
        unsqueeze_4: "i64[1, 196, 196]" = torch.ops.aten.unsqueeze.default(repeat, 0);  repeat = None
        copy_2: "f32[1, 196, 196]" = torch.ops.aten.copy.default(select_6, unsqueeze_4);  select_6 = unsqueeze_4 = None
        
        # No stacktrace found for following nodes
        select_scatter_default_2: "f32[1, 196, 196, 3]" = torch.ops.aten.select_scatter.default(select_scatter_default_1, copy_2, 3, 0);  select_scatter_default_1 = copy_2 = None
        
         # File: /home/sahanp/.conda/envs/pytorch-benchmarks/lib/python3.12/site-packages/timm/models/convit.py:128 in get_rel_indices, code: return rel_indices.to(device)
        device_put: "f32[1, 196, 196, 3]" = torch.ops.prims.device_put.default(select_scatter_default_2, device(type='cuda', index=0));  select_scatter_default_2 = None
        
         # File: /home/sahanp/.conda/envs/pytorch-benchmarks/lib/python3.12/site-packages/timm/models/convit.py:82 in get_attention, code: pos_score = self.rel_indices.expand(B, -1, -1, -1)
        expand_80: "f32[8, 196, 196, 3]" = torch.ops.aten.expand.default(device_put, [8, -1, -1, -1])
        
         # File: /home/sahanp/.conda/envs/pytorch-benchmarks/lib/python3.12/site-packages/timm/models/convit.py:83 in get_attention, code: pos_score = self.pos_proj(pos_score).permute(0, 3, 1, 2)
        clone_160: "f32[8, 196, 196, 3]" = torch.ops.aten.clone.default(expand_80, memory_format = torch.contiguous_format);  expand_80 = None
        view_297: "f32[307328, 3]" = torch.ops.aten.reshape.default(clone_160, [307328, 3]);  clone_160 = None
        permute_129: "f32[3, 16]" = torch.ops.aten.permute.default(arg8_1, [1, 0]);  arg8_1 = None
        mm_33: "f32[307328, 16]" = torch.ops.aten.mm.default(view_297, permute_129);  view_297 = permute_129 = None
        view_298: "f32[8, 196, 196, 16]" = torch.ops.aten.reshape.default(mm_33, [8, 196, 196, 16]);  mm_33 = None
        add_120: "f32[8, 196, 196, 16]" = torch.ops.aten.add.Tensor(view_298, arg9_1);  view_298 = arg9_1 = None
        permute_130: "f32[8, 16, 196, 196]" = torch.ops.aten.permute.default(add_120, [0, 3, 1, 2]);  add_120 = None
        
         # File: /home/sahanp/.conda/envs/pytorch-benchmarks/lib/python3.12/site-packages/timm/models/convit.py:86 in get_attention, code: pos_score = pos_score.softmax(dim=-1)
        clone_163: "f32[8, 16, 196, 196]" = torch.ops.aten.clone.default(permute_130, memory_format = torch.contiguous_format);  permute_130 = None
        amax_23: "f32[8, 16, 196, 1]" = torch.ops.aten.amax.default(clone_163, [-1], True)
        sub_69: "f32[8, 16, 196, 196]" = torch.ops.aten.sub.Tensor(clone_163, amax_23);  clone_163 = amax_23 = None
        exp_23: "f32[8, 16, 196, 196]" = torch.ops.aten.exp.default(sub_69);  sub_69 = None
        sum_34: "f32[8, 16, 196, 1]" = torch.ops.aten.sum.dim_IntList(exp_23, [-1], True)
        div_33: "f32[8, 16, 196, 196]" = torch.ops.aten.div.Tensor(exp_23, sum_34);  exp_23 = sum_34 = None
        
         # File: /home/sahanp/.conda/envs/pytorch-benchmarks/lib/python3.12/site-packages/timm/models/convit.py:89 in get_attention, code: attn = (1. - torch.sigmoid(gating)) * patch_score + torch.sigmoid(gating) * pos_score
        mul_122: "f32[8, 16, 196, 196]" = torch.ops.aten.mul.Tensor(sigmoid_21, div_33);  sigmoid_21 = div_33 = None
        add_121: "f32[8, 16, 196, 196]" = torch.ops.aten.add.Tensor(mul_121, mul_122);  mul_121 = mul_122 = None
        
         # File: /home/sahanp/.conda/envs/pytorch-benchmarks/lib/python3.12/site-packages/timm/models/convit.py:90 in get_attention, code: attn /= attn.sum(dim=-1).unsqueeze(-1)
        sum_35: "f32[8, 16, 196]" = torch.ops.aten.sum.dim_IntList(add_121, [-1])
        unsqueeze_60: "f32[8, 16, 196, 1]" = torch.ops.aten.unsqueeze.default(sum_35, -1);  sum_35 = None
        div_34: "f32[8, 16, 196, 196]" = torch.ops.aten.div.Tensor(add_121, unsqueeze_60);  add_121 = unsqueeze_60 = None
        
         # File: /home/sahanp/.conda/envs/pytorch-benchmarks/lib/python3.12/site-packages/timm/models/convit.py:73 in forward, code: x = (attn @ v).transpose(1, 2).reshape(B, N, C)
        expand_83: "f32[8, 16, 196, 196]" = torch.ops.aten.expand.default(div_34, [8, 16, 196, 196]);  div_34 = None
        view_306: "f32[128, 196, 196]" = torch.ops.aten.reshape.default(expand_83, [128, 196, 196]);  expand_83 = None
        
         # File: /home/sahanp/.conda/envs/pytorch-benchmarks/lib/python3.12/site-packages/timm/models/convit.py:72 in forward, code: v = self.v(x).reshape(B, N, self.num_heads, C // self.num_heads).permute(0, 2, 1, 3)
        view_303: "f32[1568, 768]" = torch.ops.aten.reshape.default(add_119, [1568, 768]);  add_119 = None
        permute_132: "f32[768, 768]" = torch.ops.aten.permute.default(arg11_1, [1, 0]);  arg11_1 = None
        mm_34: "f32[1568, 768]" = torch.ops.aten.mm.default(view_303, permute_132);  view_303 = permute_132 = None
        view_304: "f32[8, 196, 768]" = torch.ops.aten.reshape.default(mm_34, [8, 196, 768]);  mm_34 = None
        view_305: "f32[8, 196, 16, 48]" = torch.ops.aten.reshape.default(view_304, [8, 196, 16, 48]);  view_304 = None
        permute_133: "f32[8, 16, 196, 48]" = torch.ops.aten.permute.default(view_305, [0, 2, 1, 3]);  view_305 = None
        
         # File: /home/sahanp/.conda/envs/pytorch-benchmarks/lib/python3.12/site-packages/timm/models/convit.py:73 in forward, code: x = (attn @ v).transpose(1, 2).reshape(B, N, C)
        expand_84: "f32[8, 16, 196, 48]" = torch.ops.aten.expand.default(permute_133, [8, 16, 196, 48]);  permute_133 = None
        clone_165: "f32[8, 16, 196, 48]" = torch.ops.aten.clone.default(expand_84, memory_format = torch.contiguous_format);  expand_84 = None
        view_307: "f32[128, 196, 48]" = torch.ops.aten.reshape.default(clone_165, [128, 196, 48]);  clone_165 = None
        bmm_25: "f32[128, 196, 48]" = torch.ops.aten.bmm.default(view_306, view_307);  view_306 = view_307 = None
        view_308: "f32[8, 16, 196, 48]" = torch.ops.aten.reshape.default(bmm_25, [8, 16, 196, 48]);  bmm_25 = None
        permute_134: "f32[8, 196, 16, 48]" = torch.ops.aten.permute.default(view_308, [0, 2, 1, 3]);  view_308 = None
        clone_166: "f32[8, 196, 16, 48]" = torch.ops.aten.clone.default(permute_134, memory_format = torch.contiguous_format);  permute_134 = None
        view_309: "f32[8, 196, 768]" = torch.ops.aten.reshape.default(clone_166, [8, 196, 768]);  clone_166 = None
        
         # File: /home/sahanp/.conda/envs/pytorch-benchmarks/lib/python3.12/site-packages/timm/models/convit.py:74 in forward, code: x = self.proj(x)
        view_310: "f32[1568, 768]" = torch.ops.aten.reshape.default(view_309, [1568, 768]);  view_309 = None
        permute_135: "f32[768, 768]" = torch.ops.aten.permute.default(arg12_1, [1, 0]);  arg12_1 = None
        
        # No stacktrace found for following nodes
        mm_default_35: "f32[1568, 768]" = torch.ops.aten.mm.default(view_310, permute_135);  view_310 = permute_135 = None
        add_tensor_35: "f32[1568, 768]" = torch.ops.aten.add.Tensor(mm_default_35, arg13_1);  mm_default_35 = arg13_1 = None
        
         # File: /home/sahanp/.conda/envs/pytorch-benchmarks/lib/python3.12/site-packages/timm/models/convit.py:74 in forward, code: x = self.proj(x)
        view_311: "f32[8, 196, 768]" = torch.ops.aten.reshape.default(add_tensor_35, [8, 196, 768]);  add_tensor_35 = None
        
         # File: /home/sahanp/.conda/envs/pytorch-benchmarks/lib/python3.12/site-packages/timm/models/convit.py:233 in forward, code: x = x + self.drop_path(self.attn(self.norm1(x)))
        add_122: "f32[8, 196, 768]" = torch.ops.aten.add.Tensor(add_117, view_311);  add_117 = view_311 = None
        
         # File: /home/sahanp/.conda/envs/pytorch-benchmarks/lib/python3.12/site-packages/timm/layers/norm.py:57 in forward, code: x = F.layer_norm(x, self.normalized_shape, self.weight, self.bias, self.eps)
        clone_168: "f32[8, 196, 768]" = torch.ops.aten.clone.default(add_122, memory_format = torch.contiguous_format)
        var_mean_26 = torch.ops.aten.var_mean.correction(clone_168, [2], correction = 0, keepdim = True)
        getitem_58: "f32[8, 196, 1]" = var_mean_26[0]
        getitem_59: "f32[8, 196, 1]" = var_mean_26[1];  var_mean_26 = None
        sub_71: "f32[8, 196, 768]" = torch.ops.aten.sub.Tensor(clone_168, getitem_59);  clone_168 = getitem_59 = None
        add_123: "f32[8, 196, 1]" = torch.ops.aten.add.Tensor(getitem_58, 1e-06);  getitem_58 = None
        rsqrt_26: "f32[8, 196, 1]" = torch.ops.aten.rsqrt.default(add_123);  add_123 = None
        mul_123: "f32[8, 196, 768]" = torch.ops.aten.mul.Tensor(sub_71, rsqrt_26);  sub_71 = rsqrt_26 = None
        mul_124: "f32[8, 196, 768]" = torch.ops.aten.mul.Tensor(mul_123, arg14_1);  mul_123 = arg14_1 = None
        add_124: "f32[8, 196, 768]" = torch.ops.aten.add.Tensor(mul_124, arg15_1);  mul_124 = arg15_1 = None
        
         # File: /home/sahanp/.conda/envs/pytorch-benchmarks/lib/python3.12/site-packages/timm/layers/mlp.py:42 in forward, code: x = self.fc1(x)
        view_312: "f32[1568, 768]" = torch.ops.aten.reshape.default(add_124, [1568, 768]);  add_124 = None
        permute_136: "f32[768, 3072]" = torch.ops.aten.permute.default(arg16_1, [1, 0]);  arg16_1 = None
        
        # No stacktrace found for following nodes
        mm_default_34: "f32[1568, 3072]" = torch.ops.aten.mm.default(view_312, permute_136);  view_312 = permute_136 = None
        add_tensor_34: "f32[1568, 3072]" = torch.ops.aten.add.Tensor(mm_default_34, arg17_1);  mm_default_34 = arg17_1 = None
        
         # File: /home/sahanp/.conda/envs/pytorch-benchmarks/lib/python3.12/site-packages/timm/layers/mlp.py:42 in forward, code: x = self.fc1(x)
        view_313: "f32[8, 196, 3072]" = torch.ops.aten.reshape.default(add_tensor_34, [8, 196, 3072]);  add_tensor_34 = None
        
         # File: /home/sahanp/.conda/envs/pytorch-benchmarks/lib/python3.12/site-packages/timm/layers/mlp.py:43 in forward, code: x = self.act(x)
        mul_125: "f32[8, 196, 3072]" = torch.ops.aten.mul.Tensor(view_313, 0.5)
        mul_126: "f32[8, 196, 3072]" = torch.ops.aten.mul.Tensor(view_313, 0.7071067811865476);  view_313 = None
        erf_12: "f32[8, 196, 3072]" = torch.ops.aten.erf.default(mul_126);  mul_126 = None
        add_125: "f32[8, 196, 3072]" = torch.ops.aten.add.Tensor(erf_12, 1);  erf_12 = None
        mul_127: "f32[8, 196, 3072]" = torch.ops.aten.mul.Tensor(mul_125, add_125);  mul_125 = add_125 = None
        
         # File: /home/sahanp/.conda/envs/pytorch-benchmarks/lib/python3.12/site-packages/timm/layers/mlp.py:46 in forward, code: x = self.fc2(x)
        view_314: "f32[1568, 3072]" = torch.ops.aten.reshape.default(mul_127, [1568, 3072]);  mul_127 = None
        permute_137: "f32[3072, 768]" = torch.ops.aten.permute.default(arg18_1, [1, 0]);  arg18_1 = None
        
        # No stacktrace found for following nodes
        mm_default_33: "f32[1568, 768]" = torch.ops.aten.mm.default(view_314, permute_137);  view_314 = permute_137 = None
        add_tensor_33: "f32[1568, 768]" = torch.ops.aten.add.Tensor(mm_default_33, arg19_1);  mm_default_33 = arg19_1 = None
        
         # File: /home/sahanp/.conda/envs/pytorch-benchmarks/lib/python3.12/site-packages/timm/layers/mlp.py:46 in forward, code: x = self.fc2(x)
        view_315: "f32[8, 196, 768]" = torch.ops.aten.reshape.default(add_tensor_33, [8, 196, 768]);  add_tensor_33 = None
        
         # File: /home/sahanp/.conda/envs/pytorch-benchmarks/lib/python3.12/site-packages/timm/models/convit.py:234 in forward, code: x = x + self.drop_path(self.mlp(self.norm2(x)))
        add_126: "f32[8, 196, 768]" = torch.ops.aten.add.Tensor(add_122, view_315);  add_122 = view_315 = None
        
         # File: /home/sahanp/.conda/envs/pytorch-benchmarks/lib/python3.12/site-packages/timm/layers/norm.py:57 in forward, code: x = F.layer_norm(x, self.normalized_shape, self.weight, self.bias, self.eps)
        clone_171: "f32[8, 196, 768]" = torch.ops.aten.clone.default(add_126, memory_format = torch.contiguous_format)
        var_mean_27 = torch.ops.aten.var_mean.correction(clone_171, [2], correction = 0, keepdim = True)
        getitem_60: "f32[8, 196, 1]" = var_mean_27[0]
        getitem_61: "f32[8, 196, 1]" = var_mean_27[1];  var_mean_27 = None
        
         # File: /home/sahanp/.conda/envs/pytorch-benchmarks/lib/python3.12/site-packages/timm/models/convit.py:88 in get_attention, code: gating = self.gating_param.view(1, -1, 1, 1)
        view_324: "f32[1, 16, 1, 1]" = torch.ops.aten.reshape.default(arg25_1, [1, -1, 1, 1]);  arg25_1 = None
        
         # File: /home/sahanp/.conda/envs/pytorch-benchmarks/lib/python3.12/site-packages/timm/models/convit.py:89 in get_attention, code: attn = (1. - torch.sigmoid(gating)) * patch_score + torch.sigmoid(gating) * pos_score
        sigmoid_22: "f32[1, 16, 1, 1]" = torch.ops.aten.sigmoid.default(view_324)
        sub_75: "f32[1, 16, 1, 1]" = torch.ops.aten.sub.Tensor(1.0, sigmoid_22);  sigmoid_22 = None
        
         # File: /home/sahanp/.conda/envs/pytorch-benchmarks/lib/python3.12/site-packages/timm/layers/norm.py:57 in forward, code: x = F.layer_norm(x, self.normalized_shape, self.weight, self.bias, self.eps)
        sub_72: "f32[8, 196, 768]" = torch.ops.aten.sub.Tensor(clone_171, getitem_61);  clone_171 = getitem_61 = None
        add_127: "f32[8, 196, 1]" = torch.ops.aten.add.Tensor(getitem_60, 1e-06);  getitem_60 = None
        rsqrt_27: "f32[8, 196, 1]" = torch.ops.aten.rsqrt.default(add_127);  add_127 = None
        mul_128: "f32[8, 196, 768]" = torch.ops.aten.mul.Tensor(sub_72, rsqrt_27);  sub_72 = rsqrt_27 = None
        mul_129: "f32[8, 196, 768]" = torch.ops.aten.mul.Tensor(mul_128, arg20_1);  mul_128 = arg20_1 = None
        add_128: "f32[8, 196, 768]" = torch.ops.aten.add.Tensor(mul_129, arg21_1);  mul_129 = arg21_1 = None
        
         # File: /home/sahanp/.conda/envs/pytorch-benchmarks/lib/python3.12/site-packages/timm/models/convit.py:80 in get_attention, code: qk = self.qk(x).reshape(B, N, 2, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)
        view_316: "f32[1568, 768]" = torch.ops.aten.reshape.default(add_128, [1568, 768])
        permute_138: "f32[768, 1536]" = torch.ops.aten.permute.default(arg22_1, [1, 0]);  arg22_1 = None
        mm_35: "f32[1568, 1536]" = torch.ops.aten.mm.default(view_316, permute_138);  view_316 = permute_138 = None
        view_317: "f32[8, 196, 1536]" = torch.ops.aten.reshape.default(mm_35, [8, 196, 1536]);  mm_35 = None
        view_318: "f32[8, 196, 2, 16, 48]" = torch.ops.aten.reshape.default(view_317, [8, 196, 2, 16, 48]);  view_317 = None
        permute_139: "f32[2, 8, 16, 196, 48]" = torch.ops.aten.permute.default(view_318, [2, 0, 3, 1, 4]);  view_318 = None
        
         # File: /home/sahanp/.conda/envs/pytorch-benchmarks/lib/python3.12/site-packages/timm/models/convit.py:81 in get_attention, code: q, k = qk[0], qk[1]
        select_103: "f32[8, 16, 196, 48]" = torch.ops.aten.select.int(permute_139, 0, 0)
        
         # File: /home/sahanp/.conda/envs/pytorch-benchmarks/lib/python3.12/site-packages/timm/models/convit.py:84 in get_attention, code: patch_score = (q @ k.transpose(-2, -1)) * self.scale
        expand_86: "f32[8, 16, 196, 48]" = torch.ops.aten.expand.default(select_103, [8, 16, 196, 48]);  select_103 = None
        clone_173: "f32[8, 16, 196, 48]" = torch.ops.aten.clone.default(expand_86, memory_format = torch.contiguous_format);  expand_86 = None
        view_321: "f32[128, 196, 48]" = torch.ops.aten.reshape.default(clone_173, [128, 196, 48]);  clone_173 = None
        
         # File: /home/sahanp/.conda/envs/pytorch-benchmarks/lib/python3.12/site-packages/timm/models/convit.py:81 in get_attention, code: q, k = qk[0], qk[1]
        select_104: "f32[8, 16, 196, 48]" = torch.ops.aten.select.int(permute_139, 0, 1);  permute_139 = None
        
         # File: /home/sahanp/.conda/envs/pytorch-benchmarks/lib/python3.12/site-packages/timm/models/convit.py:84 in get_attention, code: patch_score = (q @ k.transpose(-2, -1)) * self.scale
        permute_142: "f32[8, 16, 48, 196]" = torch.ops.aten.permute.default(select_104, [0, 1, 3, 2]);  select_104 = None
        expand_87: "f32[8, 16, 48, 196]" = torch.ops.aten.expand.default(permute_142, [8, 16, 48, 196]);  permute_142 = None
        clone_174: "f32[8, 16, 48, 196]" = torch.ops.aten.clone.default(expand_87, memory_format = torch.contiguous_format);  expand_87 = None
        view_322: "f32[128, 48, 196]" = torch.ops.aten.reshape.default(clone_174, [128, 48, 196]);  clone_174 = None
        bmm_26: "f32[128, 196, 196]" = torch.ops.aten.bmm.default(view_321, view_322);  view_321 = view_322 = None
        view_323: "f32[8, 16, 196, 196]" = torch.ops.aten.reshape.default(bmm_26, [8, 16, 196, 196]);  bmm_26 = None
        
        # No stacktrace found for following nodes
        mul_tensor_20: "f32[8, 16, 196, 196]" = torch.ops.aten.mul.Tensor(view_323, 1);  view_323 = None
        amax_default_10: "f32[8, 16, 196, 1]" = torch.ops.aten.amax.default(mul_tensor_20, [-1], True)
        sub_tensor_10: "f32[8, 16, 196, 196]" = torch.ops.aten.sub.Tensor(mul_tensor_20, amax_default_10);  mul_tensor_20 = amax_default_10 = None
        mul_tensor_21: "f32[8, 16, 196, 196]" = torch.ops.aten.mul.Tensor(sub_tensor_10, 0.14433756729740643);  sub_tensor_10 = None
        
         # File: /home/sahanp/.conda/envs/pytorch-benchmarks/lib/python3.12/site-packages/timm/models/convit.py:85 in get_attention, code: patch_score = patch_score.softmax(dim=-1)
        exp_24: "f32[8, 16, 196, 196]" = torch.ops.aten.exp.default(mul_tensor_21);  mul_tensor_21 = None
        sum_36: "f32[8, 16, 196, 1]" = torch.ops.aten.sum.dim_IntList(exp_24, [-1], True)
        div_35: "f32[8, 16, 196, 196]" = torch.ops.aten.div.Tensor(exp_24, sum_36);  exp_24 = sum_36 = None
        
         # File: /home/sahanp/.conda/envs/pytorch-benchmarks/lib/python3.12/site-packages/timm/models/convit.py:89 in get_attention, code: attn = (1. - torch.sigmoid(gating)) * patch_score + torch.sigmoid(gating) * pos_score
        mul_131: "f32[8, 16, 196, 196]" = torch.ops.aten.mul.Tensor(sub_75, div_35);  sub_75 = div_35 = None
        sigmoid_23: "f32[1, 16, 1, 1]" = torch.ops.aten.sigmoid.default(view_324);  view_324 = None
        
         # File: /home/sahanp/.conda/envs/pytorch-benchmarks/lib/python3.12/site-packages/timm/models/convit.py:119 in get_rel_indices, code: rel_indices = torch.zeros(1, num_patches, num_patches, 3)
        full_default_1: "f32[1, 196, 196, 3]" = torch.ops.aten.full.default([1, 196, 196, 3], 0, dtype = torch.float32, layout = torch.strided, device = device(type='cpu'), pin_memory = False)
        
         # File: /home/sahanp/.conda/envs/pytorch-benchmarks/lib/python3.12/site-packages/timm/models/convit.py:124 in get_rel_indices, code: rel_indices[:, :, :, 2] = indd.unsqueeze(0)
        select_10: "f32[1, 196, 196]" = torch.ops.aten.select.int(full_default_1, 3, 2)
        
         # File: /home/sahanp/.conda/envs/pytorch-benchmarks/lib/python3.12/site-packages/timm/models/convit.py:120 in get_rel_indices, code: ind = torch.arange(img_size).view(1, -1) - torch.arange(img_size).view(-1, 1)
        iota_2: "i64[14]" = torch.ops.prims.iota.default(14, start = 0, step = 1, dtype = torch.int64, device = device(type='cpu'), requires_grad = False)
        view_27: "i64[1, 14]" = torch.ops.aten.reshape.default(iota_2, [1, -1]);  iota_2 = None
        iota_3: "i64[14]" = torch.ops.prims.iota.default(14, start = 0, step = 1, dtype = torch.int64, device = device(type='cpu'), requires_grad = False)
        view_28: "i64[14, 1]" = torch.ops.aten.reshape.default(iota_3, [-1, 1]);  iota_3 = None
        sub_7: "i64[14, 14]" = torch.ops.aten.sub.Tensor(view_27, view_28);  view_27 = view_28 = None
        
         # File: /home/sahanp/.conda/envs/pytorch-benchmarks/lib/python3.12/site-packages/timm/models/convit.py:121 in get_rel_indices, code: indx = ind.repeat(img_size, img_size)
        repeat_1: "i64[196, 196]" = torch.ops.aten.repeat.default(sub_7, [14, 14])
        
         # File: /home/sahanp/.conda/envs/pytorch-benchmarks/lib/python3.12/site-packages/timm/models/convit.py:123 in get_rel_indices, code: indd = indx ** 2 + indy ** 2
        pow_3: "i64[196, 196]" = torch.ops.aten.pow.Tensor_Scalar(repeat_1, 2)
        
         # File: /home/sahanp/.conda/envs/pytorch-benchmarks/lib/python3.12/site-packages/timm/models/convit.py:122 in get_rel_indices, code: indy = ind.repeat_interleave(img_size, dim=0).repeat_interleave(img_size, dim=1)
        unsqueeze_6: "i64[14, 1, 14]" = torch.ops.aten.unsqueeze.default(sub_7, 1);  sub_7 = None
        expand_8: "i64[14, 14, 14]" = torch.ops.aten.expand.default(unsqueeze_6, [14, 14, 14]);  unsqueeze_6 = None
        clone_16: "i64[14, 14, 14]" = torch.ops.aten.clone.default(expand_8, memory_format = torch.contiguous_format);  expand_8 = None
        view_29: "i64[196, 14]" = torch.ops.aten.reshape.default(clone_16, [196, 14]);  clone_16 = None
        unsqueeze_7: "i64[196, 14, 1]" = torch.ops.aten.unsqueeze.default(view_29, 2);  view_29 = None
        expand_9: "i64[196, 14, 14]" = torch.ops.aten.expand.default(unsqueeze_7, [196, 14, 14]);  unsqueeze_7 = None
        clone_17: "i64[196, 14, 14]" = torch.ops.aten.clone.default(expand_9, memory_format = torch.contiguous_format);  expand_9 = None
        view_30: "i64[196, 196]" = torch.ops.aten.reshape.default(clone_17, [196, 196]);  clone_17 = None
        
         # File: /home/sahanp/.conda/envs/pytorch-benchmarks/lib/python3.12/site-packages/timm/models/convit.py:123 in get_rel_indices, code: indd = indx ** 2 + indy ** 2
        pow_4: "i64[196, 196]" = torch.ops.aten.pow.Tensor_Scalar(view_30, 2)
        add_13: "i64[196, 196]" = torch.ops.aten.add.Tensor(pow_3, pow_4);  pow_3 = pow_4 = None
        
         # File: /home/sahanp/.conda/envs/pytorch-benchmarks/lib/python3.12/site-packages/timm/models/convit.py:124 in get_rel_indices, code: rel_indices[:, :, :, 2] = indd.unsqueeze(0)
        unsqueeze_8: "i64[1, 196, 196]" = torch.ops.aten.unsqueeze.default(add_13, 0);  add_13 = None
        copy_3: "f32[1, 196, 196]" = torch.ops.aten.copy.default(select_10, unsqueeze_8);  select_10 = unsqueeze_8 = None
        
        # No stacktrace found for following nodes
        select_scatter_default_3: "f32[1, 196, 196, 3]" = torch.ops.aten.select_scatter.default(full_default_1, copy_3, 3, 2);  full_default_1 = copy_3 = None
        
         # File: /home/sahanp/.conda/envs/pytorch-benchmarks/lib/python3.12/site-packages/timm/models/convit.py:125 in get_rel_indices, code: rel_indices[:, :, :, 1] = indy.unsqueeze(0)
        select_13: "f32[1, 196, 196]" = torch.ops.aten.select.int(select_scatter_default_3, 3, 1)
        unsqueeze_9: "i64[1, 196, 196]" = torch.ops.aten.unsqueeze.default(view_30, 0);  view_30 = None
        copy_4: "f32[1, 196, 196]" = torch.ops.aten.copy.default(select_13, unsqueeze_9);  select_13 = unsqueeze_9 = None
        
        # No stacktrace found for following nodes
        select_scatter_default_4: "f32[1, 196, 196, 3]" = torch.ops.aten.select_scatter.default(select_scatter_default_3, copy_4, 3, 1);  select_scatter_default_3 = copy_4 = None
        
         # File: /home/sahanp/.conda/envs/pytorch-benchmarks/lib/python3.12/site-packages/timm/models/convit.py:126 in get_rel_indices, code: rel_indices[:, :, :, 0] = indx.unsqueeze(0)
        select_16: "f32[1, 196, 196]" = torch.ops.aten.select.int(select_scatter_default_4, 3, 0)
        unsqueeze_10: "i64[1, 196, 196]" = torch.ops.aten.unsqueeze.default(repeat_1, 0);  repeat_1 = None
        copy_5: "f32[1, 196, 196]" = torch.ops.aten.copy.default(select_16, unsqueeze_10);  select_16 = unsqueeze_10 = None
        
        # No stacktrace found for following nodes
        select_scatter_default_5: "f32[1, 196, 196, 3]" = torch.ops.aten.select_scatter.default(select_scatter_default_4, copy_5, 3, 0);  select_scatter_default_4 = copy_5 = None
        
         # File: /home/sahanp/.conda/envs/pytorch-benchmarks/lib/python3.12/site-packages/timm/models/convit.py:128 in get_rel_indices, code: return rel_indices.to(device)
        device_put_1: "f32[1, 196, 196, 3]" = torch.ops.prims.device_put.default(select_scatter_default_5, device(type='cuda', index=0));  select_scatter_default_5 = None
        
         # File: /home/sahanp/.conda/envs/pytorch-benchmarks/lib/python3.12/site-packages/timm/models/convit.py:82 in get_attention, code: pos_score = self.rel_indices.expand(B, -1, -1, -1)
        expand_85: "f32[8, 196, 196, 3]" = torch.ops.aten.expand.default(device_put_1, [8, -1, -1, -1])
        
         # File: /home/sahanp/.conda/envs/pytorch-benchmarks/lib/python3.12/site-packages/timm/models/convit.py:83 in get_attention, code: pos_score = self.pos_proj(pos_score).permute(0, 3, 1, 2)
        clone_172: "f32[8, 196, 196, 3]" = torch.ops.aten.clone.default(expand_85, memory_format = torch.contiguous_format);  expand_85 = None
        view_319: "f32[307328, 3]" = torch.ops.aten.reshape.default(clone_172, [307328, 3]);  clone_172 = None
        permute_140: "f32[3, 16]" = torch.ops.aten.permute.default(arg23_1, [1, 0]);  arg23_1 = None
        mm_36: "f32[307328, 16]" = torch.ops.aten.mm.default(view_319, permute_140);  view_319 = permute_140 = None
        view_320: "f32[8, 196, 196, 16]" = torch.ops.aten.reshape.default(mm_36, [8, 196, 196, 16]);  mm_36 = None
        add_129: "f32[8, 196, 196, 16]" = torch.ops.aten.add.Tensor(view_320, arg24_1);  view_320 = arg24_1 = None
        permute_141: "f32[8, 16, 196, 196]" = torch.ops.aten.permute.default(add_129, [0, 3, 1, 2]);  add_129 = None
        
         # File: /home/sahanp/.conda/envs/pytorch-benchmarks/lib/python3.12/site-packages/timm/models/convit.py:86 in get_attention, code: pos_score = pos_score.softmax(dim=-1)
        clone_175: "f32[8, 16, 196, 196]" = torch.ops.aten.clone.default(permute_141, memory_format = torch.contiguous_format);  permute_141 = None
        amax_25: "f32[8, 16, 196, 1]" = torch.ops.aten.amax.default(clone_175, [-1], True)
        sub_74: "f32[8, 16, 196, 196]" = torch.ops.aten.sub.Tensor(clone_175, amax_25);  clone_175 = amax_25 = None
        exp_25: "f32[8, 16, 196, 196]" = torch.ops.aten.exp.default(sub_74);  sub_74 = None
        sum_37: "f32[8, 16, 196, 1]" = torch.ops.aten.sum.dim_IntList(exp_25, [-1], True)
        div_36: "f32[8, 16, 196, 196]" = torch.ops.aten.div.Tensor(exp_25, sum_37);  exp_25 = sum_37 = None
        
         # File: /home/sahanp/.conda/envs/pytorch-benchmarks/lib/python3.12/site-packages/timm/models/convit.py:89 in get_attention, code: attn = (1. - torch.sigmoid(gating)) * patch_score + torch.sigmoid(gating) * pos_score
        mul_132: "f32[8, 16, 196, 196]" = torch.ops.aten.mul.Tensor(sigmoid_23, div_36);  sigmoid_23 = div_36 = None
        add_130: "f32[8, 16, 196, 196]" = torch.ops.aten.add.Tensor(mul_131, mul_132);  mul_131 = mul_132 = None
        
         # File: /home/sahanp/.conda/envs/pytorch-benchmarks/lib/python3.12/site-packages/timm/models/convit.py:90 in get_attention, code: attn /= attn.sum(dim=-1).unsqueeze(-1)
        sum_38: "f32[8, 16, 196]" = torch.ops.aten.sum.dim_IntList(add_130, [-1])
        unsqueeze_61: "f32[8, 16, 196, 1]" = torch.ops.aten.unsqueeze.default(sum_38, -1);  sum_38 = None
        div_37: "f32[8, 16, 196, 196]" = torch.ops.aten.div.Tensor(add_130, unsqueeze_61);  add_130 = unsqueeze_61 = None
        
         # File: /home/sahanp/.conda/envs/pytorch-benchmarks/lib/python3.12/site-packages/timm/models/convit.py:73 in forward, code: x = (attn @ v).transpose(1, 2).reshape(B, N, C)
        expand_88: "f32[8, 16, 196, 196]" = torch.ops.aten.expand.default(div_37, [8, 16, 196, 196]);  div_37 = None
        view_328: "f32[128, 196, 196]" = torch.ops.aten.reshape.default(expand_88, [128, 196, 196]);  expand_88 = None
        
         # File: /home/sahanp/.conda/envs/pytorch-benchmarks/lib/python3.12/site-packages/timm/models/convit.py:72 in forward, code: v = self.v(x).reshape(B, N, self.num_heads, C // self.num_heads).permute(0, 2, 1, 3)
        view_325: "f32[1568, 768]" = torch.ops.aten.reshape.default(add_128, [1568, 768]);  add_128 = None
        permute_143: "f32[768, 768]" = torch.ops.aten.permute.default(arg26_1, [1, 0]);  arg26_1 = None
        mm_37: "f32[1568, 768]" = torch.ops.aten.mm.default(view_325, permute_143);  view_325 = permute_143 = None
        view_326: "f32[8, 196, 768]" = torch.ops.aten.reshape.default(mm_37, [8, 196, 768]);  mm_37 = None
        view_327: "f32[8, 196, 16, 48]" = torch.ops.aten.reshape.default(view_326, [8, 196, 16, 48]);  view_326 = None
        permute_144: "f32[8, 16, 196, 48]" = torch.ops.aten.permute.default(view_327, [0, 2, 1, 3]);  view_327 = None
        
         # File: /home/sahanp/.conda/envs/pytorch-benchmarks/lib/python3.12/site-packages/timm/models/convit.py:73 in forward, code: x = (attn @ v).transpose(1, 2).reshape(B, N, C)
        expand_89: "f32[8, 16, 196, 48]" = torch.ops.aten.expand.default(permute_144, [8, 16, 196, 48]);  permute_144 = None
        clone_177: "f32[8, 16, 196, 48]" = torch.ops.aten.clone.default(expand_89, memory_format = torch.contiguous_format);  expand_89 = None
        view_329: "f32[128, 196, 48]" = torch.ops.aten.reshape.default(clone_177, [128, 196, 48]);  clone_177 = None
        bmm_27: "f32[128, 196, 48]" = torch.ops.aten.bmm.default(view_328, view_329);  view_328 = view_329 = None
        view_330: "f32[8, 16, 196, 48]" = torch.ops.aten.reshape.default(bmm_27, [8, 16, 196, 48]);  bmm_27 = None
        permute_145: "f32[8, 196, 16, 48]" = torch.ops.aten.permute.default(view_330, [0, 2, 1, 3]);  view_330 = None
        clone_178: "f32[8, 196, 16, 48]" = torch.ops.aten.clone.default(permute_145, memory_format = torch.contiguous_format);  permute_145 = None
        view_331: "f32[8, 196, 768]" = torch.ops.aten.reshape.default(clone_178, [8, 196, 768]);  clone_178 = None
        
         # File: /home/sahanp/.conda/envs/pytorch-benchmarks/lib/python3.12/site-packages/timm/models/convit.py:74 in forward, code: x = self.proj(x)
        view_332: "f32[1568, 768]" = torch.ops.aten.reshape.default(view_331, [1568, 768]);  view_331 = None
        permute_146: "f32[768, 768]" = torch.ops.aten.permute.default(arg27_1, [1, 0]);  arg27_1 = None
        
        # No stacktrace found for following nodes
        mm_default_32: "f32[1568, 768]" = torch.ops.aten.mm.default(view_332, permute_146);  view_332 = permute_146 = None
        add_tensor_32: "f32[1568, 768]" = torch.ops.aten.add.Tensor(mm_default_32, arg28_1);  mm_default_32 = arg28_1 = None
        
         # File: /home/sahanp/.conda/envs/pytorch-benchmarks/lib/python3.12/site-packages/timm/models/convit.py:74 in forward, code: x = self.proj(x)
        view_333: "f32[8, 196, 768]" = torch.ops.aten.reshape.default(add_tensor_32, [8, 196, 768]);  add_tensor_32 = None
        
         # File: /home/sahanp/.conda/envs/pytorch-benchmarks/lib/python3.12/site-packages/timm/models/convit.py:233 in forward, code: x = x + self.drop_path(self.attn(self.norm1(x)))
        add_131: "f32[8, 196, 768]" = torch.ops.aten.add.Tensor(add_126, view_333);  add_126 = view_333 = None
        
         # File: /home/sahanp/.conda/envs/pytorch-benchmarks/lib/python3.12/site-packages/timm/layers/norm.py:57 in forward, code: x = F.layer_norm(x, self.normalized_shape, self.weight, self.bias, self.eps)
        clone_180: "f32[8, 196, 768]" = torch.ops.aten.clone.default(add_131, memory_format = torch.contiguous_format)
        var_mean_28 = torch.ops.aten.var_mean.correction(clone_180, [2], correction = 0, keepdim = True)
        getitem_62: "f32[8, 196, 1]" = var_mean_28[0]
        getitem_63: "f32[8, 196, 1]" = var_mean_28[1];  var_mean_28 = None
        sub_76: "f32[8, 196, 768]" = torch.ops.aten.sub.Tensor(clone_180, getitem_63);  clone_180 = getitem_63 = None
        add_132: "f32[8, 196, 1]" = torch.ops.aten.add.Tensor(getitem_62, 1e-06);  getitem_62 = None
        rsqrt_28: "f32[8, 196, 1]" = torch.ops.aten.rsqrt.default(add_132);  add_132 = None
        mul_133: "f32[8, 196, 768]" = torch.ops.aten.mul.Tensor(sub_76, rsqrt_28);  sub_76 = rsqrt_28 = None
        mul_134: "f32[8, 196, 768]" = torch.ops.aten.mul.Tensor(mul_133, arg29_1);  mul_133 = arg29_1 = None
        add_133: "f32[8, 196, 768]" = torch.ops.aten.add.Tensor(mul_134, arg30_1);  mul_134 = arg30_1 = None
        
         # File: /home/sahanp/.conda/envs/pytorch-benchmarks/lib/python3.12/site-packages/timm/layers/mlp.py:42 in forward, code: x = self.fc1(x)
        view_334: "f32[1568, 768]" = torch.ops.aten.reshape.default(add_133, [1568, 768]);  add_133 = None
        permute_147: "f32[768, 3072]" = torch.ops.aten.permute.default(arg31_1, [1, 0]);  arg31_1 = None
        
        # No stacktrace found for following nodes
        mm_default_31: "f32[1568, 3072]" = torch.ops.aten.mm.default(view_334, permute_147);  view_334 = permute_147 = None
        add_tensor_31: "f32[1568, 3072]" = torch.ops.aten.add.Tensor(mm_default_31, arg32_1);  mm_default_31 = arg32_1 = None
        
         # File: /home/sahanp/.conda/envs/pytorch-benchmarks/lib/python3.12/site-packages/timm/layers/mlp.py:42 in forward, code: x = self.fc1(x)
        view_335: "f32[8, 196, 3072]" = torch.ops.aten.reshape.default(add_tensor_31, [8, 196, 3072]);  add_tensor_31 = None
        
         # File: /home/sahanp/.conda/envs/pytorch-benchmarks/lib/python3.12/site-packages/timm/layers/mlp.py:43 in forward, code: x = self.act(x)
        mul_135: "f32[8, 196, 3072]" = torch.ops.aten.mul.Tensor(view_335, 0.5)
        mul_136: "f32[8, 196, 3072]" = torch.ops.aten.mul.Tensor(view_335, 0.7071067811865476);  view_335 = None
        erf_13: "f32[8, 196, 3072]" = torch.ops.aten.erf.default(mul_136);  mul_136 = None
        add_134: "f32[8, 196, 3072]" = torch.ops.aten.add.Tensor(erf_13, 1);  erf_13 = None
        mul_137: "f32[8, 196, 3072]" = torch.ops.aten.mul.Tensor(mul_135, add_134);  mul_135 = add_134 = None
        
         # File: /home/sahanp/.conda/envs/pytorch-benchmarks/lib/python3.12/site-packages/timm/layers/mlp.py:46 in forward, code: x = self.fc2(x)
        view_336: "f32[1568, 3072]" = torch.ops.aten.reshape.default(mul_137, [1568, 3072]);  mul_137 = None
        permute_148: "f32[3072, 768]" = torch.ops.aten.permute.default(arg33_1, [1, 0]);  arg33_1 = None
        
        # No stacktrace found for following nodes
        mm_default_30: "f32[1568, 768]" = torch.ops.aten.mm.default(view_336, permute_148);  view_336 = permute_148 = None
        add_tensor_30: "f32[1568, 768]" = torch.ops.aten.add.Tensor(mm_default_30, arg34_1);  mm_default_30 = arg34_1 = None
        
         # File: /home/sahanp/.conda/envs/pytorch-benchmarks/lib/python3.12/site-packages/timm/layers/mlp.py:46 in forward, code: x = self.fc2(x)
        view_337: "f32[8, 196, 768]" = torch.ops.aten.reshape.default(add_tensor_30, [8, 196, 768]);  add_tensor_30 = None
        
         # File: /home/sahanp/.conda/envs/pytorch-benchmarks/lib/python3.12/site-packages/timm/models/convit.py:234 in forward, code: x = x + self.drop_path(self.mlp(self.norm2(x)))
        add_135: "f32[8, 196, 768]" = torch.ops.aten.add.Tensor(add_131, view_337);  add_131 = view_337 = None
        
         # File: /home/sahanp/.conda/envs/pytorch-benchmarks/lib/python3.12/site-packages/timm/layers/norm.py:57 in forward, code: x = F.layer_norm(x, self.normalized_shape, self.weight, self.bias, self.eps)
        clone_183: "f32[8, 196, 768]" = torch.ops.aten.clone.default(add_135, memory_format = torch.contiguous_format)
        var_mean_29 = torch.ops.aten.var_mean.correction(clone_183, [2], correction = 0, keepdim = True)
        getitem_64: "f32[8, 196, 1]" = var_mean_29[0]
        getitem_65: "f32[8, 196, 1]" = var_mean_29[1];  var_mean_29 = None
        
         # File: /home/sahanp/.conda/envs/pytorch-benchmarks/lib/python3.12/site-packages/timm/models/convit.py:88 in get_attention, code: gating = self.gating_param.view(1, -1, 1, 1)
        view_346: "f32[1, 16, 1, 1]" = torch.ops.aten.reshape.default(arg40_1, [1, -1, 1, 1]);  arg40_1 = None
        
         # File: /home/sahanp/.conda/envs/pytorch-benchmarks/lib/python3.12/site-packages/timm/models/convit.py:89 in get_attention, code: attn = (1. - torch.sigmoid(gating)) * patch_score + torch.sigmoid(gating) * pos_score
        sigmoid_24: "f32[1, 16, 1, 1]" = torch.ops.aten.sigmoid.default(view_346)
        sub_80: "f32[1, 16, 1, 1]" = torch.ops.aten.sub.Tensor(1.0, sigmoid_24);  sigmoid_24 = None
        
         # File: /home/sahanp/.conda/envs/pytorch-benchmarks/lib/python3.12/site-packages/timm/layers/norm.py:57 in forward, code: x = F.layer_norm(x, self.normalized_shape, self.weight, self.bias, self.eps)
        sub_77: "f32[8, 196, 768]" = torch.ops.aten.sub.Tensor(clone_183, getitem_65);  clone_183 = getitem_65 = None
        add_136: "f32[8, 196, 1]" = torch.ops.aten.add.Tensor(getitem_64, 1e-06);  getitem_64 = None
        rsqrt_29: "f32[8, 196, 1]" = torch.ops.aten.rsqrt.default(add_136);  add_136 = None
        mul_138: "f32[8, 196, 768]" = torch.ops.aten.mul.Tensor(sub_77, rsqrt_29);  sub_77 = rsqrt_29 = None
        mul_139: "f32[8, 196, 768]" = torch.ops.aten.mul.Tensor(mul_138, arg35_1);  mul_138 = arg35_1 = None
        add_137: "f32[8, 196, 768]" = torch.ops.aten.add.Tensor(mul_139, arg36_1);  mul_139 = arg36_1 = None
        
         # File: /home/sahanp/.conda/envs/pytorch-benchmarks/lib/python3.12/site-packages/timm/models/convit.py:80 in get_attention, code: qk = self.qk(x).reshape(B, N, 2, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)
        view_338: "f32[1568, 768]" = torch.ops.aten.reshape.default(add_137, [1568, 768])
        permute_149: "f32[768, 1536]" = torch.ops.aten.permute.default(arg37_1, [1, 0]);  arg37_1 = None
        mm_38: "f32[1568, 1536]" = torch.ops.aten.mm.default(view_338, permute_149);  view_338 = permute_149 = None
        view_339: "f32[8, 196, 1536]" = torch.ops.aten.reshape.default(mm_38, [8, 196, 1536]);  mm_38 = None
        view_340: "f32[8, 196, 2, 16, 48]" = torch.ops.aten.reshape.default(view_339, [8, 196, 2, 16, 48]);  view_339 = None
        permute_150: "f32[2, 8, 16, 196, 48]" = torch.ops.aten.permute.default(view_340, [2, 0, 3, 1, 4]);  view_340 = None
        
         # File: /home/sahanp/.conda/envs/pytorch-benchmarks/lib/python3.12/site-packages/timm/models/convit.py:81 in get_attention, code: q, k = qk[0], qk[1]
        select_105: "f32[8, 16, 196, 48]" = torch.ops.aten.select.int(permute_150, 0, 0)
        
         # File: /home/sahanp/.conda/envs/pytorch-benchmarks/lib/python3.12/site-packages/timm/models/convit.py:84 in get_attention, code: patch_score = (q @ k.transpose(-2, -1)) * self.scale
        expand_91: "f32[8, 16, 196, 48]" = torch.ops.aten.expand.default(select_105, [8, 16, 196, 48]);  select_105 = None
        clone_185: "f32[8, 16, 196, 48]" = torch.ops.aten.clone.default(expand_91, memory_format = torch.contiguous_format);  expand_91 = None
        view_343: "f32[128, 196, 48]" = torch.ops.aten.reshape.default(clone_185, [128, 196, 48]);  clone_185 = None
        
         # File: /home/sahanp/.conda/envs/pytorch-benchmarks/lib/python3.12/site-packages/timm/models/convit.py:81 in get_attention, code: q, k = qk[0], qk[1]
        select_106: "f32[8, 16, 196, 48]" = torch.ops.aten.select.int(permute_150, 0, 1);  permute_150 = None
        
         # File: /home/sahanp/.conda/envs/pytorch-benchmarks/lib/python3.12/site-packages/timm/models/convit.py:84 in get_attention, code: patch_score = (q @ k.transpose(-2, -1)) * self.scale
        permute_153: "f32[8, 16, 48, 196]" = torch.ops.aten.permute.default(select_106, [0, 1, 3, 2]);  select_106 = None
        expand_92: "f32[8, 16, 48, 196]" = torch.ops.aten.expand.default(permute_153, [8, 16, 48, 196]);  permute_153 = None
        clone_186: "f32[8, 16, 48, 196]" = torch.ops.aten.clone.default(expand_92, memory_format = torch.contiguous_format);  expand_92 = None
        view_344: "f32[128, 48, 196]" = torch.ops.aten.reshape.default(clone_186, [128, 48, 196]);  clone_186 = None
        bmm_28: "f32[128, 196, 196]" = torch.ops.aten.bmm.default(view_343, view_344);  view_343 = view_344 = None
        view_345: "f32[8, 16, 196, 196]" = torch.ops.aten.reshape.default(bmm_28, [8, 16, 196, 196]);  bmm_28 = None
        
        # No stacktrace found for following nodes
        mul_tensor_18: "f32[8, 16, 196, 196]" = torch.ops.aten.mul.Tensor(view_345, 1);  view_345 = None
        amax_default_9: "f32[8, 16, 196, 1]" = torch.ops.aten.amax.default(mul_tensor_18, [-1], True)
        sub_tensor_9: "f32[8, 16, 196, 196]" = torch.ops.aten.sub.Tensor(mul_tensor_18, amax_default_9);  mul_tensor_18 = amax_default_9 = None
        mul_tensor_19: "f32[8, 16, 196, 196]" = torch.ops.aten.mul.Tensor(sub_tensor_9, 0.14433756729740643);  sub_tensor_9 = None
        
         # File: /home/sahanp/.conda/envs/pytorch-benchmarks/lib/python3.12/site-packages/timm/models/convit.py:85 in get_attention, code: patch_score = patch_score.softmax(dim=-1)
        exp_26: "f32[8, 16, 196, 196]" = torch.ops.aten.exp.default(mul_tensor_19);  mul_tensor_19 = None
        sum_39: "f32[8, 16, 196, 1]" = torch.ops.aten.sum.dim_IntList(exp_26, [-1], True)
        div_38: "f32[8, 16, 196, 196]" = torch.ops.aten.div.Tensor(exp_26, sum_39);  exp_26 = sum_39 = None
        
         # File: /home/sahanp/.conda/envs/pytorch-benchmarks/lib/python3.12/site-packages/timm/models/convit.py:89 in get_attention, code: attn = (1. - torch.sigmoid(gating)) * patch_score + torch.sigmoid(gating) * pos_score
        mul_141: "f32[8, 16, 196, 196]" = torch.ops.aten.mul.Tensor(sub_80, div_38);  sub_80 = div_38 = None
        sigmoid_25: "f32[1, 16, 1, 1]" = torch.ops.aten.sigmoid.default(view_346);  view_346 = None
        
         # File: /home/sahanp/.conda/envs/pytorch-benchmarks/lib/python3.12/site-packages/timm/models/convit.py:119 in get_rel_indices, code: rel_indices = torch.zeros(1, num_patches, num_patches, 3)
        full_default_2: "f32[1, 196, 196, 3]" = torch.ops.aten.full.default([1, 196, 196, 3], 0, dtype = torch.float32, layout = torch.strided, device = device(type='cpu'), pin_memory = False)
        
         # File: /home/sahanp/.conda/envs/pytorch-benchmarks/lib/python3.12/site-packages/timm/models/convit.py:124 in get_rel_indices, code: rel_indices[:, :, :, 2] = indd.unsqueeze(0)
        select_20: "f32[1, 196, 196]" = torch.ops.aten.select.int(full_default_2, 3, 2)
        
         # File: /home/sahanp/.conda/envs/pytorch-benchmarks/lib/python3.12/site-packages/timm/models/convit.py:120 in get_rel_indices, code: ind = torch.arange(img_size).view(1, -1) - torch.arange(img_size).view(-1, 1)
        iota_4: "i64[14]" = torch.ops.prims.iota.default(14, start = 0, step = 1, dtype = torch.int64, device = device(type='cpu'), requires_grad = False)
        view_53: "i64[1, 14]" = torch.ops.aten.reshape.default(iota_4, [1, -1]);  iota_4 = None
        iota_5: "i64[14]" = torch.ops.prims.iota.default(14, start = 0, step = 1, dtype = torch.int64, device = device(type='cpu'), requires_grad = False)
        view_54: "i64[14, 1]" = torch.ops.aten.reshape.default(iota_5, [-1, 1]);  iota_5 = None
        sub_13: "i64[14, 14]" = torch.ops.aten.sub.Tensor(view_53, view_54);  view_53 = view_54 = None
        
         # File: /home/sahanp/.conda/envs/pytorch-benchmarks/lib/python3.12/site-packages/timm/models/convit.py:121 in get_rel_indices, code: indx = ind.repeat(img_size, img_size)
        repeat_2: "i64[196, 196]" = torch.ops.aten.repeat.default(sub_13, [14, 14])
        
         # File: /home/sahanp/.conda/envs/pytorch-benchmarks/lib/python3.12/site-packages/timm/models/convit.py:123 in get_rel_indices, code: indd = indx ** 2 + indy ** 2
        pow_5: "i64[196, 196]" = torch.ops.aten.pow.Tensor_Scalar(repeat_2, 2)
        
         # File: /home/sahanp/.conda/envs/pytorch-benchmarks/lib/python3.12/site-packages/timm/models/convit.py:122 in get_rel_indices, code: indy = ind.repeat_interleave(img_size, dim=0).repeat_interleave(img_size, dim=1)
        unsqueeze_12: "i64[14, 1, 14]" = torch.ops.aten.unsqueeze.default(sub_13, 1);  sub_13 = None
        expand_15: "i64[14, 14, 14]" = torch.ops.aten.expand.default(unsqueeze_12, [14, 14, 14]);  unsqueeze_12 = None
        clone_30: "i64[14, 14, 14]" = torch.ops.aten.clone.default(expand_15, memory_format = torch.contiguous_format);  expand_15 = None
        view_55: "i64[196, 14]" = torch.ops.aten.reshape.default(clone_30, [196, 14]);  clone_30 = None
        unsqueeze_13: "i64[196, 14, 1]" = torch.ops.aten.unsqueeze.default(view_55, 2);  view_55 = None
        expand_16: "i64[196, 14, 14]" = torch.ops.aten.expand.default(unsqueeze_13, [196, 14, 14]);  unsqueeze_13 = None
        clone_31: "i64[196, 14, 14]" = torch.ops.aten.clone.default(expand_16, memory_format = torch.contiguous_format);  expand_16 = None
        view_56: "i64[196, 196]" = torch.ops.aten.reshape.default(clone_31, [196, 196]);  clone_31 = None
        
         # File: /home/sahanp/.conda/envs/pytorch-benchmarks/lib/python3.12/site-packages/timm/models/convit.py:123 in get_rel_indices, code: indd = indx ** 2 + indy ** 2
        pow_6: "i64[196, 196]" = torch.ops.aten.pow.Tensor_Scalar(view_56, 2)
        add_23: "i64[196, 196]" = torch.ops.aten.add.Tensor(pow_5, pow_6);  pow_5 = pow_6 = None
        
         # File: /home/sahanp/.conda/envs/pytorch-benchmarks/lib/python3.12/site-packages/timm/models/convit.py:124 in get_rel_indices, code: rel_indices[:, :, :, 2] = indd.unsqueeze(0)
        unsqueeze_14: "i64[1, 196, 196]" = torch.ops.aten.unsqueeze.default(add_23, 0);  add_23 = None
        copy_6: "f32[1, 196, 196]" = torch.ops.aten.copy.default(select_20, unsqueeze_14);  select_20 = unsqueeze_14 = None
        
        # No stacktrace found for following nodes
        select_scatter_default_6: "f32[1, 196, 196, 3]" = torch.ops.aten.select_scatter.default(full_default_2, copy_6, 3, 2);  full_default_2 = copy_6 = None
        
         # File: /home/sahanp/.conda/envs/pytorch-benchmarks/lib/python3.12/site-packages/timm/models/convit.py:125 in get_rel_indices, code: rel_indices[:, :, :, 1] = indy.unsqueeze(0)
        select_23: "f32[1, 196, 196]" = torch.ops.aten.select.int(select_scatter_default_6, 3, 1)
        unsqueeze_15: "i64[1, 196, 196]" = torch.ops.aten.unsqueeze.default(view_56, 0);  view_56 = None
        copy_7: "f32[1, 196, 196]" = torch.ops.aten.copy.default(select_23, unsqueeze_15);  select_23 = unsqueeze_15 = None
        
        # No stacktrace found for following nodes
        select_scatter_default_7: "f32[1, 196, 196, 3]" = torch.ops.aten.select_scatter.default(select_scatter_default_6, copy_7, 3, 1);  select_scatter_default_6 = copy_7 = None
        
         # File: /home/sahanp/.conda/envs/pytorch-benchmarks/lib/python3.12/site-packages/timm/models/convit.py:126 in get_rel_indices, code: rel_indices[:, :, :, 0] = indx.unsqueeze(0)
        select_26: "f32[1, 196, 196]" = torch.ops.aten.select.int(select_scatter_default_7, 3, 0)
        unsqueeze_16: "i64[1, 196, 196]" = torch.ops.aten.unsqueeze.default(repeat_2, 0);  repeat_2 = None
        copy_8: "f32[1, 196, 196]" = torch.ops.aten.copy.default(select_26, unsqueeze_16);  select_26 = unsqueeze_16 = None
        
        # No stacktrace found for following nodes
        select_scatter_default_8: "f32[1, 196, 196, 3]" = torch.ops.aten.select_scatter.default(select_scatter_default_7, copy_8, 3, 0);  select_scatter_default_7 = copy_8 = None
        
         # File: /home/sahanp/.conda/envs/pytorch-benchmarks/lib/python3.12/site-packages/timm/models/convit.py:128 in get_rel_indices, code: return rel_indices.to(device)
        device_put_2: "f32[1, 196, 196, 3]" = torch.ops.prims.device_put.default(select_scatter_default_8, device(type='cuda', index=0));  select_scatter_default_8 = None
        
         # File: /home/sahanp/.conda/envs/pytorch-benchmarks/lib/python3.12/site-packages/timm/models/convit.py:82 in get_attention, code: pos_score = self.rel_indices.expand(B, -1, -1, -1)
        expand_90: "f32[8, 196, 196, 3]" = torch.ops.aten.expand.default(device_put_2, [8, -1, -1, -1])
        
         # File: /home/sahanp/.conda/envs/pytorch-benchmarks/lib/python3.12/site-packages/timm/models/convit.py:83 in get_attention, code: pos_score = self.pos_proj(pos_score).permute(0, 3, 1, 2)
        clone_184: "f32[8, 196, 196, 3]" = torch.ops.aten.clone.default(expand_90, memory_format = torch.contiguous_format);  expand_90 = None
        view_341: "f32[307328, 3]" = torch.ops.aten.reshape.default(clone_184, [307328, 3]);  clone_184 = None
        permute_151: "f32[3, 16]" = torch.ops.aten.permute.default(arg38_1, [1, 0]);  arg38_1 = None
        mm_39: "f32[307328, 16]" = torch.ops.aten.mm.default(view_341, permute_151);  view_341 = permute_151 = None
        view_342: "f32[8, 196, 196, 16]" = torch.ops.aten.reshape.default(mm_39, [8, 196, 196, 16]);  mm_39 = None
        add_138: "f32[8, 196, 196, 16]" = torch.ops.aten.add.Tensor(view_342, arg39_1);  view_342 = arg39_1 = None
        permute_152: "f32[8, 16, 196, 196]" = torch.ops.aten.permute.default(add_138, [0, 3, 1, 2]);  add_138 = None
        
         # File: /home/sahanp/.conda/envs/pytorch-benchmarks/lib/python3.12/site-packages/timm/models/convit.py:86 in get_attention, code: pos_score = pos_score.softmax(dim=-1)
        clone_187: "f32[8, 16, 196, 196]" = torch.ops.aten.clone.default(permute_152, memory_format = torch.contiguous_format);  permute_152 = None
        amax_27: "f32[8, 16, 196, 1]" = torch.ops.aten.amax.default(clone_187, [-1], True)
        sub_79: "f32[8, 16, 196, 196]" = torch.ops.aten.sub.Tensor(clone_187, amax_27);  clone_187 = amax_27 = None
        exp_27: "f32[8, 16, 196, 196]" = torch.ops.aten.exp.default(sub_79);  sub_79 = None
        sum_40: "f32[8, 16, 196, 1]" = torch.ops.aten.sum.dim_IntList(exp_27, [-1], True)
        div_39: "f32[8, 16, 196, 196]" = torch.ops.aten.div.Tensor(exp_27, sum_40);  exp_27 = sum_40 = None
        
         # File: /home/sahanp/.conda/envs/pytorch-benchmarks/lib/python3.12/site-packages/timm/models/convit.py:89 in get_attention, code: attn = (1. - torch.sigmoid(gating)) * patch_score + torch.sigmoid(gating) * pos_score
        mul_142: "f32[8, 16, 196, 196]" = torch.ops.aten.mul.Tensor(sigmoid_25, div_39);  sigmoid_25 = div_39 = None
        add_139: "f32[8, 16, 196, 196]" = torch.ops.aten.add.Tensor(mul_141, mul_142);  mul_141 = mul_142 = None
        
         # File: /home/sahanp/.conda/envs/pytorch-benchmarks/lib/python3.12/site-packages/timm/models/convit.py:90 in get_attention, code: attn /= attn.sum(dim=-1).unsqueeze(-1)
        sum_41: "f32[8, 16, 196]" = torch.ops.aten.sum.dim_IntList(add_139, [-1])
        unsqueeze_62: "f32[8, 16, 196, 1]" = torch.ops.aten.unsqueeze.default(sum_41, -1);  sum_41 = None
        div_40: "f32[8, 16, 196, 196]" = torch.ops.aten.div.Tensor(add_139, unsqueeze_62);  add_139 = unsqueeze_62 = None
        
         # File: /home/sahanp/.conda/envs/pytorch-benchmarks/lib/python3.12/site-packages/timm/models/convit.py:73 in forward, code: x = (attn @ v).transpose(1, 2).reshape(B, N, C)
        expand_93: "f32[8, 16, 196, 196]" = torch.ops.aten.expand.default(div_40, [8, 16, 196, 196]);  div_40 = None
        view_350: "f32[128, 196, 196]" = torch.ops.aten.reshape.default(expand_93, [128, 196, 196]);  expand_93 = None
        
         # File: /home/sahanp/.conda/envs/pytorch-benchmarks/lib/python3.12/site-packages/timm/models/convit.py:72 in forward, code: v = self.v(x).reshape(B, N, self.num_heads, C // self.num_heads).permute(0, 2, 1, 3)
        view_347: "f32[1568, 768]" = torch.ops.aten.reshape.default(add_137, [1568, 768]);  add_137 = None
        permute_154: "f32[768, 768]" = torch.ops.aten.permute.default(arg41_1, [1, 0]);  arg41_1 = None
        mm_40: "f32[1568, 768]" = torch.ops.aten.mm.default(view_347, permute_154);  view_347 = permute_154 = None
        view_348: "f32[8, 196, 768]" = torch.ops.aten.reshape.default(mm_40, [8, 196, 768]);  mm_40 = None
        view_349: "f32[8, 196, 16, 48]" = torch.ops.aten.reshape.default(view_348, [8, 196, 16, 48]);  view_348 = None
        permute_155: "f32[8, 16, 196, 48]" = torch.ops.aten.permute.default(view_349, [0, 2, 1, 3]);  view_349 = None
        
         # File: /home/sahanp/.conda/envs/pytorch-benchmarks/lib/python3.12/site-packages/timm/models/convit.py:73 in forward, code: x = (attn @ v).transpose(1, 2).reshape(B, N, C)
        expand_94: "f32[8, 16, 196, 48]" = torch.ops.aten.expand.default(permute_155, [8, 16, 196, 48]);  permute_155 = None
        clone_189: "f32[8, 16, 196, 48]" = torch.ops.aten.clone.default(expand_94, memory_format = torch.contiguous_format);  expand_94 = None
        view_351: "f32[128, 196, 48]" = torch.ops.aten.reshape.default(clone_189, [128, 196, 48]);  clone_189 = None
        bmm_29: "f32[128, 196, 48]" = torch.ops.aten.bmm.default(view_350, view_351);  view_350 = view_351 = None
        view_352: "f32[8, 16, 196, 48]" = torch.ops.aten.reshape.default(bmm_29, [8, 16, 196, 48]);  bmm_29 = None
        permute_156: "f32[8, 196, 16, 48]" = torch.ops.aten.permute.default(view_352, [0, 2, 1, 3]);  view_352 = None
        clone_190: "f32[8, 196, 16, 48]" = torch.ops.aten.clone.default(permute_156, memory_format = torch.contiguous_format);  permute_156 = None
        view_353: "f32[8, 196, 768]" = torch.ops.aten.reshape.default(clone_190, [8, 196, 768]);  clone_190 = None
        
         # File: /home/sahanp/.conda/envs/pytorch-benchmarks/lib/python3.12/site-packages/timm/models/convit.py:74 in forward, code: x = self.proj(x)
        view_354: "f32[1568, 768]" = torch.ops.aten.reshape.default(view_353, [1568, 768]);  view_353 = None
        permute_157: "f32[768, 768]" = torch.ops.aten.permute.default(arg42_1, [1, 0]);  arg42_1 = None
        
        # No stacktrace found for following nodes
        mm_default_29: "f32[1568, 768]" = torch.ops.aten.mm.default(view_354, permute_157);  view_354 = permute_157 = None
        add_tensor_29: "f32[1568, 768]" = torch.ops.aten.add.Tensor(mm_default_29, arg43_1);  mm_default_29 = arg43_1 = None
        
         # File: /home/sahanp/.conda/envs/pytorch-benchmarks/lib/python3.12/site-packages/timm/models/convit.py:74 in forward, code: x = self.proj(x)
        view_355: "f32[8, 196, 768]" = torch.ops.aten.reshape.default(add_tensor_29, [8, 196, 768]);  add_tensor_29 = None
        
         # File: /home/sahanp/.conda/envs/pytorch-benchmarks/lib/python3.12/site-packages/timm/models/convit.py:233 in forward, code: x = x + self.drop_path(self.attn(self.norm1(x)))
        add_140: "f32[8, 196, 768]" = torch.ops.aten.add.Tensor(add_135, view_355);  add_135 = view_355 = None
        
         # File: /home/sahanp/.conda/envs/pytorch-benchmarks/lib/python3.12/site-packages/timm/layers/norm.py:57 in forward, code: x = F.layer_norm(x, self.normalized_shape, self.weight, self.bias, self.eps)
        clone_192: "f32[8, 196, 768]" = torch.ops.aten.clone.default(add_140, memory_format = torch.contiguous_format)
        var_mean_30 = torch.ops.aten.var_mean.correction(clone_192, [2], correction = 0, keepdim = True)
        getitem_66: "f32[8, 196, 1]" = var_mean_30[0]
        getitem_67: "f32[8, 196, 1]" = var_mean_30[1];  var_mean_30 = None
        sub_81: "f32[8, 196, 768]" = torch.ops.aten.sub.Tensor(clone_192, getitem_67);  clone_192 = getitem_67 = None
        add_141: "f32[8, 196, 1]" = torch.ops.aten.add.Tensor(getitem_66, 1e-06);  getitem_66 = None
        rsqrt_30: "f32[8, 196, 1]" = torch.ops.aten.rsqrt.default(add_141);  add_141 = None
        mul_143: "f32[8, 196, 768]" = torch.ops.aten.mul.Tensor(sub_81, rsqrt_30);  sub_81 = rsqrt_30 = None
        mul_144: "f32[8, 196, 768]" = torch.ops.aten.mul.Tensor(mul_143, arg44_1);  mul_143 = arg44_1 = None
        add_142: "f32[8, 196, 768]" = torch.ops.aten.add.Tensor(mul_144, arg45_1);  mul_144 = arg45_1 = None
        
         # File: /home/sahanp/.conda/envs/pytorch-benchmarks/lib/python3.12/site-packages/timm/layers/mlp.py:42 in forward, code: x = self.fc1(x)
        view_356: "f32[1568, 768]" = torch.ops.aten.reshape.default(add_142, [1568, 768]);  add_142 = None
        permute_158: "f32[768, 3072]" = torch.ops.aten.permute.default(arg46_1, [1, 0]);  arg46_1 = None
        
        # No stacktrace found for following nodes
        mm_default_28: "f32[1568, 3072]" = torch.ops.aten.mm.default(view_356, permute_158);  view_356 = permute_158 = None
        add_tensor_28: "f32[1568, 3072]" = torch.ops.aten.add.Tensor(mm_default_28, arg47_1);  mm_default_28 = arg47_1 = None
        
         # File: /home/sahanp/.conda/envs/pytorch-benchmarks/lib/python3.12/site-packages/timm/layers/mlp.py:42 in forward, code: x = self.fc1(x)
        view_357: "f32[8, 196, 3072]" = torch.ops.aten.reshape.default(add_tensor_28, [8, 196, 3072]);  add_tensor_28 = None
        
         # File: /home/sahanp/.conda/envs/pytorch-benchmarks/lib/python3.12/site-packages/timm/layers/mlp.py:43 in forward, code: x = self.act(x)
        mul_145: "f32[8, 196, 3072]" = torch.ops.aten.mul.Tensor(view_357, 0.5)
        mul_146: "f32[8, 196, 3072]" = torch.ops.aten.mul.Tensor(view_357, 0.7071067811865476);  view_357 = None
        erf_14: "f32[8, 196, 3072]" = torch.ops.aten.erf.default(mul_146);  mul_146 = None
        add_143: "f32[8, 196, 3072]" = torch.ops.aten.add.Tensor(erf_14, 1);  erf_14 = None
        mul_147: "f32[8, 196, 3072]" = torch.ops.aten.mul.Tensor(mul_145, add_143);  mul_145 = add_143 = None
        
         # File: /home/sahanp/.conda/envs/pytorch-benchmarks/lib/python3.12/site-packages/timm/layers/mlp.py:46 in forward, code: x = self.fc2(x)
        view_358: "f32[1568, 3072]" = torch.ops.aten.reshape.default(mul_147, [1568, 3072]);  mul_147 = None
        permute_159: "f32[3072, 768]" = torch.ops.aten.permute.default(arg48_1, [1, 0]);  arg48_1 = None
        
        # No stacktrace found for following nodes
        mm_default_27: "f32[1568, 768]" = torch.ops.aten.mm.default(view_358, permute_159);  view_358 = permute_159 = None
        add_tensor_27: "f32[1568, 768]" = torch.ops.aten.add.Tensor(mm_default_27, arg49_1);  mm_default_27 = arg49_1 = None
        
         # File: /home/sahanp/.conda/envs/pytorch-benchmarks/lib/python3.12/site-packages/timm/layers/mlp.py:46 in forward, code: x = self.fc2(x)
        view_359: "f32[8, 196, 768]" = torch.ops.aten.reshape.default(add_tensor_27, [8, 196, 768]);  add_tensor_27 = None
        
         # File: /home/sahanp/.conda/envs/pytorch-benchmarks/lib/python3.12/site-packages/timm/models/convit.py:234 in forward, code: x = x + self.drop_path(self.mlp(self.norm2(x)))
        add_144: "f32[8, 196, 768]" = torch.ops.aten.add.Tensor(add_140, view_359);  add_140 = view_359 = None
        
         # File: /home/sahanp/.conda/envs/pytorch-benchmarks/lib/python3.12/site-packages/timm/layers/norm.py:57 in forward, code: x = F.layer_norm(x, self.normalized_shape, self.weight, self.bias, self.eps)
        clone_195: "f32[8, 196, 768]" = torch.ops.aten.clone.default(add_144, memory_format = torch.contiguous_format)
        var_mean_31 = torch.ops.aten.var_mean.correction(clone_195, [2], correction = 0, keepdim = True)
        getitem_68: "f32[8, 196, 1]" = var_mean_31[0]
        getitem_69: "f32[8, 196, 1]" = var_mean_31[1];  var_mean_31 = None
        
         # File: /home/sahanp/.conda/envs/pytorch-benchmarks/lib/python3.12/site-packages/timm/models/convit.py:88 in get_attention, code: gating = self.gating_param.view(1, -1, 1, 1)
        view_368: "f32[1, 16, 1, 1]" = torch.ops.aten.reshape.default(arg55_1, [1, -1, 1, 1]);  arg55_1 = None
        
         # File: /home/sahanp/.conda/envs/pytorch-benchmarks/lib/python3.12/site-packages/timm/models/convit.py:89 in get_attention, code: attn = (1. - torch.sigmoid(gating)) * patch_score + torch.sigmoid(gating) * pos_score
        sigmoid_26: "f32[1, 16, 1, 1]" = torch.ops.aten.sigmoid.default(view_368)
        sub_85: "f32[1, 16, 1, 1]" = torch.ops.aten.sub.Tensor(1.0, sigmoid_26);  sigmoid_26 = None
        
         # File: /home/sahanp/.conda/envs/pytorch-benchmarks/lib/python3.12/site-packages/timm/layers/norm.py:57 in forward, code: x = F.layer_norm(x, self.normalized_shape, self.weight, self.bias, self.eps)
        sub_82: "f32[8, 196, 768]" = torch.ops.aten.sub.Tensor(clone_195, getitem_69);  clone_195 = getitem_69 = None
        add_145: "f32[8, 196, 1]" = torch.ops.aten.add.Tensor(getitem_68, 1e-06);  getitem_68 = None
        rsqrt_31: "f32[8, 196, 1]" = torch.ops.aten.rsqrt.default(add_145);  add_145 = None
        mul_148: "f32[8, 196, 768]" = torch.ops.aten.mul.Tensor(sub_82, rsqrt_31);  sub_82 = rsqrt_31 = None
        mul_149: "f32[8, 196, 768]" = torch.ops.aten.mul.Tensor(mul_148, arg50_1);  mul_148 = arg50_1 = None
        add_146: "f32[8, 196, 768]" = torch.ops.aten.add.Tensor(mul_149, arg51_1);  mul_149 = arg51_1 = None
        
         # File: /home/sahanp/.conda/envs/pytorch-benchmarks/lib/python3.12/site-packages/timm/models/convit.py:80 in get_attention, code: qk = self.qk(x).reshape(B, N, 2, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)
        view_360: "f32[1568, 768]" = torch.ops.aten.reshape.default(add_146, [1568, 768])
        permute_160: "f32[768, 1536]" = torch.ops.aten.permute.default(arg52_1, [1, 0]);  arg52_1 = None
        mm_41: "f32[1568, 1536]" = torch.ops.aten.mm.default(view_360, permute_160);  view_360 = permute_160 = None
        view_361: "f32[8, 196, 1536]" = torch.ops.aten.reshape.default(mm_41, [8, 196, 1536]);  mm_41 = None
        view_362: "f32[8, 196, 2, 16, 48]" = torch.ops.aten.reshape.default(view_361, [8, 196, 2, 16, 48]);  view_361 = None
        permute_161: "f32[2, 8, 16, 196, 48]" = torch.ops.aten.permute.default(view_362, [2, 0, 3, 1, 4]);  view_362 = None
        
         # File: /home/sahanp/.conda/envs/pytorch-benchmarks/lib/python3.12/site-packages/timm/models/convit.py:81 in get_attention, code: q, k = qk[0], qk[1]
        select_107: "f32[8, 16, 196, 48]" = torch.ops.aten.select.int(permute_161, 0, 0)
        
         # File: /home/sahanp/.conda/envs/pytorch-benchmarks/lib/python3.12/site-packages/timm/models/convit.py:84 in get_attention, code: patch_score = (q @ k.transpose(-2, -1)) * self.scale
        expand_96: "f32[8, 16, 196, 48]" = torch.ops.aten.expand.default(select_107, [8, 16, 196, 48]);  select_107 = None
        clone_197: "f32[8, 16, 196, 48]" = torch.ops.aten.clone.default(expand_96, memory_format = torch.contiguous_format);  expand_96 = None
        view_365: "f32[128, 196, 48]" = torch.ops.aten.reshape.default(clone_197, [128, 196, 48]);  clone_197 = None
        
         # File: /home/sahanp/.conda/envs/pytorch-benchmarks/lib/python3.12/site-packages/timm/models/convit.py:81 in get_attention, code: q, k = qk[0], qk[1]
        select_108: "f32[8, 16, 196, 48]" = torch.ops.aten.select.int(permute_161, 0, 1);  permute_161 = None
        
         # File: /home/sahanp/.conda/envs/pytorch-benchmarks/lib/python3.12/site-packages/timm/models/convit.py:84 in get_attention, code: patch_score = (q @ k.transpose(-2, -1)) * self.scale
        permute_164: "f32[8, 16, 48, 196]" = torch.ops.aten.permute.default(select_108, [0, 1, 3, 2]);  select_108 = None
        expand_97: "f32[8, 16, 48, 196]" = torch.ops.aten.expand.default(permute_164, [8, 16, 48, 196]);  permute_164 = None
        clone_198: "f32[8, 16, 48, 196]" = torch.ops.aten.clone.default(expand_97, memory_format = torch.contiguous_format);  expand_97 = None
        view_366: "f32[128, 48, 196]" = torch.ops.aten.reshape.default(clone_198, [128, 48, 196]);  clone_198 = None
        bmm_30: "f32[128, 196, 196]" = torch.ops.aten.bmm.default(view_365, view_366);  view_365 = view_366 = None
        view_367: "f32[8, 16, 196, 196]" = torch.ops.aten.reshape.default(bmm_30, [8, 16, 196, 196]);  bmm_30 = None
        
        # No stacktrace found for following nodes
        mul_tensor_16: "f32[8, 16, 196, 196]" = torch.ops.aten.mul.Tensor(view_367, 1);  view_367 = None
        amax_default_8: "f32[8, 16, 196, 1]" = torch.ops.aten.amax.default(mul_tensor_16, [-1], True)
        sub_tensor_8: "f32[8, 16, 196, 196]" = torch.ops.aten.sub.Tensor(mul_tensor_16, amax_default_8);  mul_tensor_16 = amax_default_8 = None
        mul_tensor_17: "f32[8, 16, 196, 196]" = torch.ops.aten.mul.Tensor(sub_tensor_8, 0.14433756729740643);  sub_tensor_8 = None
        
         # File: /home/sahanp/.conda/envs/pytorch-benchmarks/lib/python3.12/site-packages/timm/models/convit.py:85 in get_attention, code: patch_score = patch_score.softmax(dim=-1)
        exp_28: "f32[8, 16, 196, 196]" = torch.ops.aten.exp.default(mul_tensor_17);  mul_tensor_17 = None
        sum_42: "f32[8, 16, 196, 1]" = torch.ops.aten.sum.dim_IntList(exp_28, [-1], True)
        div_41: "f32[8, 16, 196, 196]" = torch.ops.aten.div.Tensor(exp_28, sum_42);  exp_28 = sum_42 = None
        
         # File: /home/sahanp/.conda/envs/pytorch-benchmarks/lib/python3.12/site-packages/timm/models/convit.py:89 in get_attention, code: attn = (1. - torch.sigmoid(gating)) * patch_score + torch.sigmoid(gating) * pos_score
        mul_151: "f32[8, 16, 196, 196]" = torch.ops.aten.mul.Tensor(sub_85, div_41);  sub_85 = div_41 = None
        sigmoid_27: "f32[1, 16, 1, 1]" = torch.ops.aten.sigmoid.default(view_368);  view_368 = None
        
         # File: /home/sahanp/.conda/envs/pytorch-benchmarks/lib/python3.12/site-packages/timm/models/convit.py:119 in get_rel_indices, code: rel_indices = torch.zeros(1, num_patches, num_patches, 3)
        full_default_3: "f32[1, 196, 196, 3]" = torch.ops.aten.full.default([1, 196, 196, 3], 0, dtype = torch.float32, layout = torch.strided, device = device(type='cpu'), pin_memory = False)
        
         # File: /home/sahanp/.conda/envs/pytorch-benchmarks/lib/python3.12/site-packages/timm/models/convit.py:124 in get_rel_indices, code: rel_indices[:, :, :, 2] = indd.unsqueeze(0)
        select_30: "f32[1, 196, 196]" = torch.ops.aten.select.int(full_default_3, 3, 2)
        
         # File: /home/sahanp/.conda/envs/pytorch-benchmarks/lib/python3.12/site-packages/timm/models/convit.py:120 in get_rel_indices, code: ind = torch.arange(img_size).view(1, -1) - torch.arange(img_size).view(-1, 1)
        iota_6: "i64[14]" = torch.ops.prims.iota.default(14, start = 0, step = 1, dtype = torch.int64, device = device(type='cpu'), requires_grad = False)
        view_79: "i64[1, 14]" = torch.ops.aten.reshape.default(iota_6, [1, -1]);  iota_6 = None
        iota_7: "i64[14]" = torch.ops.prims.iota.default(14, start = 0, step = 1, dtype = torch.int64, device = device(type='cpu'), requires_grad = False)
        view_80: "i64[14, 1]" = torch.ops.aten.reshape.default(iota_7, [-1, 1]);  iota_7 = None
        sub_19: "i64[14, 14]" = torch.ops.aten.sub.Tensor(view_79, view_80);  view_79 = view_80 = None
        
         # File: /home/sahanp/.conda/envs/pytorch-benchmarks/lib/python3.12/site-packages/timm/models/convit.py:121 in get_rel_indices, code: indx = ind.repeat(img_size, img_size)
        repeat_3: "i64[196, 196]" = torch.ops.aten.repeat.default(sub_19, [14, 14])
        
         # File: /home/sahanp/.conda/envs/pytorch-benchmarks/lib/python3.12/site-packages/timm/models/convit.py:123 in get_rel_indices, code: indd = indx ** 2 + indy ** 2
        pow_7: "i64[196, 196]" = torch.ops.aten.pow.Tensor_Scalar(repeat_3, 2)
        
         # File: /home/sahanp/.conda/envs/pytorch-benchmarks/lib/python3.12/site-packages/timm/models/convit.py:122 in get_rel_indices, code: indy = ind.repeat_interleave(img_size, dim=0).repeat_interleave(img_size, dim=1)
        unsqueeze_18: "i64[14, 1, 14]" = torch.ops.aten.unsqueeze.default(sub_19, 1);  sub_19 = None
        expand_22: "i64[14, 14, 14]" = torch.ops.aten.expand.default(unsqueeze_18, [14, 14, 14]);  unsqueeze_18 = None
        clone_44: "i64[14, 14, 14]" = torch.ops.aten.clone.default(expand_22, memory_format = torch.contiguous_format);  expand_22 = None
        view_81: "i64[196, 14]" = torch.ops.aten.reshape.default(clone_44, [196, 14]);  clone_44 = None
        unsqueeze_19: "i64[196, 14, 1]" = torch.ops.aten.unsqueeze.default(view_81, 2);  view_81 = None
        expand_23: "i64[196, 14, 14]" = torch.ops.aten.expand.default(unsqueeze_19, [196, 14, 14]);  unsqueeze_19 = None
        clone_45: "i64[196, 14, 14]" = torch.ops.aten.clone.default(expand_23, memory_format = torch.contiguous_format);  expand_23 = None
        view_82: "i64[196, 196]" = torch.ops.aten.reshape.default(clone_45, [196, 196]);  clone_45 = None
        
         # File: /home/sahanp/.conda/envs/pytorch-benchmarks/lib/python3.12/site-packages/timm/models/convit.py:123 in get_rel_indices, code: indd = indx ** 2 + indy ** 2
        pow_8: "i64[196, 196]" = torch.ops.aten.pow.Tensor_Scalar(view_82, 2)
        add_33: "i64[196, 196]" = torch.ops.aten.add.Tensor(pow_7, pow_8);  pow_7 = pow_8 = None
        
         # File: /home/sahanp/.conda/envs/pytorch-benchmarks/lib/python3.12/site-packages/timm/models/convit.py:124 in get_rel_indices, code: rel_indices[:, :, :, 2] = indd.unsqueeze(0)
        unsqueeze_20: "i64[1, 196, 196]" = torch.ops.aten.unsqueeze.default(add_33, 0);  add_33 = None
        copy_9: "f32[1, 196, 196]" = torch.ops.aten.copy.default(select_30, unsqueeze_20);  select_30 = unsqueeze_20 = None
        
        # No stacktrace found for following nodes
        select_scatter_default_9: "f32[1, 196, 196, 3]" = torch.ops.aten.select_scatter.default(full_default_3, copy_9, 3, 2);  full_default_3 = copy_9 = None
        
         # File: /home/sahanp/.conda/envs/pytorch-benchmarks/lib/python3.12/site-packages/timm/models/convit.py:125 in get_rel_indices, code: rel_indices[:, :, :, 1] = indy.unsqueeze(0)
        select_33: "f32[1, 196, 196]" = torch.ops.aten.select.int(select_scatter_default_9, 3, 1)
        unsqueeze_21: "i64[1, 196, 196]" = torch.ops.aten.unsqueeze.default(view_82, 0);  view_82 = None
        copy_10: "f32[1, 196, 196]" = torch.ops.aten.copy.default(select_33, unsqueeze_21);  select_33 = unsqueeze_21 = None
        
        # No stacktrace found for following nodes
        select_scatter_default_10: "f32[1, 196, 196, 3]" = torch.ops.aten.select_scatter.default(select_scatter_default_9, copy_10, 3, 1);  select_scatter_default_9 = copy_10 = None
        
         # File: /home/sahanp/.conda/envs/pytorch-benchmarks/lib/python3.12/site-packages/timm/models/convit.py:126 in get_rel_indices, code: rel_indices[:, :, :, 0] = indx.unsqueeze(0)
        select_36: "f32[1, 196, 196]" = torch.ops.aten.select.int(select_scatter_default_10, 3, 0)
        unsqueeze_22: "i64[1, 196, 196]" = torch.ops.aten.unsqueeze.default(repeat_3, 0);  repeat_3 = None
        copy_11: "f32[1, 196, 196]" = torch.ops.aten.copy.default(select_36, unsqueeze_22);  select_36 = unsqueeze_22 = None
        
        # No stacktrace found for following nodes
        select_scatter_default_11: "f32[1, 196, 196, 3]" = torch.ops.aten.select_scatter.default(select_scatter_default_10, copy_11, 3, 0);  select_scatter_default_10 = copy_11 = None
        
         # File: /home/sahanp/.conda/envs/pytorch-benchmarks/lib/python3.12/site-packages/timm/models/convit.py:128 in get_rel_indices, code: return rel_indices.to(device)
        device_put_3: "f32[1, 196, 196, 3]" = torch.ops.prims.device_put.default(select_scatter_default_11, device(type='cuda', index=0));  select_scatter_default_11 = None
        
         # File: /home/sahanp/.conda/envs/pytorch-benchmarks/lib/python3.12/site-packages/timm/models/convit.py:82 in get_attention, code: pos_score = self.rel_indices.expand(B, -1, -1, -1)
        expand_95: "f32[8, 196, 196, 3]" = torch.ops.aten.expand.default(device_put_3, [8, -1, -1, -1])
        
         # File: /home/sahanp/.conda/envs/pytorch-benchmarks/lib/python3.12/site-packages/timm/models/convit.py:83 in get_attention, code: pos_score = self.pos_proj(pos_score).permute(0, 3, 1, 2)
        clone_196: "f32[8, 196, 196, 3]" = torch.ops.aten.clone.default(expand_95, memory_format = torch.contiguous_format);  expand_95 = None
        view_363: "f32[307328, 3]" = torch.ops.aten.reshape.default(clone_196, [307328, 3]);  clone_196 = None
        permute_162: "f32[3, 16]" = torch.ops.aten.permute.default(arg53_1, [1, 0]);  arg53_1 = None
        mm_42: "f32[307328, 16]" = torch.ops.aten.mm.default(view_363, permute_162);  view_363 = permute_162 = None
        view_364: "f32[8, 196, 196, 16]" = torch.ops.aten.reshape.default(mm_42, [8, 196, 196, 16]);  mm_42 = None
        add_147: "f32[8, 196, 196, 16]" = torch.ops.aten.add.Tensor(view_364, arg54_1);  view_364 = arg54_1 = None
        permute_163: "f32[8, 16, 196, 196]" = torch.ops.aten.permute.default(add_147, [0, 3, 1, 2]);  add_147 = None
        
         # File: /home/sahanp/.conda/envs/pytorch-benchmarks/lib/python3.12/site-packages/timm/models/convit.py:86 in get_attention, code: pos_score = pos_score.softmax(dim=-1)
        clone_199: "f32[8, 16, 196, 196]" = torch.ops.aten.clone.default(permute_163, memory_format = torch.contiguous_format);  permute_163 = None
        amax_29: "f32[8, 16, 196, 1]" = torch.ops.aten.amax.default(clone_199, [-1], True)
        sub_84: "f32[8, 16, 196, 196]" = torch.ops.aten.sub.Tensor(clone_199, amax_29);  clone_199 = amax_29 = None
        exp_29: "f32[8, 16, 196, 196]" = torch.ops.aten.exp.default(sub_84);  sub_84 = None
        sum_43: "f32[8, 16, 196, 1]" = torch.ops.aten.sum.dim_IntList(exp_29, [-1], True)
        div_42: "f32[8, 16, 196, 196]" = torch.ops.aten.div.Tensor(exp_29, sum_43);  exp_29 = sum_43 = None
        
         # File: /home/sahanp/.conda/envs/pytorch-benchmarks/lib/python3.12/site-packages/timm/models/convit.py:89 in get_attention, code: attn = (1. - torch.sigmoid(gating)) * patch_score + torch.sigmoid(gating) * pos_score
        mul_152: "f32[8, 16, 196, 196]" = torch.ops.aten.mul.Tensor(sigmoid_27, div_42);  sigmoid_27 = div_42 = None
        add_148: "f32[8, 16, 196, 196]" = torch.ops.aten.add.Tensor(mul_151, mul_152);  mul_151 = mul_152 = None
        
         # File: /home/sahanp/.conda/envs/pytorch-benchmarks/lib/python3.12/site-packages/timm/models/convit.py:90 in get_attention, code: attn /= attn.sum(dim=-1).unsqueeze(-1)
        sum_44: "f32[8, 16, 196]" = torch.ops.aten.sum.dim_IntList(add_148, [-1])
        unsqueeze_63: "f32[8, 16, 196, 1]" = torch.ops.aten.unsqueeze.default(sum_44, -1);  sum_44 = None
        div_43: "f32[8, 16, 196, 196]" = torch.ops.aten.div.Tensor(add_148, unsqueeze_63);  add_148 = unsqueeze_63 = None
        
         # File: /home/sahanp/.conda/envs/pytorch-benchmarks/lib/python3.12/site-packages/timm/models/convit.py:73 in forward, code: x = (attn @ v).transpose(1, 2).reshape(B, N, C)
        expand_98: "f32[8, 16, 196, 196]" = torch.ops.aten.expand.default(div_43, [8, 16, 196, 196]);  div_43 = None
        view_372: "f32[128, 196, 196]" = torch.ops.aten.reshape.default(expand_98, [128, 196, 196]);  expand_98 = None
        
         # File: /home/sahanp/.conda/envs/pytorch-benchmarks/lib/python3.12/site-packages/timm/models/convit.py:72 in forward, code: v = self.v(x).reshape(B, N, self.num_heads, C // self.num_heads).permute(0, 2, 1, 3)
        view_369: "f32[1568, 768]" = torch.ops.aten.reshape.default(add_146, [1568, 768]);  add_146 = None
        permute_165: "f32[768, 768]" = torch.ops.aten.permute.default(arg56_1, [1, 0]);  arg56_1 = None
        mm_43: "f32[1568, 768]" = torch.ops.aten.mm.default(view_369, permute_165);  view_369 = permute_165 = None
        view_370: "f32[8, 196, 768]" = torch.ops.aten.reshape.default(mm_43, [8, 196, 768]);  mm_43 = None
        view_371: "f32[8, 196, 16, 48]" = torch.ops.aten.reshape.default(view_370, [8, 196, 16, 48]);  view_370 = None
        permute_166: "f32[8, 16, 196, 48]" = torch.ops.aten.permute.default(view_371, [0, 2, 1, 3]);  view_371 = None
        
         # File: /home/sahanp/.conda/envs/pytorch-benchmarks/lib/python3.12/site-packages/timm/models/convit.py:73 in forward, code: x = (attn @ v).transpose(1, 2).reshape(B, N, C)
        expand_99: "f32[8, 16, 196, 48]" = torch.ops.aten.expand.default(permute_166, [8, 16, 196, 48]);  permute_166 = None
        clone_201: "f32[8, 16, 196, 48]" = torch.ops.aten.clone.default(expand_99, memory_format = torch.contiguous_format);  expand_99 = None
        view_373: "f32[128, 196, 48]" = torch.ops.aten.reshape.default(clone_201, [128, 196, 48]);  clone_201 = None
        bmm_31: "f32[128, 196, 48]" = torch.ops.aten.bmm.default(view_372, view_373);  view_372 = view_373 = None
        view_374: "f32[8, 16, 196, 48]" = torch.ops.aten.reshape.default(bmm_31, [8, 16, 196, 48]);  bmm_31 = None
        permute_167: "f32[8, 196, 16, 48]" = torch.ops.aten.permute.default(view_374, [0, 2, 1, 3]);  view_374 = None
        clone_202: "f32[8, 196, 16, 48]" = torch.ops.aten.clone.default(permute_167, memory_format = torch.contiguous_format);  permute_167 = None
        view_375: "f32[8, 196, 768]" = torch.ops.aten.reshape.default(clone_202, [8, 196, 768]);  clone_202 = None
        
         # File: /home/sahanp/.conda/envs/pytorch-benchmarks/lib/python3.12/site-packages/timm/models/convit.py:74 in forward, code: x = self.proj(x)
        view_376: "f32[1568, 768]" = torch.ops.aten.reshape.default(view_375, [1568, 768]);  view_375 = None
        permute_168: "f32[768, 768]" = torch.ops.aten.permute.default(arg57_1, [1, 0]);  arg57_1 = None
        
        # No stacktrace found for following nodes
        mm_default_26: "f32[1568, 768]" = torch.ops.aten.mm.default(view_376, permute_168);  view_376 = permute_168 = None
        add_tensor_26: "f32[1568, 768]" = torch.ops.aten.add.Tensor(mm_default_26, arg58_1);  mm_default_26 = arg58_1 = None
        
         # File: /home/sahanp/.conda/envs/pytorch-benchmarks/lib/python3.12/site-packages/timm/models/convit.py:74 in forward, code: x = self.proj(x)
        view_377: "f32[8, 196, 768]" = torch.ops.aten.reshape.default(add_tensor_26, [8, 196, 768]);  add_tensor_26 = None
        
         # File: /home/sahanp/.conda/envs/pytorch-benchmarks/lib/python3.12/site-packages/timm/models/convit.py:233 in forward, code: x = x + self.drop_path(self.attn(self.norm1(x)))
        add_149: "f32[8, 196, 768]" = torch.ops.aten.add.Tensor(add_144, view_377);  add_144 = view_377 = None
        
         # File: /home/sahanp/.conda/envs/pytorch-benchmarks/lib/python3.12/site-packages/timm/layers/norm.py:57 in forward, code: x = F.layer_norm(x, self.normalized_shape, self.weight, self.bias, self.eps)
        clone_204: "f32[8, 196, 768]" = torch.ops.aten.clone.default(add_149, memory_format = torch.contiguous_format)
        var_mean_32 = torch.ops.aten.var_mean.correction(clone_204, [2], correction = 0, keepdim = True)
        getitem_70: "f32[8, 196, 1]" = var_mean_32[0]
        getitem_71: "f32[8, 196, 1]" = var_mean_32[1];  var_mean_32 = None
        sub_86: "f32[8, 196, 768]" = torch.ops.aten.sub.Tensor(clone_204, getitem_71);  clone_204 = getitem_71 = None
        add_150: "f32[8, 196, 1]" = torch.ops.aten.add.Tensor(getitem_70, 1e-06);  getitem_70 = None
        rsqrt_32: "f32[8, 196, 1]" = torch.ops.aten.rsqrt.default(add_150);  add_150 = None
        mul_153: "f32[8, 196, 768]" = torch.ops.aten.mul.Tensor(sub_86, rsqrt_32);  sub_86 = rsqrt_32 = None
        mul_154: "f32[8, 196, 768]" = torch.ops.aten.mul.Tensor(mul_153, arg59_1);  mul_153 = arg59_1 = None
        add_151: "f32[8, 196, 768]" = torch.ops.aten.add.Tensor(mul_154, arg60_1);  mul_154 = arg60_1 = None
        
         # File: /home/sahanp/.conda/envs/pytorch-benchmarks/lib/python3.12/site-packages/timm/layers/mlp.py:42 in forward, code: x = self.fc1(x)
        view_378: "f32[1568, 768]" = torch.ops.aten.reshape.default(add_151, [1568, 768]);  add_151 = None
        permute_169: "f32[768, 3072]" = torch.ops.aten.permute.default(arg61_1, [1, 0]);  arg61_1 = None
        
        # No stacktrace found for following nodes
        mm_default_25: "f32[1568, 3072]" = torch.ops.aten.mm.default(view_378, permute_169);  view_378 = permute_169 = None
        add_tensor_25: "f32[1568, 3072]" = torch.ops.aten.add.Tensor(mm_default_25, arg62_1);  mm_default_25 = arg62_1 = None
        
         # File: /home/sahanp/.conda/envs/pytorch-benchmarks/lib/python3.12/site-packages/timm/layers/mlp.py:42 in forward, code: x = self.fc1(x)
        view_379: "f32[8, 196, 3072]" = torch.ops.aten.reshape.default(add_tensor_25, [8, 196, 3072]);  add_tensor_25 = None
        
         # File: /home/sahanp/.conda/envs/pytorch-benchmarks/lib/python3.12/site-packages/timm/layers/mlp.py:43 in forward, code: x = self.act(x)
        mul_155: "f32[8, 196, 3072]" = torch.ops.aten.mul.Tensor(view_379, 0.5)
        mul_156: "f32[8, 196, 3072]" = torch.ops.aten.mul.Tensor(view_379, 0.7071067811865476);  view_379 = None
        erf_15: "f32[8, 196, 3072]" = torch.ops.aten.erf.default(mul_156);  mul_156 = None
        add_152: "f32[8, 196, 3072]" = torch.ops.aten.add.Tensor(erf_15, 1);  erf_15 = None
        mul_157: "f32[8, 196, 3072]" = torch.ops.aten.mul.Tensor(mul_155, add_152);  mul_155 = add_152 = None
        
         # File: /home/sahanp/.conda/envs/pytorch-benchmarks/lib/python3.12/site-packages/timm/layers/mlp.py:46 in forward, code: x = self.fc2(x)
        view_380: "f32[1568, 3072]" = torch.ops.aten.reshape.default(mul_157, [1568, 3072]);  mul_157 = None
        permute_170: "f32[3072, 768]" = torch.ops.aten.permute.default(arg63_1, [1, 0]);  arg63_1 = None
        
        # No stacktrace found for following nodes
        mm_default_24: "f32[1568, 768]" = torch.ops.aten.mm.default(view_380, permute_170);  view_380 = permute_170 = None
        add_tensor_24: "f32[1568, 768]" = torch.ops.aten.add.Tensor(mm_default_24, arg64_1);  mm_default_24 = arg64_1 = None
        
         # File: /home/sahanp/.conda/envs/pytorch-benchmarks/lib/python3.12/site-packages/timm/layers/mlp.py:46 in forward, code: x = self.fc2(x)
        view_381: "f32[8, 196, 768]" = torch.ops.aten.reshape.default(add_tensor_24, [8, 196, 768]);  add_tensor_24 = None
        
         # File: /home/sahanp/.conda/envs/pytorch-benchmarks/lib/python3.12/site-packages/timm/models/convit.py:234 in forward, code: x = x + self.drop_path(self.mlp(self.norm2(x)))
        add_153: "f32[8, 196, 768]" = torch.ops.aten.add.Tensor(add_149, view_381);  add_149 = view_381 = None
        
         # File: /home/sahanp/.conda/envs/pytorch-benchmarks/lib/python3.12/site-packages/timm/layers/norm.py:57 in forward, code: x = F.layer_norm(x, self.normalized_shape, self.weight, self.bias, self.eps)
        clone_207: "f32[8, 196, 768]" = torch.ops.aten.clone.default(add_153, memory_format = torch.contiguous_format)
        var_mean_33 = torch.ops.aten.var_mean.correction(clone_207, [2], correction = 0, keepdim = True)
        getitem_72: "f32[8, 196, 1]" = var_mean_33[0]
        getitem_73: "f32[8, 196, 1]" = var_mean_33[1];  var_mean_33 = None
        
         # File: /home/sahanp/.conda/envs/pytorch-benchmarks/lib/python3.12/site-packages/timm/models/convit.py:88 in get_attention, code: gating = self.gating_param.view(1, -1, 1, 1)
        view_390: "f32[1, 16, 1, 1]" = torch.ops.aten.reshape.default(arg70_1, [1, -1, 1, 1]);  arg70_1 = None
        
         # File: /home/sahanp/.conda/envs/pytorch-benchmarks/lib/python3.12/site-packages/timm/models/convit.py:89 in get_attention, code: attn = (1. - torch.sigmoid(gating)) * patch_score + torch.sigmoid(gating) * pos_score
        sigmoid_28: "f32[1, 16, 1, 1]" = torch.ops.aten.sigmoid.default(view_390)
        sub_90: "f32[1, 16, 1, 1]" = torch.ops.aten.sub.Tensor(1.0, sigmoid_28);  sigmoid_28 = None
        
         # File: /home/sahanp/.conda/envs/pytorch-benchmarks/lib/python3.12/site-packages/timm/layers/norm.py:57 in forward, code: x = F.layer_norm(x, self.normalized_shape, self.weight, self.bias, self.eps)
        sub_87: "f32[8, 196, 768]" = torch.ops.aten.sub.Tensor(clone_207, getitem_73);  clone_207 = getitem_73 = None
        add_154: "f32[8, 196, 1]" = torch.ops.aten.add.Tensor(getitem_72, 1e-06);  getitem_72 = None
        rsqrt_33: "f32[8, 196, 1]" = torch.ops.aten.rsqrt.default(add_154);  add_154 = None
        mul_158: "f32[8, 196, 768]" = torch.ops.aten.mul.Tensor(sub_87, rsqrt_33);  sub_87 = rsqrt_33 = None
        mul_159: "f32[8, 196, 768]" = torch.ops.aten.mul.Tensor(mul_158, arg65_1);  mul_158 = arg65_1 = None
        add_155: "f32[8, 196, 768]" = torch.ops.aten.add.Tensor(mul_159, arg66_1);  mul_159 = arg66_1 = None
        
         # File: /home/sahanp/.conda/envs/pytorch-benchmarks/lib/python3.12/site-packages/timm/models/convit.py:80 in get_attention, code: qk = self.qk(x).reshape(B, N, 2, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)
        view_382: "f32[1568, 768]" = torch.ops.aten.reshape.default(add_155, [1568, 768])
        permute_171: "f32[768, 1536]" = torch.ops.aten.permute.default(arg67_1, [1, 0]);  arg67_1 = None
        mm_44: "f32[1568, 1536]" = torch.ops.aten.mm.default(view_382, permute_171);  view_382 = permute_171 = None
        view_383: "f32[8, 196, 1536]" = torch.ops.aten.reshape.default(mm_44, [8, 196, 1536]);  mm_44 = None
        view_384: "f32[8, 196, 2, 16, 48]" = torch.ops.aten.reshape.default(view_383, [8, 196, 2, 16, 48]);  view_383 = None
        permute_172: "f32[2, 8, 16, 196, 48]" = torch.ops.aten.permute.default(view_384, [2, 0, 3, 1, 4]);  view_384 = None
        
         # File: /home/sahanp/.conda/envs/pytorch-benchmarks/lib/python3.12/site-packages/timm/models/convit.py:81 in get_attention, code: q, k = qk[0], qk[1]
        select_109: "f32[8, 16, 196, 48]" = torch.ops.aten.select.int(permute_172, 0, 0)
        
         # File: /home/sahanp/.conda/envs/pytorch-benchmarks/lib/python3.12/site-packages/timm/models/convit.py:84 in get_attention, code: patch_score = (q @ k.transpose(-2, -1)) * self.scale
        expand_101: "f32[8, 16, 196, 48]" = torch.ops.aten.expand.default(select_109, [8, 16, 196, 48]);  select_109 = None
        clone_209: "f32[8, 16, 196, 48]" = torch.ops.aten.clone.default(expand_101, memory_format = torch.contiguous_format);  expand_101 = None
        view_387: "f32[128, 196, 48]" = torch.ops.aten.reshape.default(clone_209, [128, 196, 48]);  clone_209 = None
        
         # File: /home/sahanp/.conda/envs/pytorch-benchmarks/lib/python3.12/site-packages/timm/models/convit.py:81 in get_attention, code: q, k = qk[0], qk[1]
        select_110: "f32[8, 16, 196, 48]" = torch.ops.aten.select.int(permute_172, 0, 1);  permute_172 = None
        
         # File: /home/sahanp/.conda/envs/pytorch-benchmarks/lib/python3.12/site-packages/timm/models/convit.py:84 in get_attention, code: patch_score = (q @ k.transpose(-2, -1)) * self.scale
        permute_175: "f32[8, 16, 48, 196]" = torch.ops.aten.permute.default(select_110, [0, 1, 3, 2]);  select_110 = None
        expand_102: "f32[8, 16, 48, 196]" = torch.ops.aten.expand.default(permute_175, [8, 16, 48, 196]);  permute_175 = None
        clone_210: "f32[8, 16, 48, 196]" = torch.ops.aten.clone.default(expand_102, memory_format = torch.contiguous_format);  expand_102 = None
        view_388: "f32[128, 48, 196]" = torch.ops.aten.reshape.default(clone_210, [128, 48, 196]);  clone_210 = None
        bmm_32: "f32[128, 196, 196]" = torch.ops.aten.bmm.default(view_387, view_388);  view_387 = view_388 = None
        view_389: "f32[8, 16, 196, 196]" = torch.ops.aten.reshape.default(bmm_32, [8, 16, 196, 196]);  bmm_32 = None
        
        # No stacktrace found for following nodes
        mul_tensor_14: "f32[8, 16, 196, 196]" = torch.ops.aten.mul.Tensor(view_389, 1);  view_389 = None
        amax_default_7: "f32[8, 16, 196, 1]" = torch.ops.aten.amax.default(mul_tensor_14, [-1], True)
        sub_tensor_7: "f32[8, 16, 196, 196]" = torch.ops.aten.sub.Tensor(mul_tensor_14, amax_default_7);  mul_tensor_14 = amax_default_7 = None
        mul_tensor_15: "f32[8, 16, 196, 196]" = torch.ops.aten.mul.Tensor(sub_tensor_7, 0.14433756729740643);  sub_tensor_7 = None
        
         # File: /home/sahanp/.conda/envs/pytorch-benchmarks/lib/python3.12/site-packages/timm/models/convit.py:85 in get_attention, code: patch_score = patch_score.softmax(dim=-1)
        exp_30: "f32[8, 16, 196, 196]" = torch.ops.aten.exp.default(mul_tensor_15);  mul_tensor_15 = None
        sum_45: "f32[8, 16, 196, 1]" = torch.ops.aten.sum.dim_IntList(exp_30, [-1], True)
        div_44: "f32[8, 16, 196, 196]" = torch.ops.aten.div.Tensor(exp_30, sum_45);  exp_30 = sum_45 = None
        
         # File: /home/sahanp/.conda/envs/pytorch-benchmarks/lib/python3.12/site-packages/timm/models/convit.py:89 in get_attention, code: attn = (1. - torch.sigmoid(gating)) * patch_score + torch.sigmoid(gating) * pos_score
        mul_161: "f32[8, 16, 196, 196]" = torch.ops.aten.mul.Tensor(sub_90, div_44);  sub_90 = div_44 = None
        sigmoid_29: "f32[1, 16, 1, 1]" = torch.ops.aten.sigmoid.default(view_390);  view_390 = None
        
         # File: /home/sahanp/.conda/envs/pytorch-benchmarks/lib/python3.12/site-packages/timm/models/convit.py:119 in get_rel_indices, code: rel_indices = torch.zeros(1, num_patches, num_patches, 3)
        full_default_4: "f32[1, 196, 196, 3]" = torch.ops.aten.full.default([1, 196, 196, 3], 0, dtype = torch.float32, layout = torch.strided, device = device(type='cpu'), pin_memory = False)
        
         # File: /home/sahanp/.conda/envs/pytorch-benchmarks/lib/python3.12/site-packages/timm/models/convit.py:124 in get_rel_indices, code: rel_indices[:, :, :, 2] = indd.unsqueeze(0)
        select_40: "f32[1, 196, 196]" = torch.ops.aten.select.int(full_default_4, 3, 2)
        
         # File: /home/sahanp/.conda/envs/pytorch-benchmarks/lib/python3.12/site-packages/timm/models/convit.py:120 in get_rel_indices, code: ind = torch.arange(img_size).view(1, -1) - torch.arange(img_size).view(-1, 1)
        iota_8: "i64[14]" = torch.ops.prims.iota.default(14, start = 0, step = 1, dtype = torch.int64, device = device(type='cpu'), requires_grad = False)
        view_105: "i64[1, 14]" = torch.ops.aten.reshape.default(iota_8, [1, -1]);  iota_8 = None
        iota_9: "i64[14]" = torch.ops.prims.iota.default(14, start = 0, step = 1, dtype = torch.int64, device = device(type='cpu'), requires_grad = False)
        view_106: "i64[14, 1]" = torch.ops.aten.reshape.default(iota_9, [-1, 1]);  iota_9 = None
        sub_25: "i64[14, 14]" = torch.ops.aten.sub.Tensor(view_105, view_106);  view_105 = view_106 = None
        
         # File: /home/sahanp/.conda/envs/pytorch-benchmarks/lib/python3.12/site-packages/timm/models/convit.py:121 in get_rel_indices, code: indx = ind.repeat(img_size, img_size)
        repeat_4: "i64[196, 196]" = torch.ops.aten.repeat.default(sub_25, [14, 14])
        
         # File: /home/sahanp/.conda/envs/pytorch-benchmarks/lib/python3.12/site-packages/timm/models/convit.py:123 in get_rel_indices, code: indd = indx ** 2 + indy ** 2
        pow_9: "i64[196, 196]" = torch.ops.aten.pow.Tensor_Scalar(repeat_4, 2)
        
         # File: /home/sahanp/.conda/envs/pytorch-benchmarks/lib/python3.12/site-packages/timm/models/convit.py:122 in get_rel_indices, code: indy = ind.repeat_interleave(img_size, dim=0).repeat_interleave(img_size, dim=1)
        unsqueeze_24: "i64[14, 1, 14]" = torch.ops.aten.unsqueeze.default(sub_25, 1);  sub_25 = None
        expand_29: "i64[14, 14, 14]" = torch.ops.aten.expand.default(unsqueeze_24, [14, 14, 14]);  unsqueeze_24 = None
        clone_58: "i64[14, 14, 14]" = torch.ops.aten.clone.default(expand_29, memory_format = torch.contiguous_format);  expand_29 = None
        view_107: "i64[196, 14]" = torch.ops.aten.reshape.default(clone_58, [196, 14]);  clone_58 = None
        unsqueeze_25: "i64[196, 14, 1]" = torch.ops.aten.unsqueeze.default(view_107, 2);  view_107 = None
        expand_30: "i64[196, 14, 14]" = torch.ops.aten.expand.default(unsqueeze_25, [196, 14, 14]);  unsqueeze_25 = None
        clone_59: "i64[196, 14, 14]" = torch.ops.aten.clone.default(expand_30, memory_format = torch.contiguous_format);  expand_30 = None
        view_108: "i64[196, 196]" = torch.ops.aten.reshape.default(clone_59, [196, 196]);  clone_59 = None
        
         # File: /home/sahanp/.conda/envs/pytorch-benchmarks/lib/python3.12/site-packages/timm/models/convit.py:123 in get_rel_indices, code: indd = indx ** 2 + indy ** 2
        pow_10: "i64[196, 196]" = torch.ops.aten.pow.Tensor_Scalar(view_108, 2)
        add_43: "i64[196, 196]" = torch.ops.aten.add.Tensor(pow_9, pow_10);  pow_9 = pow_10 = None
        
         # File: /home/sahanp/.conda/envs/pytorch-benchmarks/lib/python3.12/site-packages/timm/models/convit.py:124 in get_rel_indices, code: rel_indices[:, :, :, 2] = indd.unsqueeze(0)
        unsqueeze_26: "i64[1, 196, 196]" = torch.ops.aten.unsqueeze.default(add_43, 0);  add_43 = None
        copy_12: "f32[1, 196, 196]" = torch.ops.aten.copy.default(select_40, unsqueeze_26);  select_40 = unsqueeze_26 = None
        
        # No stacktrace found for following nodes
        select_scatter_default_12: "f32[1, 196, 196, 3]" = torch.ops.aten.select_scatter.default(full_default_4, copy_12, 3, 2);  full_default_4 = copy_12 = None
        
         # File: /home/sahanp/.conda/envs/pytorch-benchmarks/lib/python3.12/site-packages/timm/models/convit.py:125 in get_rel_indices, code: rel_indices[:, :, :, 1] = indy.unsqueeze(0)
        select_43: "f32[1, 196, 196]" = torch.ops.aten.select.int(select_scatter_default_12, 3, 1)
        unsqueeze_27: "i64[1, 196, 196]" = torch.ops.aten.unsqueeze.default(view_108, 0);  view_108 = None
        copy_13: "f32[1, 196, 196]" = torch.ops.aten.copy.default(select_43, unsqueeze_27);  select_43 = unsqueeze_27 = None
        
        # No stacktrace found for following nodes
        select_scatter_default_13: "f32[1, 196, 196, 3]" = torch.ops.aten.select_scatter.default(select_scatter_default_12, copy_13, 3, 1);  select_scatter_default_12 = copy_13 = None
        
         # File: /home/sahanp/.conda/envs/pytorch-benchmarks/lib/python3.12/site-packages/timm/models/convit.py:126 in get_rel_indices, code: rel_indices[:, :, :, 0] = indx.unsqueeze(0)
        select_46: "f32[1, 196, 196]" = torch.ops.aten.select.int(select_scatter_default_13, 3, 0)
        unsqueeze_28: "i64[1, 196, 196]" = torch.ops.aten.unsqueeze.default(repeat_4, 0);  repeat_4 = None
        copy_14: "f32[1, 196, 196]" = torch.ops.aten.copy.default(select_46, unsqueeze_28);  select_46 = unsqueeze_28 = None
        
        # No stacktrace found for following nodes
        select_scatter_default_14: "f32[1, 196, 196, 3]" = torch.ops.aten.select_scatter.default(select_scatter_default_13, copy_14, 3, 0);  select_scatter_default_13 = copy_14 = None
        
         # File: /home/sahanp/.conda/envs/pytorch-benchmarks/lib/python3.12/site-packages/timm/models/convit.py:128 in get_rel_indices, code: return rel_indices.to(device)
        device_put_4: "f32[1, 196, 196, 3]" = torch.ops.prims.device_put.default(select_scatter_default_14, device(type='cuda', index=0));  select_scatter_default_14 = None
        
         # File: /home/sahanp/.conda/envs/pytorch-benchmarks/lib/python3.12/site-packages/timm/models/convit.py:82 in get_attention, code: pos_score = self.rel_indices.expand(B, -1, -1, -1)
        expand_100: "f32[8, 196, 196, 3]" = torch.ops.aten.expand.default(device_put_4, [8, -1, -1, -1])
        
         # File: /home/sahanp/.conda/envs/pytorch-benchmarks/lib/python3.12/site-packages/timm/models/convit.py:83 in get_attention, code: pos_score = self.pos_proj(pos_score).permute(0, 3, 1, 2)
        clone_208: "f32[8, 196, 196, 3]" = torch.ops.aten.clone.default(expand_100, memory_format = torch.contiguous_format);  expand_100 = None
        view_385: "f32[307328, 3]" = torch.ops.aten.reshape.default(clone_208, [307328, 3]);  clone_208 = None
        permute_173: "f32[3, 16]" = torch.ops.aten.permute.default(arg68_1, [1, 0]);  arg68_1 = None
        mm_45: "f32[307328, 16]" = torch.ops.aten.mm.default(view_385, permute_173);  view_385 = permute_173 = None
        view_386: "f32[8, 196, 196, 16]" = torch.ops.aten.reshape.default(mm_45, [8, 196, 196, 16]);  mm_45 = None
        add_156: "f32[8, 196, 196, 16]" = torch.ops.aten.add.Tensor(view_386, arg69_1);  view_386 = arg69_1 = None
        permute_174: "f32[8, 16, 196, 196]" = torch.ops.aten.permute.default(add_156, [0, 3, 1, 2]);  add_156 = None
        
         # File: /home/sahanp/.conda/envs/pytorch-benchmarks/lib/python3.12/site-packages/timm/models/convit.py:86 in get_attention, code: pos_score = pos_score.softmax(dim=-1)
        clone_211: "f32[8, 16, 196, 196]" = torch.ops.aten.clone.default(permute_174, memory_format = torch.contiguous_format);  permute_174 = None
        amax_31: "f32[8, 16, 196, 1]" = torch.ops.aten.amax.default(clone_211, [-1], True)
        sub_89: "f32[8, 16, 196, 196]" = torch.ops.aten.sub.Tensor(clone_211, amax_31);  clone_211 = amax_31 = None
        exp_31: "f32[8, 16, 196, 196]" = torch.ops.aten.exp.default(sub_89);  sub_89 = None
        sum_46: "f32[8, 16, 196, 1]" = torch.ops.aten.sum.dim_IntList(exp_31, [-1], True)
        div_45: "f32[8, 16, 196, 196]" = torch.ops.aten.div.Tensor(exp_31, sum_46);  exp_31 = sum_46 = None
        
         # File: /home/sahanp/.conda/envs/pytorch-benchmarks/lib/python3.12/site-packages/timm/models/convit.py:89 in get_attention, code: attn = (1. - torch.sigmoid(gating)) * patch_score + torch.sigmoid(gating) * pos_score
        mul_162: "f32[8, 16, 196, 196]" = torch.ops.aten.mul.Tensor(sigmoid_29, div_45);  sigmoid_29 = div_45 = None
        add_157: "f32[8, 16, 196, 196]" = torch.ops.aten.add.Tensor(mul_161, mul_162);  mul_161 = mul_162 = None
        
         # File: /home/sahanp/.conda/envs/pytorch-benchmarks/lib/python3.12/site-packages/timm/models/convit.py:90 in get_attention, code: attn /= attn.sum(dim=-1).unsqueeze(-1)
        sum_47: "f32[8, 16, 196]" = torch.ops.aten.sum.dim_IntList(add_157, [-1])
        unsqueeze_64: "f32[8, 16, 196, 1]" = torch.ops.aten.unsqueeze.default(sum_47, -1);  sum_47 = None
        div_46: "f32[8, 16, 196, 196]" = torch.ops.aten.div.Tensor(add_157, unsqueeze_64);  add_157 = unsqueeze_64 = None
        
         # File: /home/sahanp/.conda/envs/pytorch-benchmarks/lib/python3.12/site-packages/timm/models/convit.py:73 in forward, code: x = (attn @ v).transpose(1, 2).reshape(B, N, C)
        expand_103: "f32[8, 16, 196, 196]" = torch.ops.aten.expand.default(div_46, [8, 16, 196, 196]);  div_46 = None
        view_394: "f32[128, 196, 196]" = torch.ops.aten.reshape.default(expand_103, [128, 196, 196]);  expand_103 = None
        
         # File: /home/sahanp/.conda/envs/pytorch-benchmarks/lib/python3.12/site-packages/timm/models/convit.py:72 in forward, code: v = self.v(x).reshape(B, N, self.num_heads, C // self.num_heads).permute(0, 2, 1, 3)
        view_391: "f32[1568, 768]" = torch.ops.aten.reshape.default(add_155, [1568, 768]);  add_155 = None
        permute_176: "f32[768, 768]" = torch.ops.aten.permute.default(arg71_1, [1, 0]);  arg71_1 = None
        mm_46: "f32[1568, 768]" = torch.ops.aten.mm.default(view_391, permute_176);  view_391 = permute_176 = None
        view_392: "f32[8, 196, 768]" = torch.ops.aten.reshape.default(mm_46, [8, 196, 768]);  mm_46 = None
        view_393: "f32[8, 196, 16, 48]" = torch.ops.aten.reshape.default(view_392, [8, 196, 16, 48]);  view_392 = None
        permute_177: "f32[8, 16, 196, 48]" = torch.ops.aten.permute.default(view_393, [0, 2, 1, 3]);  view_393 = None
        
         # File: /home/sahanp/.conda/envs/pytorch-benchmarks/lib/python3.12/site-packages/timm/models/convit.py:73 in forward, code: x = (attn @ v).transpose(1, 2).reshape(B, N, C)
        expand_104: "f32[8, 16, 196, 48]" = torch.ops.aten.expand.default(permute_177, [8, 16, 196, 48]);  permute_177 = None
        clone_213: "f32[8, 16, 196, 48]" = torch.ops.aten.clone.default(expand_104, memory_format = torch.contiguous_format);  expand_104 = None
        view_395: "f32[128, 196, 48]" = torch.ops.aten.reshape.default(clone_213, [128, 196, 48]);  clone_213 = None
        bmm_33: "f32[128, 196, 48]" = torch.ops.aten.bmm.default(view_394, view_395);  view_394 = view_395 = None
        view_396: "f32[8, 16, 196, 48]" = torch.ops.aten.reshape.default(bmm_33, [8, 16, 196, 48]);  bmm_33 = None
        permute_178: "f32[8, 196, 16, 48]" = torch.ops.aten.permute.default(view_396, [0, 2, 1, 3]);  view_396 = None
        clone_214: "f32[8, 196, 16, 48]" = torch.ops.aten.clone.default(permute_178, memory_format = torch.contiguous_format);  permute_178 = None
        view_397: "f32[8, 196, 768]" = torch.ops.aten.reshape.default(clone_214, [8, 196, 768]);  clone_214 = None
        
         # File: /home/sahanp/.conda/envs/pytorch-benchmarks/lib/python3.12/site-packages/timm/models/convit.py:74 in forward, code: x = self.proj(x)
        view_398: "f32[1568, 768]" = torch.ops.aten.reshape.default(view_397, [1568, 768]);  view_397 = None
        permute_179: "f32[768, 768]" = torch.ops.aten.permute.default(arg72_1, [1, 0]);  arg72_1 = None
        
        # No stacktrace found for following nodes
        mm_default_23: "f32[1568, 768]" = torch.ops.aten.mm.default(view_398, permute_179);  view_398 = permute_179 = None
        add_tensor_23: "f32[1568, 768]" = torch.ops.aten.add.Tensor(mm_default_23, arg73_1);  mm_default_23 = arg73_1 = None
        
         # File: /home/sahanp/.conda/envs/pytorch-benchmarks/lib/python3.12/site-packages/timm/models/convit.py:74 in forward, code: x = self.proj(x)
        view_399: "f32[8, 196, 768]" = torch.ops.aten.reshape.default(add_tensor_23, [8, 196, 768]);  add_tensor_23 = None
        
         # File: /home/sahanp/.conda/envs/pytorch-benchmarks/lib/python3.12/site-packages/timm/models/convit.py:233 in forward, code: x = x + self.drop_path(self.attn(self.norm1(x)))
        add_158: "f32[8, 196, 768]" = torch.ops.aten.add.Tensor(add_153, view_399);  add_153 = view_399 = None
        
         # File: /home/sahanp/.conda/envs/pytorch-benchmarks/lib/python3.12/site-packages/timm/layers/norm.py:57 in forward, code: x = F.layer_norm(x, self.normalized_shape, self.weight, self.bias, self.eps)
        clone_216: "f32[8, 196, 768]" = torch.ops.aten.clone.default(add_158, memory_format = torch.contiguous_format)
        var_mean_34 = torch.ops.aten.var_mean.correction(clone_216, [2], correction = 0, keepdim = True)
        getitem_74: "f32[8, 196, 1]" = var_mean_34[0]
        getitem_75: "f32[8, 196, 1]" = var_mean_34[1];  var_mean_34 = None
        sub_91: "f32[8, 196, 768]" = torch.ops.aten.sub.Tensor(clone_216, getitem_75);  clone_216 = getitem_75 = None
        add_159: "f32[8, 196, 1]" = torch.ops.aten.add.Tensor(getitem_74, 1e-06);  getitem_74 = None
        rsqrt_34: "f32[8, 196, 1]" = torch.ops.aten.rsqrt.default(add_159);  add_159 = None
        mul_163: "f32[8, 196, 768]" = torch.ops.aten.mul.Tensor(sub_91, rsqrt_34);  sub_91 = rsqrt_34 = None
        mul_164: "f32[8, 196, 768]" = torch.ops.aten.mul.Tensor(mul_163, arg74_1);  mul_163 = arg74_1 = None
        add_160: "f32[8, 196, 768]" = torch.ops.aten.add.Tensor(mul_164, arg75_1);  mul_164 = arg75_1 = None
        
         # File: /home/sahanp/.conda/envs/pytorch-benchmarks/lib/python3.12/site-packages/timm/layers/mlp.py:42 in forward, code: x = self.fc1(x)
        view_400: "f32[1568, 768]" = torch.ops.aten.reshape.default(add_160, [1568, 768]);  add_160 = None
        permute_180: "f32[768, 3072]" = torch.ops.aten.permute.default(arg76_1, [1, 0]);  arg76_1 = None
        
        # No stacktrace found for following nodes
        mm_default_22: "f32[1568, 3072]" = torch.ops.aten.mm.default(view_400, permute_180);  view_400 = permute_180 = None
        add_tensor_22: "f32[1568, 3072]" = torch.ops.aten.add.Tensor(mm_default_22, arg77_1);  mm_default_22 = arg77_1 = None
        
         # File: /home/sahanp/.conda/envs/pytorch-benchmarks/lib/python3.12/site-packages/timm/layers/mlp.py:42 in forward, code: x = self.fc1(x)
        view_401: "f32[8, 196, 3072]" = torch.ops.aten.reshape.default(add_tensor_22, [8, 196, 3072]);  add_tensor_22 = None
        
         # File: /home/sahanp/.conda/envs/pytorch-benchmarks/lib/python3.12/site-packages/timm/layers/mlp.py:43 in forward, code: x = self.act(x)
        mul_165: "f32[8, 196, 3072]" = torch.ops.aten.mul.Tensor(view_401, 0.5)
        mul_166: "f32[8, 196, 3072]" = torch.ops.aten.mul.Tensor(view_401, 0.7071067811865476);  view_401 = None
        erf_16: "f32[8, 196, 3072]" = torch.ops.aten.erf.default(mul_166);  mul_166 = None
        add_161: "f32[8, 196, 3072]" = torch.ops.aten.add.Tensor(erf_16, 1);  erf_16 = None
        mul_167: "f32[8, 196, 3072]" = torch.ops.aten.mul.Tensor(mul_165, add_161);  mul_165 = add_161 = None
        
         # File: /home/sahanp/.conda/envs/pytorch-benchmarks/lib/python3.12/site-packages/timm/layers/mlp.py:46 in forward, code: x = self.fc2(x)
        view_402: "f32[1568, 3072]" = torch.ops.aten.reshape.default(mul_167, [1568, 3072]);  mul_167 = None
        permute_181: "f32[3072, 768]" = torch.ops.aten.permute.default(arg78_1, [1, 0]);  arg78_1 = None
        
        # No stacktrace found for following nodes
        mm_default_21: "f32[1568, 768]" = torch.ops.aten.mm.default(view_402, permute_181);  view_402 = permute_181 = None
        add_tensor_21: "f32[1568, 768]" = torch.ops.aten.add.Tensor(mm_default_21, arg79_1);  mm_default_21 = arg79_1 = None
        
         # File: /home/sahanp/.conda/envs/pytorch-benchmarks/lib/python3.12/site-packages/timm/layers/mlp.py:46 in forward, code: x = self.fc2(x)
        view_403: "f32[8, 196, 768]" = torch.ops.aten.reshape.default(add_tensor_21, [8, 196, 768]);  add_tensor_21 = None
        
         # File: /home/sahanp/.conda/envs/pytorch-benchmarks/lib/python3.12/site-packages/timm/models/convit.py:234 in forward, code: x = x + self.drop_path(self.mlp(self.norm2(x)))
        add_162: "f32[8, 196, 768]" = torch.ops.aten.add.Tensor(add_158, view_403);  add_158 = view_403 = None
        
         # File: /home/sahanp/.conda/envs/pytorch-benchmarks/lib/python3.12/site-packages/timm/layers/norm.py:57 in forward, code: x = F.layer_norm(x, self.normalized_shape, self.weight, self.bias, self.eps)
        clone_219: "f32[8, 196, 768]" = torch.ops.aten.clone.default(add_162, memory_format = torch.contiguous_format)
        var_mean_35 = torch.ops.aten.var_mean.correction(clone_219, [2], correction = 0, keepdim = True)
        getitem_76: "f32[8, 196, 1]" = var_mean_35[0]
        getitem_77: "f32[8, 196, 1]" = var_mean_35[1];  var_mean_35 = None
        
         # File: /home/sahanp/.conda/envs/pytorch-benchmarks/lib/python3.12/site-packages/timm/models/convit.py:88 in get_attention, code: gating = self.gating_param.view(1, -1, 1, 1)
        view_412: "f32[1, 16, 1, 1]" = torch.ops.aten.reshape.default(arg85_1, [1, -1, 1, 1]);  arg85_1 = None
        
         # File: /home/sahanp/.conda/envs/pytorch-benchmarks/lib/python3.12/site-packages/timm/models/convit.py:89 in get_attention, code: attn = (1. - torch.sigmoid(gating)) * patch_score + torch.sigmoid(gating) * pos_score
        sigmoid_30: "f32[1, 16, 1, 1]" = torch.ops.aten.sigmoid.default(view_412)
        sub_95: "f32[1, 16, 1, 1]" = torch.ops.aten.sub.Tensor(1.0, sigmoid_30);  sigmoid_30 = None
        
         # File: /home/sahanp/.conda/envs/pytorch-benchmarks/lib/python3.12/site-packages/timm/layers/norm.py:57 in forward, code: x = F.layer_norm(x, self.normalized_shape, self.weight, self.bias, self.eps)
        sub_92: "f32[8, 196, 768]" = torch.ops.aten.sub.Tensor(clone_219, getitem_77);  clone_219 = getitem_77 = None
        add_163: "f32[8, 196, 1]" = torch.ops.aten.add.Tensor(getitem_76, 1e-06);  getitem_76 = None
        rsqrt_35: "f32[8, 196, 1]" = torch.ops.aten.rsqrt.default(add_163);  add_163 = None
        mul_168: "f32[8, 196, 768]" = torch.ops.aten.mul.Tensor(sub_92, rsqrt_35);  sub_92 = rsqrt_35 = None
        mul_169: "f32[8, 196, 768]" = torch.ops.aten.mul.Tensor(mul_168, arg80_1);  mul_168 = arg80_1 = None
        add_164: "f32[8, 196, 768]" = torch.ops.aten.add.Tensor(mul_169, arg81_1);  mul_169 = arg81_1 = None
        
         # File: /home/sahanp/.conda/envs/pytorch-benchmarks/lib/python3.12/site-packages/timm/models/convit.py:80 in get_attention, code: qk = self.qk(x).reshape(B, N, 2, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)
        view_404: "f32[1568, 768]" = torch.ops.aten.reshape.default(add_164, [1568, 768])
        permute_182: "f32[768, 1536]" = torch.ops.aten.permute.default(arg82_1, [1, 0]);  arg82_1 = None
        mm_47: "f32[1568, 1536]" = torch.ops.aten.mm.default(view_404, permute_182);  view_404 = permute_182 = None
        view_405: "f32[8, 196, 1536]" = torch.ops.aten.reshape.default(mm_47, [8, 196, 1536]);  mm_47 = None
        view_406: "f32[8, 196, 2, 16, 48]" = torch.ops.aten.reshape.default(view_405, [8, 196, 2, 16, 48]);  view_405 = None
        permute_183: "f32[2, 8, 16, 196, 48]" = torch.ops.aten.permute.default(view_406, [2, 0, 3, 1, 4]);  view_406 = None
        
         # File: /home/sahanp/.conda/envs/pytorch-benchmarks/lib/python3.12/site-packages/timm/models/convit.py:81 in get_attention, code: q, k = qk[0], qk[1]
        select_111: "f32[8, 16, 196, 48]" = torch.ops.aten.select.int(permute_183, 0, 0)
        
         # File: /home/sahanp/.conda/envs/pytorch-benchmarks/lib/python3.12/site-packages/timm/models/convit.py:84 in get_attention, code: patch_score = (q @ k.transpose(-2, -1)) * self.scale
        expand_106: "f32[8, 16, 196, 48]" = torch.ops.aten.expand.default(select_111, [8, 16, 196, 48]);  select_111 = None
        clone_221: "f32[8, 16, 196, 48]" = torch.ops.aten.clone.default(expand_106, memory_format = torch.contiguous_format);  expand_106 = None
        view_409: "f32[128, 196, 48]" = torch.ops.aten.reshape.default(clone_221, [128, 196, 48]);  clone_221 = None
        
         # File: /home/sahanp/.conda/envs/pytorch-benchmarks/lib/python3.12/site-packages/timm/models/convit.py:81 in get_attention, code: q, k = qk[0], qk[1]
        select_112: "f32[8, 16, 196, 48]" = torch.ops.aten.select.int(permute_183, 0, 1);  permute_183 = None
        
         # File: /home/sahanp/.conda/envs/pytorch-benchmarks/lib/python3.12/site-packages/timm/models/convit.py:84 in get_attention, code: patch_score = (q @ k.transpose(-2, -1)) * self.scale
        permute_186: "f32[8, 16, 48, 196]" = torch.ops.aten.permute.default(select_112, [0, 1, 3, 2]);  select_112 = None
        expand_107: "f32[8, 16, 48, 196]" = torch.ops.aten.expand.default(permute_186, [8, 16, 48, 196]);  permute_186 = None
        clone_222: "f32[8, 16, 48, 196]" = torch.ops.aten.clone.default(expand_107, memory_format = torch.contiguous_format);  expand_107 = None
        view_410: "f32[128, 48, 196]" = torch.ops.aten.reshape.default(clone_222, [128, 48, 196]);  clone_222 = None
        bmm_34: "f32[128, 196, 196]" = torch.ops.aten.bmm.default(view_409, view_410);  view_409 = view_410 = None
        view_411: "f32[8, 16, 196, 196]" = torch.ops.aten.reshape.default(bmm_34, [8, 16, 196, 196]);  bmm_34 = None
        
        # No stacktrace found for following nodes
        mul_tensor_12: "f32[8, 16, 196, 196]" = torch.ops.aten.mul.Tensor(view_411, 1);  view_411 = None
        amax_default_6: "f32[8, 16, 196, 1]" = torch.ops.aten.amax.default(mul_tensor_12, [-1], True)
        sub_tensor_6: "f32[8, 16, 196, 196]" = torch.ops.aten.sub.Tensor(mul_tensor_12, amax_default_6);  mul_tensor_12 = amax_default_6 = None
        mul_tensor_13: "f32[8, 16, 196, 196]" = torch.ops.aten.mul.Tensor(sub_tensor_6, 0.14433756729740643);  sub_tensor_6 = None
        
         # File: /home/sahanp/.conda/envs/pytorch-benchmarks/lib/python3.12/site-packages/timm/models/convit.py:85 in get_attention, code: patch_score = patch_score.softmax(dim=-1)
        exp_32: "f32[8, 16, 196, 196]" = torch.ops.aten.exp.default(mul_tensor_13);  mul_tensor_13 = None
        sum_48: "f32[8, 16, 196, 1]" = torch.ops.aten.sum.dim_IntList(exp_32, [-1], True)
        div_47: "f32[8, 16, 196, 196]" = torch.ops.aten.div.Tensor(exp_32, sum_48);  exp_32 = sum_48 = None
        
         # File: /home/sahanp/.conda/envs/pytorch-benchmarks/lib/python3.12/site-packages/timm/models/convit.py:89 in get_attention, code: attn = (1. - torch.sigmoid(gating)) * patch_score + torch.sigmoid(gating) * pos_score
        mul_171: "f32[8, 16, 196, 196]" = torch.ops.aten.mul.Tensor(sub_95, div_47);  sub_95 = div_47 = None
        sigmoid_31: "f32[1, 16, 1, 1]" = torch.ops.aten.sigmoid.default(view_412);  view_412 = None
        
         # File: /home/sahanp/.conda/envs/pytorch-benchmarks/lib/python3.12/site-packages/timm/models/convit.py:119 in get_rel_indices, code: rel_indices = torch.zeros(1, num_patches, num_patches, 3)
        full_default_5: "f32[1, 196, 196, 3]" = torch.ops.aten.full.default([1, 196, 196, 3], 0, dtype = torch.float32, layout = torch.strided, device = device(type='cpu'), pin_memory = False)
        
         # File: /home/sahanp/.conda/envs/pytorch-benchmarks/lib/python3.12/site-packages/timm/models/convit.py:124 in get_rel_indices, code: rel_indices[:, :, :, 2] = indd.unsqueeze(0)
        select_50: "f32[1, 196, 196]" = torch.ops.aten.select.int(full_default_5, 3, 2)
        
         # File: /home/sahanp/.conda/envs/pytorch-benchmarks/lib/python3.12/site-packages/timm/models/convit.py:120 in get_rel_indices, code: ind = torch.arange(img_size).view(1, -1) - torch.arange(img_size).view(-1, 1)
        iota_10: "i64[14]" = torch.ops.prims.iota.default(14, start = 0, step = 1, dtype = torch.int64, device = device(type='cpu'), requires_grad = False)
        view_131: "i64[1, 14]" = torch.ops.aten.reshape.default(iota_10, [1, -1]);  iota_10 = None
        iota_11: "i64[14]" = torch.ops.prims.iota.default(14, start = 0, step = 1, dtype = torch.int64, device = device(type='cpu'), requires_grad = False)
        view_132: "i64[14, 1]" = torch.ops.aten.reshape.default(iota_11, [-1, 1]);  iota_11 = None
        sub_31: "i64[14, 14]" = torch.ops.aten.sub.Tensor(view_131, view_132);  view_131 = view_132 = None
        
         # File: /home/sahanp/.conda/envs/pytorch-benchmarks/lib/python3.12/site-packages/timm/models/convit.py:121 in get_rel_indices, code: indx = ind.repeat(img_size, img_size)
        repeat_5: "i64[196, 196]" = torch.ops.aten.repeat.default(sub_31, [14, 14])
        
         # File: /home/sahanp/.conda/envs/pytorch-benchmarks/lib/python3.12/site-packages/timm/models/convit.py:123 in get_rel_indices, code: indd = indx ** 2 + indy ** 2
        pow_11: "i64[196, 196]" = torch.ops.aten.pow.Tensor_Scalar(repeat_5, 2)
        
         # File: /home/sahanp/.conda/envs/pytorch-benchmarks/lib/python3.12/site-packages/timm/models/convit.py:122 in get_rel_indices, code: indy = ind.repeat_interleave(img_size, dim=0).repeat_interleave(img_size, dim=1)
        unsqueeze_30: "i64[14, 1, 14]" = torch.ops.aten.unsqueeze.default(sub_31, 1);  sub_31 = None
        expand_36: "i64[14, 14, 14]" = torch.ops.aten.expand.default(unsqueeze_30, [14, 14, 14]);  unsqueeze_30 = None
        clone_72: "i64[14, 14, 14]" = torch.ops.aten.clone.default(expand_36, memory_format = torch.contiguous_format);  expand_36 = None
        view_133: "i64[196, 14]" = torch.ops.aten.reshape.default(clone_72, [196, 14]);  clone_72 = None
        unsqueeze_31: "i64[196, 14, 1]" = torch.ops.aten.unsqueeze.default(view_133, 2);  view_133 = None
        expand_37: "i64[196, 14, 14]" = torch.ops.aten.expand.default(unsqueeze_31, [196, 14, 14]);  unsqueeze_31 = None
        clone_73: "i64[196, 14, 14]" = torch.ops.aten.clone.default(expand_37, memory_format = torch.contiguous_format);  expand_37 = None
        view_134: "i64[196, 196]" = torch.ops.aten.reshape.default(clone_73, [196, 196]);  clone_73 = None
        
         # File: /home/sahanp/.conda/envs/pytorch-benchmarks/lib/python3.12/site-packages/timm/models/convit.py:123 in get_rel_indices, code: indd = indx ** 2 + indy ** 2
        pow_12: "i64[196, 196]" = torch.ops.aten.pow.Tensor_Scalar(view_134, 2)
        add_53: "i64[196, 196]" = torch.ops.aten.add.Tensor(pow_11, pow_12);  pow_11 = pow_12 = None
        
         # File: /home/sahanp/.conda/envs/pytorch-benchmarks/lib/python3.12/site-packages/timm/models/convit.py:124 in get_rel_indices, code: rel_indices[:, :, :, 2] = indd.unsqueeze(0)
        unsqueeze_32: "i64[1, 196, 196]" = torch.ops.aten.unsqueeze.default(add_53, 0);  add_53 = None
        copy_15: "f32[1, 196, 196]" = torch.ops.aten.copy.default(select_50, unsqueeze_32);  select_50 = unsqueeze_32 = None
        
        # No stacktrace found for following nodes
        select_scatter_default_15: "f32[1, 196, 196, 3]" = torch.ops.aten.select_scatter.default(full_default_5, copy_15, 3, 2);  full_default_5 = copy_15 = None
        
         # File: /home/sahanp/.conda/envs/pytorch-benchmarks/lib/python3.12/site-packages/timm/models/convit.py:125 in get_rel_indices, code: rel_indices[:, :, :, 1] = indy.unsqueeze(0)
        select_53: "f32[1, 196, 196]" = torch.ops.aten.select.int(select_scatter_default_15, 3, 1)
        unsqueeze_33: "i64[1, 196, 196]" = torch.ops.aten.unsqueeze.default(view_134, 0);  view_134 = None
        copy_16: "f32[1, 196, 196]" = torch.ops.aten.copy.default(select_53, unsqueeze_33);  select_53 = unsqueeze_33 = None
        
        # No stacktrace found for following nodes
        select_scatter_default_16: "f32[1, 196, 196, 3]" = torch.ops.aten.select_scatter.default(select_scatter_default_15, copy_16, 3, 1);  select_scatter_default_15 = copy_16 = None
        
         # File: /home/sahanp/.conda/envs/pytorch-benchmarks/lib/python3.12/site-packages/timm/models/convit.py:126 in get_rel_indices, code: rel_indices[:, :, :, 0] = indx.unsqueeze(0)
        select_56: "f32[1, 196, 196]" = torch.ops.aten.select.int(select_scatter_default_16, 3, 0)
        unsqueeze_34: "i64[1, 196, 196]" = torch.ops.aten.unsqueeze.default(repeat_5, 0);  repeat_5 = None
        copy_17: "f32[1, 196, 196]" = torch.ops.aten.copy.default(select_56, unsqueeze_34);  select_56 = unsqueeze_34 = None
        
        # No stacktrace found for following nodes
        select_scatter_default_17: "f32[1, 196, 196, 3]" = torch.ops.aten.select_scatter.default(select_scatter_default_16, copy_17, 3, 0);  select_scatter_default_16 = copy_17 = None
        
         # File: /home/sahanp/.conda/envs/pytorch-benchmarks/lib/python3.12/site-packages/timm/models/convit.py:128 in get_rel_indices, code: return rel_indices.to(device)
        device_put_5: "f32[1, 196, 196, 3]" = torch.ops.prims.device_put.default(select_scatter_default_17, device(type='cuda', index=0));  select_scatter_default_17 = None
        
         # File: /home/sahanp/.conda/envs/pytorch-benchmarks/lib/python3.12/site-packages/timm/models/convit.py:82 in get_attention, code: pos_score = self.rel_indices.expand(B, -1, -1, -1)
        expand_105: "f32[8, 196, 196, 3]" = torch.ops.aten.expand.default(device_put_5, [8, -1, -1, -1])
        
         # File: /home/sahanp/.conda/envs/pytorch-benchmarks/lib/python3.12/site-packages/timm/models/convit.py:83 in get_attention, code: pos_score = self.pos_proj(pos_score).permute(0, 3, 1, 2)
        clone_220: "f32[8, 196, 196, 3]" = torch.ops.aten.clone.default(expand_105, memory_format = torch.contiguous_format);  expand_105 = None
        view_407: "f32[307328, 3]" = torch.ops.aten.reshape.default(clone_220, [307328, 3]);  clone_220 = None
        permute_184: "f32[3, 16]" = torch.ops.aten.permute.default(arg83_1, [1, 0]);  arg83_1 = None
        mm_48: "f32[307328, 16]" = torch.ops.aten.mm.default(view_407, permute_184);  view_407 = permute_184 = None
        view_408: "f32[8, 196, 196, 16]" = torch.ops.aten.reshape.default(mm_48, [8, 196, 196, 16]);  mm_48 = None
        add_165: "f32[8, 196, 196, 16]" = torch.ops.aten.add.Tensor(view_408, arg84_1);  view_408 = arg84_1 = None
        permute_185: "f32[8, 16, 196, 196]" = torch.ops.aten.permute.default(add_165, [0, 3, 1, 2]);  add_165 = None
        
         # File: /home/sahanp/.conda/envs/pytorch-benchmarks/lib/python3.12/site-packages/timm/models/convit.py:86 in get_attention, code: pos_score = pos_score.softmax(dim=-1)
        clone_223: "f32[8, 16, 196, 196]" = torch.ops.aten.clone.default(permute_185, memory_format = torch.contiguous_format);  permute_185 = None
        amax_33: "f32[8, 16, 196, 1]" = torch.ops.aten.amax.default(clone_223, [-1], True)
        sub_94: "f32[8, 16, 196, 196]" = torch.ops.aten.sub.Tensor(clone_223, amax_33);  clone_223 = amax_33 = None
        exp_33: "f32[8, 16, 196, 196]" = torch.ops.aten.exp.default(sub_94);  sub_94 = None
        sum_49: "f32[8, 16, 196, 1]" = torch.ops.aten.sum.dim_IntList(exp_33, [-1], True)
        div_48: "f32[8, 16, 196, 196]" = torch.ops.aten.div.Tensor(exp_33, sum_49);  exp_33 = sum_49 = None
        
         # File: /home/sahanp/.conda/envs/pytorch-benchmarks/lib/python3.12/site-packages/timm/models/convit.py:89 in get_attention, code: attn = (1. - torch.sigmoid(gating)) * patch_score + torch.sigmoid(gating) * pos_score
        mul_172: "f32[8, 16, 196, 196]" = torch.ops.aten.mul.Tensor(sigmoid_31, div_48);  sigmoid_31 = div_48 = None
        add_166: "f32[8, 16, 196, 196]" = torch.ops.aten.add.Tensor(mul_171, mul_172);  mul_171 = mul_172 = None
        
         # File: /home/sahanp/.conda/envs/pytorch-benchmarks/lib/python3.12/site-packages/timm/models/convit.py:90 in get_attention, code: attn /= attn.sum(dim=-1).unsqueeze(-1)
        sum_50: "f32[8, 16, 196]" = torch.ops.aten.sum.dim_IntList(add_166, [-1])
        unsqueeze_65: "f32[8, 16, 196, 1]" = torch.ops.aten.unsqueeze.default(sum_50, -1);  sum_50 = None
        div_49: "f32[8, 16, 196, 196]" = torch.ops.aten.div.Tensor(add_166, unsqueeze_65);  add_166 = unsqueeze_65 = None
        
         # File: /home/sahanp/.conda/envs/pytorch-benchmarks/lib/python3.12/site-packages/timm/models/convit.py:73 in forward, code: x = (attn @ v).transpose(1, 2).reshape(B, N, C)
        expand_108: "f32[8, 16, 196, 196]" = torch.ops.aten.expand.default(div_49, [8, 16, 196, 196]);  div_49 = None
        view_416: "f32[128, 196, 196]" = torch.ops.aten.reshape.default(expand_108, [128, 196, 196]);  expand_108 = None
        
         # File: /home/sahanp/.conda/envs/pytorch-benchmarks/lib/python3.12/site-packages/timm/models/convit.py:72 in forward, code: v = self.v(x).reshape(B, N, self.num_heads, C // self.num_heads).permute(0, 2, 1, 3)
        view_413: "f32[1568, 768]" = torch.ops.aten.reshape.default(add_164, [1568, 768]);  add_164 = None
        permute_187: "f32[768, 768]" = torch.ops.aten.permute.default(arg86_1, [1, 0]);  arg86_1 = None
        mm_49: "f32[1568, 768]" = torch.ops.aten.mm.default(view_413, permute_187);  view_413 = permute_187 = None
        view_414: "f32[8, 196, 768]" = torch.ops.aten.reshape.default(mm_49, [8, 196, 768]);  mm_49 = None
        view_415: "f32[8, 196, 16, 48]" = torch.ops.aten.reshape.default(view_414, [8, 196, 16, 48]);  view_414 = None
        permute_188: "f32[8, 16, 196, 48]" = torch.ops.aten.permute.default(view_415, [0, 2, 1, 3]);  view_415 = None
        
         # File: /home/sahanp/.conda/envs/pytorch-benchmarks/lib/python3.12/site-packages/timm/models/convit.py:73 in forward, code: x = (attn @ v).transpose(1, 2).reshape(B, N, C)
        expand_109: "f32[8, 16, 196, 48]" = torch.ops.aten.expand.default(permute_188, [8, 16, 196, 48]);  permute_188 = None
        clone_225: "f32[8, 16, 196, 48]" = torch.ops.aten.clone.default(expand_109, memory_format = torch.contiguous_format);  expand_109 = None
        view_417: "f32[128, 196, 48]" = torch.ops.aten.reshape.default(clone_225, [128, 196, 48]);  clone_225 = None
        bmm_35: "f32[128, 196, 48]" = torch.ops.aten.bmm.default(view_416, view_417);  view_416 = view_417 = None
        view_418: "f32[8, 16, 196, 48]" = torch.ops.aten.reshape.default(bmm_35, [8, 16, 196, 48]);  bmm_35 = None
        permute_189: "f32[8, 196, 16, 48]" = torch.ops.aten.permute.default(view_418, [0, 2, 1, 3]);  view_418 = None
        clone_226: "f32[8, 196, 16, 48]" = torch.ops.aten.clone.default(permute_189, memory_format = torch.contiguous_format);  permute_189 = None
        view_419: "f32[8, 196, 768]" = torch.ops.aten.reshape.default(clone_226, [8, 196, 768]);  clone_226 = None
        
         # File: /home/sahanp/.conda/envs/pytorch-benchmarks/lib/python3.12/site-packages/timm/models/convit.py:74 in forward, code: x = self.proj(x)
        view_420: "f32[1568, 768]" = torch.ops.aten.reshape.default(view_419, [1568, 768]);  view_419 = None
        permute_190: "f32[768, 768]" = torch.ops.aten.permute.default(arg87_1, [1, 0]);  arg87_1 = None
        
        # No stacktrace found for following nodes
        mm_default_20: "f32[1568, 768]" = torch.ops.aten.mm.default(view_420, permute_190);  view_420 = permute_190 = None
        add_tensor_20: "f32[1568, 768]" = torch.ops.aten.add.Tensor(mm_default_20, arg88_1);  mm_default_20 = arg88_1 = None
        
         # File: /home/sahanp/.conda/envs/pytorch-benchmarks/lib/python3.12/site-packages/timm/models/convit.py:74 in forward, code: x = self.proj(x)
        view_421: "f32[8, 196, 768]" = torch.ops.aten.reshape.default(add_tensor_20, [8, 196, 768]);  add_tensor_20 = None
        
         # File: /home/sahanp/.conda/envs/pytorch-benchmarks/lib/python3.12/site-packages/timm/models/convit.py:233 in forward, code: x = x + self.drop_path(self.attn(self.norm1(x)))
        add_167: "f32[8, 196, 768]" = torch.ops.aten.add.Tensor(add_162, view_421);  add_162 = view_421 = None
        
         # File: /home/sahanp/.conda/envs/pytorch-benchmarks/lib/python3.12/site-packages/timm/layers/norm.py:57 in forward, code: x = F.layer_norm(x, self.normalized_shape, self.weight, self.bias, self.eps)
        clone_228: "f32[8, 196, 768]" = torch.ops.aten.clone.default(add_167, memory_format = torch.contiguous_format)
        var_mean_36 = torch.ops.aten.var_mean.correction(clone_228, [2], correction = 0, keepdim = True)
        getitem_78: "f32[8, 196, 1]" = var_mean_36[0]
        getitem_79: "f32[8, 196, 1]" = var_mean_36[1];  var_mean_36 = None
        sub_96: "f32[8, 196, 768]" = torch.ops.aten.sub.Tensor(clone_228, getitem_79);  clone_228 = getitem_79 = None
        add_168: "f32[8, 196, 1]" = torch.ops.aten.add.Tensor(getitem_78, 1e-06);  getitem_78 = None
        rsqrt_36: "f32[8, 196, 1]" = torch.ops.aten.rsqrt.default(add_168);  add_168 = None
        mul_173: "f32[8, 196, 768]" = torch.ops.aten.mul.Tensor(sub_96, rsqrt_36);  sub_96 = rsqrt_36 = None
        mul_174: "f32[8, 196, 768]" = torch.ops.aten.mul.Tensor(mul_173, arg89_1);  mul_173 = arg89_1 = None
        add_169: "f32[8, 196, 768]" = torch.ops.aten.add.Tensor(mul_174, arg90_1);  mul_174 = arg90_1 = None
        
         # File: /home/sahanp/.conda/envs/pytorch-benchmarks/lib/python3.12/site-packages/timm/layers/mlp.py:42 in forward, code: x = self.fc1(x)
        view_422: "f32[1568, 768]" = torch.ops.aten.reshape.default(add_169, [1568, 768]);  add_169 = None
        permute_191: "f32[768, 3072]" = torch.ops.aten.permute.default(arg91_1, [1, 0]);  arg91_1 = None
        
        # No stacktrace found for following nodes
        mm_default_19: "f32[1568, 3072]" = torch.ops.aten.mm.default(view_422, permute_191);  view_422 = permute_191 = None
        add_tensor_19: "f32[1568, 3072]" = torch.ops.aten.add.Tensor(mm_default_19, arg92_1);  mm_default_19 = arg92_1 = None
        
         # File: /home/sahanp/.conda/envs/pytorch-benchmarks/lib/python3.12/site-packages/timm/layers/mlp.py:42 in forward, code: x = self.fc1(x)
        view_423: "f32[8, 196, 3072]" = torch.ops.aten.reshape.default(add_tensor_19, [8, 196, 3072]);  add_tensor_19 = None
        
         # File: /home/sahanp/.conda/envs/pytorch-benchmarks/lib/python3.12/site-packages/timm/layers/mlp.py:43 in forward, code: x = self.act(x)
        mul_175: "f32[8, 196, 3072]" = torch.ops.aten.mul.Tensor(view_423, 0.5)
        mul_176: "f32[8, 196, 3072]" = torch.ops.aten.mul.Tensor(view_423, 0.7071067811865476);  view_423 = None
        erf_17: "f32[8, 196, 3072]" = torch.ops.aten.erf.default(mul_176);  mul_176 = None
        add_170: "f32[8, 196, 3072]" = torch.ops.aten.add.Tensor(erf_17, 1);  erf_17 = None
        mul_177: "f32[8, 196, 3072]" = torch.ops.aten.mul.Tensor(mul_175, add_170);  mul_175 = add_170 = None
        
         # File: /home/sahanp/.conda/envs/pytorch-benchmarks/lib/python3.12/site-packages/timm/layers/mlp.py:46 in forward, code: x = self.fc2(x)
        view_424: "f32[1568, 3072]" = torch.ops.aten.reshape.default(mul_177, [1568, 3072]);  mul_177 = None
        permute_192: "f32[3072, 768]" = torch.ops.aten.permute.default(arg93_1, [1, 0]);  arg93_1 = None
        
        # No stacktrace found for following nodes
        mm_default_18: "f32[1568, 768]" = torch.ops.aten.mm.default(view_424, permute_192);  view_424 = permute_192 = None
        add_tensor_18: "f32[1568, 768]" = torch.ops.aten.add.Tensor(mm_default_18, arg94_1);  mm_default_18 = arg94_1 = None
        
         # File: /home/sahanp/.conda/envs/pytorch-benchmarks/lib/python3.12/site-packages/timm/layers/mlp.py:46 in forward, code: x = self.fc2(x)
        view_425: "f32[8, 196, 768]" = torch.ops.aten.reshape.default(add_tensor_18, [8, 196, 768]);  add_tensor_18 = None
        
         # File: /home/sahanp/.conda/envs/pytorch-benchmarks/lib/python3.12/site-packages/timm/models/convit.py:234 in forward, code: x = x + self.drop_path(self.mlp(self.norm2(x)))
        add_171: "f32[8, 196, 768]" = torch.ops.aten.add.Tensor(add_167, view_425);  add_167 = view_425 = None
        
         # File: /home/sahanp/.conda/envs/pytorch-benchmarks/lib/python3.12/site-packages/timm/layers/norm.py:57 in forward, code: x = F.layer_norm(x, self.normalized_shape, self.weight, self.bias, self.eps)
        clone_231: "f32[8, 196, 768]" = torch.ops.aten.clone.default(add_171, memory_format = torch.contiguous_format)
        var_mean_37 = torch.ops.aten.var_mean.correction(clone_231, [2], correction = 0, keepdim = True)
        getitem_80: "f32[8, 196, 1]" = var_mean_37[0]
        getitem_81: "f32[8, 196, 1]" = var_mean_37[1];  var_mean_37 = None
        
         # File: /home/sahanp/.conda/envs/pytorch-benchmarks/lib/python3.12/site-packages/timm/models/convit.py:88 in get_attention, code: gating = self.gating_param.view(1, -1, 1, 1)
        view_434: "f32[1, 16, 1, 1]" = torch.ops.aten.reshape.default(arg100_1, [1, -1, 1, 1]);  arg100_1 = None
        
         # File: /home/sahanp/.conda/envs/pytorch-benchmarks/lib/python3.12/site-packages/timm/models/convit.py:89 in get_attention, code: attn = (1. - torch.sigmoid(gating)) * patch_score + torch.sigmoid(gating) * pos_score
        sigmoid_32: "f32[1, 16, 1, 1]" = torch.ops.aten.sigmoid.default(view_434)
        sub_100: "f32[1, 16, 1, 1]" = torch.ops.aten.sub.Tensor(1.0, sigmoid_32);  sigmoid_32 = None
        
         # File: /home/sahanp/.conda/envs/pytorch-benchmarks/lib/python3.12/site-packages/timm/layers/norm.py:57 in forward, code: x = F.layer_norm(x, self.normalized_shape, self.weight, self.bias, self.eps)
        sub_97: "f32[8, 196, 768]" = torch.ops.aten.sub.Tensor(clone_231, getitem_81);  clone_231 = getitem_81 = None
        add_172: "f32[8, 196, 1]" = torch.ops.aten.add.Tensor(getitem_80, 1e-06);  getitem_80 = None
        rsqrt_37: "f32[8, 196, 1]" = torch.ops.aten.rsqrt.default(add_172);  add_172 = None
        mul_178: "f32[8, 196, 768]" = torch.ops.aten.mul.Tensor(sub_97, rsqrt_37);  sub_97 = rsqrt_37 = None
        mul_179: "f32[8, 196, 768]" = torch.ops.aten.mul.Tensor(mul_178, arg95_1);  mul_178 = arg95_1 = None
        add_173: "f32[8, 196, 768]" = torch.ops.aten.add.Tensor(mul_179, arg96_1);  mul_179 = arg96_1 = None
        
         # File: /home/sahanp/.conda/envs/pytorch-benchmarks/lib/python3.12/site-packages/timm/models/convit.py:80 in get_attention, code: qk = self.qk(x).reshape(B, N, 2, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)
        view_426: "f32[1568, 768]" = torch.ops.aten.reshape.default(add_173, [1568, 768])
        permute_193: "f32[768, 1536]" = torch.ops.aten.permute.default(arg97_1, [1, 0]);  arg97_1 = None
        mm_50: "f32[1568, 1536]" = torch.ops.aten.mm.default(view_426, permute_193);  view_426 = permute_193 = None
        view_427: "f32[8, 196, 1536]" = torch.ops.aten.reshape.default(mm_50, [8, 196, 1536]);  mm_50 = None
        view_428: "f32[8, 196, 2, 16, 48]" = torch.ops.aten.reshape.default(view_427, [8, 196, 2, 16, 48]);  view_427 = None
        permute_194: "f32[2, 8, 16, 196, 48]" = torch.ops.aten.permute.default(view_428, [2, 0, 3, 1, 4]);  view_428 = None
        
         # File: /home/sahanp/.conda/envs/pytorch-benchmarks/lib/python3.12/site-packages/timm/models/convit.py:81 in get_attention, code: q, k = qk[0], qk[1]
        select_113: "f32[8, 16, 196, 48]" = torch.ops.aten.select.int(permute_194, 0, 0)
        
         # File: /home/sahanp/.conda/envs/pytorch-benchmarks/lib/python3.12/site-packages/timm/models/convit.py:84 in get_attention, code: patch_score = (q @ k.transpose(-2, -1)) * self.scale
        expand_111: "f32[8, 16, 196, 48]" = torch.ops.aten.expand.default(select_113, [8, 16, 196, 48]);  select_113 = None
        clone_233: "f32[8, 16, 196, 48]" = torch.ops.aten.clone.default(expand_111, memory_format = torch.contiguous_format);  expand_111 = None
        view_431: "f32[128, 196, 48]" = torch.ops.aten.reshape.default(clone_233, [128, 196, 48]);  clone_233 = None
        
         # File: /home/sahanp/.conda/envs/pytorch-benchmarks/lib/python3.12/site-packages/timm/models/convit.py:81 in get_attention, code: q, k = qk[0], qk[1]
        select_114: "f32[8, 16, 196, 48]" = torch.ops.aten.select.int(permute_194, 0, 1);  permute_194 = None
        
         # File: /home/sahanp/.conda/envs/pytorch-benchmarks/lib/python3.12/site-packages/timm/models/convit.py:84 in get_attention, code: patch_score = (q @ k.transpose(-2, -1)) * self.scale
        permute_197: "f32[8, 16, 48, 196]" = torch.ops.aten.permute.default(select_114, [0, 1, 3, 2]);  select_114 = None
        expand_112: "f32[8, 16, 48, 196]" = torch.ops.aten.expand.default(permute_197, [8, 16, 48, 196]);  permute_197 = None
        clone_234: "f32[8, 16, 48, 196]" = torch.ops.aten.clone.default(expand_112, memory_format = torch.contiguous_format);  expand_112 = None
        view_432: "f32[128, 48, 196]" = torch.ops.aten.reshape.default(clone_234, [128, 48, 196]);  clone_234 = None
        bmm_36: "f32[128, 196, 196]" = torch.ops.aten.bmm.default(view_431, view_432);  view_431 = view_432 = None
        view_433: "f32[8, 16, 196, 196]" = torch.ops.aten.reshape.default(bmm_36, [8, 16, 196, 196]);  bmm_36 = None
        
        # No stacktrace found for following nodes
        mul_tensor_10: "f32[8, 16, 196, 196]" = torch.ops.aten.mul.Tensor(view_433, 1);  view_433 = None
        amax_default_5: "f32[8, 16, 196, 1]" = torch.ops.aten.amax.default(mul_tensor_10, [-1], True)
        sub_tensor_5: "f32[8, 16, 196, 196]" = torch.ops.aten.sub.Tensor(mul_tensor_10, amax_default_5);  mul_tensor_10 = amax_default_5 = None
        mul_tensor_11: "f32[8, 16, 196, 196]" = torch.ops.aten.mul.Tensor(sub_tensor_5, 0.14433756729740643);  sub_tensor_5 = None
        
         # File: /home/sahanp/.conda/envs/pytorch-benchmarks/lib/python3.12/site-packages/timm/models/convit.py:85 in get_attention, code: patch_score = patch_score.softmax(dim=-1)
        exp_34: "f32[8, 16, 196, 196]" = torch.ops.aten.exp.default(mul_tensor_11);  mul_tensor_11 = None
        sum_51: "f32[8, 16, 196, 1]" = torch.ops.aten.sum.dim_IntList(exp_34, [-1], True)
        div_50: "f32[8, 16, 196, 196]" = torch.ops.aten.div.Tensor(exp_34, sum_51);  exp_34 = sum_51 = None
        
         # File: /home/sahanp/.conda/envs/pytorch-benchmarks/lib/python3.12/site-packages/timm/models/convit.py:89 in get_attention, code: attn = (1. - torch.sigmoid(gating)) * patch_score + torch.sigmoid(gating) * pos_score
        mul_181: "f32[8, 16, 196, 196]" = torch.ops.aten.mul.Tensor(sub_100, div_50);  sub_100 = div_50 = None
        sigmoid_33: "f32[1, 16, 1, 1]" = torch.ops.aten.sigmoid.default(view_434);  view_434 = None
        
         # File: /home/sahanp/.conda/envs/pytorch-benchmarks/lib/python3.12/site-packages/timm/models/convit.py:119 in get_rel_indices, code: rel_indices = torch.zeros(1, num_patches, num_patches, 3)
        full_default_6: "f32[1, 196, 196, 3]" = torch.ops.aten.full.default([1, 196, 196, 3], 0, dtype = torch.float32, layout = torch.strided, device = device(type='cpu'), pin_memory = False)
        
         # File: /home/sahanp/.conda/envs/pytorch-benchmarks/lib/python3.12/site-packages/timm/models/convit.py:124 in get_rel_indices, code: rel_indices[:, :, :, 2] = indd.unsqueeze(0)
        select_60: "f32[1, 196, 196]" = torch.ops.aten.select.int(full_default_6, 3, 2)
        
         # File: /home/sahanp/.conda/envs/pytorch-benchmarks/lib/python3.12/site-packages/timm/models/convit.py:120 in get_rel_indices, code: ind = torch.arange(img_size).view(1, -1) - torch.arange(img_size).view(-1, 1)
        iota_12: "i64[14]" = torch.ops.prims.iota.default(14, start = 0, step = 1, dtype = torch.int64, device = device(type='cpu'), requires_grad = False)
        view_157: "i64[1, 14]" = torch.ops.aten.reshape.default(iota_12, [1, -1]);  iota_12 = None
        iota_13: "i64[14]" = torch.ops.prims.iota.default(14, start = 0, step = 1, dtype = torch.int64, device = device(type='cpu'), requires_grad = False)
        view_158: "i64[14, 1]" = torch.ops.aten.reshape.default(iota_13, [-1, 1]);  iota_13 = None
        sub_37: "i64[14, 14]" = torch.ops.aten.sub.Tensor(view_157, view_158);  view_157 = view_158 = None
        
         # File: /home/sahanp/.conda/envs/pytorch-benchmarks/lib/python3.12/site-packages/timm/models/convit.py:121 in get_rel_indices, code: indx = ind.repeat(img_size, img_size)
        repeat_6: "i64[196, 196]" = torch.ops.aten.repeat.default(sub_37, [14, 14])
        
         # File: /home/sahanp/.conda/envs/pytorch-benchmarks/lib/python3.12/site-packages/timm/models/convit.py:123 in get_rel_indices, code: indd = indx ** 2 + indy ** 2
        pow_13: "i64[196, 196]" = torch.ops.aten.pow.Tensor_Scalar(repeat_6, 2)
        
         # File: /home/sahanp/.conda/envs/pytorch-benchmarks/lib/python3.12/site-packages/timm/models/convit.py:122 in get_rel_indices, code: indy = ind.repeat_interleave(img_size, dim=0).repeat_interleave(img_size, dim=1)
        unsqueeze_36: "i64[14, 1, 14]" = torch.ops.aten.unsqueeze.default(sub_37, 1);  sub_37 = None
        expand_43: "i64[14, 14, 14]" = torch.ops.aten.expand.default(unsqueeze_36, [14, 14, 14]);  unsqueeze_36 = None
        clone_86: "i64[14, 14, 14]" = torch.ops.aten.clone.default(expand_43, memory_format = torch.contiguous_format);  expand_43 = None
        view_159: "i64[196, 14]" = torch.ops.aten.reshape.default(clone_86, [196, 14]);  clone_86 = None
        unsqueeze_37: "i64[196, 14, 1]" = torch.ops.aten.unsqueeze.default(view_159, 2);  view_159 = None
        expand_44: "i64[196, 14, 14]" = torch.ops.aten.expand.default(unsqueeze_37, [196, 14, 14]);  unsqueeze_37 = None
        clone_87: "i64[196, 14, 14]" = torch.ops.aten.clone.default(expand_44, memory_format = torch.contiguous_format);  expand_44 = None
        view_160: "i64[196, 196]" = torch.ops.aten.reshape.default(clone_87, [196, 196]);  clone_87 = None
        
         # File: /home/sahanp/.conda/envs/pytorch-benchmarks/lib/python3.12/site-packages/timm/models/convit.py:123 in get_rel_indices, code: indd = indx ** 2 + indy ** 2
        pow_14: "i64[196, 196]" = torch.ops.aten.pow.Tensor_Scalar(view_160, 2)
        add_63: "i64[196, 196]" = torch.ops.aten.add.Tensor(pow_13, pow_14);  pow_13 = pow_14 = None
        
         # File: /home/sahanp/.conda/envs/pytorch-benchmarks/lib/python3.12/site-packages/timm/models/convit.py:124 in get_rel_indices, code: rel_indices[:, :, :, 2] = indd.unsqueeze(0)
        unsqueeze_38: "i64[1, 196, 196]" = torch.ops.aten.unsqueeze.default(add_63, 0);  add_63 = None
        copy_18: "f32[1, 196, 196]" = torch.ops.aten.copy.default(select_60, unsqueeze_38);  select_60 = unsqueeze_38 = None
        
        # No stacktrace found for following nodes
        select_scatter_default_18: "f32[1, 196, 196, 3]" = torch.ops.aten.select_scatter.default(full_default_6, copy_18, 3, 2);  full_default_6 = copy_18 = None
        
         # File: /home/sahanp/.conda/envs/pytorch-benchmarks/lib/python3.12/site-packages/timm/models/convit.py:125 in get_rel_indices, code: rel_indices[:, :, :, 1] = indy.unsqueeze(0)
        select_63: "f32[1, 196, 196]" = torch.ops.aten.select.int(select_scatter_default_18, 3, 1)
        unsqueeze_39: "i64[1, 196, 196]" = torch.ops.aten.unsqueeze.default(view_160, 0);  view_160 = None
        copy_19: "f32[1, 196, 196]" = torch.ops.aten.copy.default(select_63, unsqueeze_39);  select_63 = unsqueeze_39 = None
        
        # No stacktrace found for following nodes
        select_scatter_default_19: "f32[1, 196, 196, 3]" = torch.ops.aten.select_scatter.default(select_scatter_default_18, copy_19, 3, 1);  select_scatter_default_18 = copy_19 = None
        
         # File: /home/sahanp/.conda/envs/pytorch-benchmarks/lib/python3.12/site-packages/timm/models/convit.py:126 in get_rel_indices, code: rel_indices[:, :, :, 0] = indx.unsqueeze(0)
        select_66: "f32[1, 196, 196]" = torch.ops.aten.select.int(select_scatter_default_19, 3, 0)
        unsqueeze_40: "i64[1, 196, 196]" = torch.ops.aten.unsqueeze.default(repeat_6, 0);  repeat_6 = None
        copy_20: "f32[1, 196, 196]" = torch.ops.aten.copy.default(select_66, unsqueeze_40);  select_66 = unsqueeze_40 = None
        
        # No stacktrace found for following nodes
        select_scatter_default_20: "f32[1, 196, 196, 3]" = torch.ops.aten.select_scatter.default(select_scatter_default_19, copy_20, 3, 0);  select_scatter_default_19 = copy_20 = None
        
         # File: /home/sahanp/.conda/envs/pytorch-benchmarks/lib/python3.12/site-packages/timm/models/convit.py:128 in get_rel_indices, code: return rel_indices.to(device)
        device_put_6: "f32[1, 196, 196, 3]" = torch.ops.prims.device_put.default(select_scatter_default_20, device(type='cuda', index=0));  select_scatter_default_20 = None
        
         # File: /home/sahanp/.conda/envs/pytorch-benchmarks/lib/python3.12/site-packages/timm/models/convit.py:82 in get_attention, code: pos_score = self.rel_indices.expand(B, -1, -1, -1)
        expand_110: "f32[8, 196, 196, 3]" = torch.ops.aten.expand.default(device_put_6, [8, -1, -1, -1])
        
         # File: /home/sahanp/.conda/envs/pytorch-benchmarks/lib/python3.12/site-packages/timm/models/convit.py:83 in get_attention, code: pos_score = self.pos_proj(pos_score).permute(0, 3, 1, 2)
        clone_232: "f32[8, 196, 196, 3]" = torch.ops.aten.clone.default(expand_110, memory_format = torch.contiguous_format);  expand_110 = None
        view_429: "f32[307328, 3]" = torch.ops.aten.reshape.default(clone_232, [307328, 3]);  clone_232 = None
        permute_195: "f32[3, 16]" = torch.ops.aten.permute.default(arg98_1, [1, 0]);  arg98_1 = None
        mm_51: "f32[307328, 16]" = torch.ops.aten.mm.default(view_429, permute_195);  view_429 = permute_195 = None
        view_430: "f32[8, 196, 196, 16]" = torch.ops.aten.reshape.default(mm_51, [8, 196, 196, 16]);  mm_51 = None
        add_174: "f32[8, 196, 196, 16]" = torch.ops.aten.add.Tensor(view_430, arg99_1);  view_430 = arg99_1 = None
        permute_196: "f32[8, 16, 196, 196]" = torch.ops.aten.permute.default(add_174, [0, 3, 1, 2]);  add_174 = None
        
         # File: /home/sahanp/.conda/envs/pytorch-benchmarks/lib/python3.12/site-packages/timm/models/convit.py:86 in get_attention, code: pos_score = pos_score.softmax(dim=-1)
        clone_235: "f32[8, 16, 196, 196]" = torch.ops.aten.clone.default(permute_196, memory_format = torch.contiguous_format);  permute_196 = None
        amax_35: "f32[8, 16, 196, 1]" = torch.ops.aten.amax.default(clone_235, [-1], True)
        sub_99: "f32[8, 16, 196, 196]" = torch.ops.aten.sub.Tensor(clone_235, amax_35);  clone_235 = amax_35 = None
        exp_35: "f32[8, 16, 196, 196]" = torch.ops.aten.exp.default(sub_99);  sub_99 = None
        sum_52: "f32[8, 16, 196, 1]" = torch.ops.aten.sum.dim_IntList(exp_35, [-1], True)
        div_51: "f32[8, 16, 196, 196]" = torch.ops.aten.div.Tensor(exp_35, sum_52);  exp_35 = sum_52 = None
        
         # File: /home/sahanp/.conda/envs/pytorch-benchmarks/lib/python3.12/site-packages/timm/models/convit.py:89 in get_attention, code: attn = (1. - torch.sigmoid(gating)) * patch_score + torch.sigmoid(gating) * pos_score
        mul_182: "f32[8, 16, 196, 196]" = torch.ops.aten.mul.Tensor(sigmoid_33, div_51);  sigmoid_33 = div_51 = None
        add_175: "f32[8, 16, 196, 196]" = torch.ops.aten.add.Tensor(mul_181, mul_182);  mul_181 = mul_182 = None
        
         # File: /home/sahanp/.conda/envs/pytorch-benchmarks/lib/python3.12/site-packages/timm/models/convit.py:90 in get_attention, code: attn /= attn.sum(dim=-1).unsqueeze(-1)
        sum_53: "f32[8, 16, 196]" = torch.ops.aten.sum.dim_IntList(add_175, [-1])
        unsqueeze_66: "f32[8, 16, 196, 1]" = torch.ops.aten.unsqueeze.default(sum_53, -1);  sum_53 = None
        div_52: "f32[8, 16, 196, 196]" = torch.ops.aten.div.Tensor(add_175, unsqueeze_66);  add_175 = unsqueeze_66 = None
        
         # File: /home/sahanp/.conda/envs/pytorch-benchmarks/lib/python3.12/site-packages/timm/models/convit.py:73 in forward, code: x = (attn @ v).transpose(1, 2).reshape(B, N, C)
        expand_113: "f32[8, 16, 196, 196]" = torch.ops.aten.expand.default(div_52, [8, 16, 196, 196]);  div_52 = None
        view_438: "f32[128, 196, 196]" = torch.ops.aten.reshape.default(expand_113, [128, 196, 196]);  expand_113 = None
        
         # File: /home/sahanp/.conda/envs/pytorch-benchmarks/lib/python3.12/site-packages/timm/models/convit.py:72 in forward, code: v = self.v(x).reshape(B, N, self.num_heads, C // self.num_heads).permute(0, 2, 1, 3)
        view_435: "f32[1568, 768]" = torch.ops.aten.reshape.default(add_173, [1568, 768]);  add_173 = None
        permute_198: "f32[768, 768]" = torch.ops.aten.permute.default(arg101_1, [1, 0]);  arg101_1 = None
        mm_52: "f32[1568, 768]" = torch.ops.aten.mm.default(view_435, permute_198);  view_435 = permute_198 = None
        view_436: "f32[8, 196, 768]" = torch.ops.aten.reshape.default(mm_52, [8, 196, 768]);  mm_52 = None
        view_437: "f32[8, 196, 16, 48]" = torch.ops.aten.reshape.default(view_436, [8, 196, 16, 48]);  view_436 = None
        permute_199: "f32[8, 16, 196, 48]" = torch.ops.aten.permute.default(view_437, [0, 2, 1, 3]);  view_437 = None
        
         # File: /home/sahanp/.conda/envs/pytorch-benchmarks/lib/python3.12/site-packages/timm/models/convit.py:73 in forward, code: x = (attn @ v).transpose(1, 2).reshape(B, N, C)
        expand_114: "f32[8, 16, 196, 48]" = torch.ops.aten.expand.default(permute_199, [8, 16, 196, 48]);  permute_199 = None
        clone_237: "f32[8, 16, 196, 48]" = torch.ops.aten.clone.default(expand_114, memory_format = torch.contiguous_format);  expand_114 = None
        view_439: "f32[128, 196, 48]" = torch.ops.aten.reshape.default(clone_237, [128, 196, 48]);  clone_237 = None
        bmm_37: "f32[128, 196, 48]" = torch.ops.aten.bmm.default(view_438, view_439);  view_438 = view_439 = None
        view_440: "f32[8, 16, 196, 48]" = torch.ops.aten.reshape.default(bmm_37, [8, 16, 196, 48]);  bmm_37 = None
        permute_200: "f32[8, 196, 16, 48]" = torch.ops.aten.permute.default(view_440, [0, 2, 1, 3]);  view_440 = None
        clone_238: "f32[8, 196, 16, 48]" = torch.ops.aten.clone.default(permute_200, memory_format = torch.contiguous_format);  permute_200 = None
        view_441: "f32[8, 196, 768]" = torch.ops.aten.reshape.default(clone_238, [8, 196, 768]);  clone_238 = None
        
         # File: /home/sahanp/.conda/envs/pytorch-benchmarks/lib/python3.12/site-packages/timm/models/convit.py:74 in forward, code: x = self.proj(x)
        view_442: "f32[1568, 768]" = torch.ops.aten.reshape.default(view_441, [1568, 768]);  view_441 = None
        permute_201: "f32[768, 768]" = torch.ops.aten.permute.default(arg102_1, [1, 0]);  arg102_1 = None
        
        # No stacktrace found for following nodes
        mm_default_17: "f32[1568, 768]" = torch.ops.aten.mm.default(view_442, permute_201);  view_442 = permute_201 = None
        add_tensor_17: "f32[1568, 768]" = torch.ops.aten.add.Tensor(mm_default_17, arg103_1);  mm_default_17 = arg103_1 = None
        
         # File: /home/sahanp/.conda/envs/pytorch-benchmarks/lib/python3.12/site-packages/timm/models/convit.py:74 in forward, code: x = self.proj(x)
        view_443: "f32[8, 196, 768]" = torch.ops.aten.reshape.default(add_tensor_17, [8, 196, 768]);  add_tensor_17 = None
        
         # File: /home/sahanp/.conda/envs/pytorch-benchmarks/lib/python3.12/site-packages/timm/models/convit.py:233 in forward, code: x = x + self.drop_path(self.attn(self.norm1(x)))
        add_176: "f32[8, 196, 768]" = torch.ops.aten.add.Tensor(add_171, view_443);  add_171 = view_443 = None
        
         # File: /home/sahanp/.conda/envs/pytorch-benchmarks/lib/python3.12/site-packages/timm/layers/norm.py:57 in forward, code: x = F.layer_norm(x, self.normalized_shape, self.weight, self.bias, self.eps)
        clone_240: "f32[8, 196, 768]" = torch.ops.aten.clone.default(add_176, memory_format = torch.contiguous_format)
        var_mean_38 = torch.ops.aten.var_mean.correction(clone_240, [2], correction = 0, keepdim = True)
        getitem_82: "f32[8, 196, 1]" = var_mean_38[0]
        getitem_83: "f32[8, 196, 1]" = var_mean_38[1];  var_mean_38 = None
        sub_101: "f32[8, 196, 768]" = torch.ops.aten.sub.Tensor(clone_240, getitem_83);  clone_240 = getitem_83 = None
        add_177: "f32[8, 196, 1]" = torch.ops.aten.add.Tensor(getitem_82, 1e-06);  getitem_82 = None
        rsqrt_38: "f32[8, 196, 1]" = torch.ops.aten.rsqrt.default(add_177);  add_177 = None
        mul_183: "f32[8, 196, 768]" = torch.ops.aten.mul.Tensor(sub_101, rsqrt_38);  sub_101 = rsqrt_38 = None
        mul_184: "f32[8, 196, 768]" = torch.ops.aten.mul.Tensor(mul_183, arg104_1);  mul_183 = arg104_1 = None
        add_178: "f32[8, 196, 768]" = torch.ops.aten.add.Tensor(mul_184, arg105_1);  mul_184 = arg105_1 = None
        
         # File: /home/sahanp/.conda/envs/pytorch-benchmarks/lib/python3.12/site-packages/timm/layers/mlp.py:42 in forward, code: x = self.fc1(x)
        view_444: "f32[1568, 768]" = torch.ops.aten.reshape.default(add_178, [1568, 768]);  add_178 = None
        permute_202: "f32[768, 3072]" = torch.ops.aten.permute.default(arg106_1, [1, 0]);  arg106_1 = None
        
        # No stacktrace found for following nodes
        mm_default_16: "f32[1568, 3072]" = torch.ops.aten.mm.default(view_444, permute_202);  view_444 = permute_202 = None
        add_tensor_16: "f32[1568, 3072]" = torch.ops.aten.add.Tensor(mm_default_16, arg107_1);  mm_default_16 = arg107_1 = None
        
         # File: /home/sahanp/.conda/envs/pytorch-benchmarks/lib/python3.12/site-packages/timm/layers/mlp.py:42 in forward, code: x = self.fc1(x)
        view_445: "f32[8, 196, 3072]" = torch.ops.aten.reshape.default(add_tensor_16, [8, 196, 3072]);  add_tensor_16 = None
        
         # File: /home/sahanp/.conda/envs/pytorch-benchmarks/lib/python3.12/site-packages/timm/layers/mlp.py:43 in forward, code: x = self.act(x)
        mul_185: "f32[8, 196, 3072]" = torch.ops.aten.mul.Tensor(view_445, 0.5)
        mul_186: "f32[8, 196, 3072]" = torch.ops.aten.mul.Tensor(view_445, 0.7071067811865476);  view_445 = None
        erf_18: "f32[8, 196, 3072]" = torch.ops.aten.erf.default(mul_186);  mul_186 = None
        add_179: "f32[8, 196, 3072]" = torch.ops.aten.add.Tensor(erf_18, 1);  erf_18 = None
        mul_187: "f32[8, 196, 3072]" = torch.ops.aten.mul.Tensor(mul_185, add_179);  mul_185 = add_179 = None
        
         # File: /home/sahanp/.conda/envs/pytorch-benchmarks/lib/python3.12/site-packages/timm/layers/mlp.py:46 in forward, code: x = self.fc2(x)
        view_446: "f32[1568, 3072]" = torch.ops.aten.reshape.default(mul_187, [1568, 3072]);  mul_187 = None
        permute_203: "f32[3072, 768]" = torch.ops.aten.permute.default(arg108_1, [1, 0]);  arg108_1 = None
        
        # No stacktrace found for following nodes
        mm_default_15: "f32[1568, 768]" = torch.ops.aten.mm.default(view_446, permute_203);  view_446 = permute_203 = None
        add_tensor_15: "f32[1568, 768]" = torch.ops.aten.add.Tensor(mm_default_15, arg109_1);  mm_default_15 = arg109_1 = None
        
         # File: /home/sahanp/.conda/envs/pytorch-benchmarks/lib/python3.12/site-packages/timm/layers/mlp.py:46 in forward, code: x = self.fc2(x)
        view_447: "f32[8, 196, 768]" = torch.ops.aten.reshape.default(add_tensor_15, [8, 196, 768]);  add_tensor_15 = None
        
         # File: /home/sahanp/.conda/envs/pytorch-benchmarks/lib/python3.12/site-packages/timm/models/convit.py:234 in forward, code: x = x + self.drop_path(self.mlp(self.norm2(x)))
        add_180: "f32[8, 196, 768]" = torch.ops.aten.add.Tensor(add_176, view_447);  add_176 = view_447 = None
        
         # File: /home/sahanp/.conda/envs/pytorch-benchmarks/lib/python3.12/site-packages/timm/layers/norm.py:57 in forward, code: x = F.layer_norm(x, self.normalized_shape, self.weight, self.bias, self.eps)
        clone_243: "f32[8, 196, 768]" = torch.ops.aten.clone.default(add_180, memory_format = torch.contiguous_format)
        var_mean_39 = torch.ops.aten.var_mean.correction(clone_243, [2], correction = 0, keepdim = True)
        getitem_84: "f32[8, 196, 1]" = var_mean_39[0]
        getitem_85: "f32[8, 196, 1]" = var_mean_39[1];  var_mean_39 = None
        
         # File: /home/sahanp/.conda/envs/pytorch-benchmarks/lib/python3.12/site-packages/timm/models/convit.py:88 in get_attention, code: gating = self.gating_param.view(1, -1, 1, 1)
        view_456: "f32[1, 16, 1, 1]" = torch.ops.aten.reshape.default(arg115_1, [1, -1, 1, 1]);  arg115_1 = None
        
         # File: /home/sahanp/.conda/envs/pytorch-benchmarks/lib/python3.12/site-packages/timm/models/convit.py:89 in get_attention, code: attn = (1. - torch.sigmoid(gating)) * patch_score + torch.sigmoid(gating) * pos_score
        sigmoid_34: "f32[1, 16, 1, 1]" = torch.ops.aten.sigmoid.default(view_456)
        sub_105: "f32[1, 16, 1, 1]" = torch.ops.aten.sub.Tensor(1.0, sigmoid_34);  sigmoid_34 = None
        
         # File: /home/sahanp/.conda/envs/pytorch-benchmarks/lib/python3.12/site-packages/timm/layers/norm.py:57 in forward, code: x = F.layer_norm(x, self.normalized_shape, self.weight, self.bias, self.eps)
        sub_102: "f32[8, 196, 768]" = torch.ops.aten.sub.Tensor(clone_243, getitem_85);  clone_243 = getitem_85 = None
        add_181: "f32[8, 196, 1]" = torch.ops.aten.add.Tensor(getitem_84, 1e-06);  getitem_84 = None
        rsqrt_39: "f32[8, 196, 1]" = torch.ops.aten.rsqrt.default(add_181);  add_181 = None
        mul_188: "f32[8, 196, 768]" = torch.ops.aten.mul.Tensor(sub_102, rsqrt_39);  sub_102 = rsqrt_39 = None
        mul_189: "f32[8, 196, 768]" = torch.ops.aten.mul.Tensor(mul_188, arg110_1);  mul_188 = arg110_1 = None
        add_182: "f32[8, 196, 768]" = torch.ops.aten.add.Tensor(mul_189, arg111_1);  mul_189 = arg111_1 = None
        
         # File: /home/sahanp/.conda/envs/pytorch-benchmarks/lib/python3.12/site-packages/timm/models/convit.py:80 in get_attention, code: qk = self.qk(x).reshape(B, N, 2, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)
        view_448: "f32[1568, 768]" = torch.ops.aten.reshape.default(add_182, [1568, 768])
        permute_204: "f32[768, 1536]" = torch.ops.aten.permute.default(arg112_1, [1, 0]);  arg112_1 = None
        mm_53: "f32[1568, 1536]" = torch.ops.aten.mm.default(view_448, permute_204);  view_448 = permute_204 = None
        view_449: "f32[8, 196, 1536]" = torch.ops.aten.reshape.default(mm_53, [8, 196, 1536]);  mm_53 = None
        view_450: "f32[8, 196, 2, 16, 48]" = torch.ops.aten.reshape.default(view_449, [8, 196, 2, 16, 48]);  view_449 = None
        permute_205: "f32[2, 8, 16, 196, 48]" = torch.ops.aten.permute.default(view_450, [2, 0, 3, 1, 4]);  view_450 = None
        
         # File: /home/sahanp/.conda/envs/pytorch-benchmarks/lib/python3.12/site-packages/timm/models/convit.py:81 in get_attention, code: q, k = qk[0], qk[1]
        select_115: "f32[8, 16, 196, 48]" = torch.ops.aten.select.int(permute_205, 0, 0)
        
         # File: /home/sahanp/.conda/envs/pytorch-benchmarks/lib/python3.12/site-packages/timm/models/convit.py:84 in get_attention, code: patch_score = (q @ k.transpose(-2, -1)) * self.scale
        expand_116: "f32[8, 16, 196, 48]" = torch.ops.aten.expand.default(select_115, [8, 16, 196, 48]);  select_115 = None
        clone_245: "f32[8, 16, 196, 48]" = torch.ops.aten.clone.default(expand_116, memory_format = torch.contiguous_format);  expand_116 = None
        view_453: "f32[128, 196, 48]" = torch.ops.aten.reshape.default(clone_245, [128, 196, 48]);  clone_245 = None
        
         # File: /home/sahanp/.conda/envs/pytorch-benchmarks/lib/python3.12/site-packages/timm/models/convit.py:81 in get_attention, code: q, k = qk[0], qk[1]
        select_116: "f32[8, 16, 196, 48]" = torch.ops.aten.select.int(permute_205, 0, 1);  permute_205 = None
        
         # File: /home/sahanp/.conda/envs/pytorch-benchmarks/lib/python3.12/site-packages/timm/models/convit.py:84 in get_attention, code: patch_score = (q @ k.transpose(-2, -1)) * self.scale
        permute_208: "f32[8, 16, 48, 196]" = torch.ops.aten.permute.default(select_116, [0, 1, 3, 2]);  select_116 = None
        expand_117: "f32[8, 16, 48, 196]" = torch.ops.aten.expand.default(permute_208, [8, 16, 48, 196]);  permute_208 = None
        clone_246: "f32[8, 16, 48, 196]" = torch.ops.aten.clone.default(expand_117, memory_format = torch.contiguous_format);  expand_117 = None
        view_454: "f32[128, 48, 196]" = torch.ops.aten.reshape.default(clone_246, [128, 48, 196]);  clone_246 = None
        bmm_38: "f32[128, 196, 196]" = torch.ops.aten.bmm.default(view_453, view_454);  view_453 = view_454 = None
        view_455: "f32[8, 16, 196, 196]" = torch.ops.aten.reshape.default(bmm_38, [8, 16, 196, 196]);  bmm_38 = None
        
        # No stacktrace found for following nodes
        mul_tensor_8: "f32[8, 16, 196, 196]" = torch.ops.aten.mul.Tensor(view_455, 1);  view_455 = None
        amax_default_4: "f32[8, 16, 196, 1]" = torch.ops.aten.amax.default(mul_tensor_8, [-1], True)
        sub_tensor_4: "f32[8, 16, 196, 196]" = torch.ops.aten.sub.Tensor(mul_tensor_8, amax_default_4);  mul_tensor_8 = amax_default_4 = None
        mul_tensor_9: "f32[8, 16, 196, 196]" = torch.ops.aten.mul.Tensor(sub_tensor_4, 0.14433756729740643);  sub_tensor_4 = None
        
         # File: /home/sahanp/.conda/envs/pytorch-benchmarks/lib/python3.12/site-packages/timm/models/convit.py:85 in get_attention, code: patch_score = patch_score.softmax(dim=-1)
        exp_36: "f32[8, 16, 196, 196]" = torch.ops.aten.exp.default(mul_tensor_9);  mul_tensor_9 = None
        sum_54: "f32[8, 16, 196, 1]" = torch.ops.aten.sum.dim_IntList(exp_36, [-1], True)
        div_53: "f32[8, 16, 196, 196]" = torch.ops.aten.div.Tensor(exp_36, sum_54);  exp_36 = sum_54 = None
        
         # File: /home/sahanp/.conda/envs/pytorch-benchmarks/lib/python3.12/site-packages/timm/models/convit.py:89 in get_attention, code: attn = (1. - torch.sigmoid(gating)) * patch_score + torch.sigmoid(gating) * pos_score
        mul_191: "f32[8, 16, 196, 196]" = torch.ops.aten.mul.Tensor(sub_105, div_53);  sub_105 = div_53 = None
        sigmoid_35: "f32[1, 16, 1, 1]" = torch.ops.aten.sigmoid.default(view_456);  view_456 = None
        
         # File: /home/sahanp/.conda/envs/pytorch-benchmarks/lib/python3.12/site-packages/timm/models/convit.py:119 in get_rel_indices, code: rel_indices = torch.zeros(1, num_patches, num_patches, 3)
        full_default_7: "f32[1, 196, 196, 3]" = torch.ops.aten.full.default([1, 196, 196, 3], 0, dtype = torch.float32, layout = torch.strided, device = device(type='cpu'), pin_memory = False)
        
         # File: /home/sahanp/.conda/envs/pytorch-benchmarks/lib/python3.12/site-packages/timm/models/convit.py:124 in get_rel_indices, code: rel_indices[:, :, :, 2] = indd.unsqueeze(0)
        select_70: "f32[1, 196, 196]" = torch.ops.aten.select.int(full_default_7, 3, 2)
        
         # File: /home/sahanp/.conda/envs/pytorch-benchmarks/lib/python3.12/site-packages/timm/models/convit.py:120 in get_rel_indices, code: ind = torch.arange(img_size).view(1, -1) - torch.arange(img_size).view(-1, 1)
        iota_14: "i64[14]" = torch.ops.prims.iota.default(14, start = 0, step = 1, dtype = torch.int64, device = device(type='cpu'), requires_grad = False)
        view_183: "i64[1, 14]" = torch.ops.aten.reshape.default(iota_14, [1, -1]);  iota_14 = None
        iota_15: "i64[14]" = torch.ops.prims.iota.default(14, start = 0, step = 1, dtype = torch.int64, device = device(type='cpu'), requires_grad = False)
        view_184: "i64[14, 1]" = torch.ops.aten.reshape.default(iota_15, [-1, 1]);  iota_15 = None
        sub_43: "i64[14, 14]" = torch.ops.aten.sub.Tensor(view_183, view_184);  view_183 = view_184 = None
        
         # File: /home/sahanp/.conda/envs/pytorch-benchmarks/lib/python3.12/site-packages/timm/models/convit.py:121 in get_rel_indices, code: indx = ind.repeat(img_size, img_size)
        repeat_7: "i64[196, 196]" = torch.ops.aten.repeat.default(sub_43, [14, 14])
        
         # File: /home/sahanp/.conda/envs/pytorch-benchmarks/lib/python3.12/site-packages/timm/models/convit.py:123 in get_rel_indices, code: indd = indx ** 2 + indy ** 2
        pow_15: "i64[196, 196]" = torch.ops.aten.pow.Tensor_Scalar(repeat_7, 2)
        
         # File: /home/sahanp/.conda/envs/pytorch-benchmarks/lib/python3.12/site-packages/timm/models/convit.py:122 in get_rel_indices, code: indy = ind.repeat_interleave(img_size, dim=0).repeat_interleave(img_size, dim=1)
        unsqueeze_42: "i64[14, 1, 14]" = torch.ops.aten.unsqueeze.default(sub_43, 1);  sub_43 = None
        expand_50: "i64[14, 14, 14]" = torch.ops.aten.expand.default(unsqueeze_42, [14, 14, 14]);  unsqueeze_42 = None
        clone_100: "i64[14, 14, 14]" = torch.ops.aten.clone.default(expand_50, memory_format = torch.contiguous_format);  expand_50 = None
        view_185: "i64[196, 14]" = torch.ops.aten.reshape.default(clone_100, [196, 14]);  clone_100 = None
        unsqueeze_43: "i64[196, 14, 1]" = torch.ops.aten.unsqueeze.default(view_185, 2);  view_185 = None
        expand_51: "i64[196, 14, 14]" = torch.ops.aten.expand.default(unsqueeze_43, [196, 14, 14]);  unsqueeze_43 = None
        clone_101: "i64[196, 14, 14]" = torch.ops.aten.clone.default(expand_51, memory_format = torch.contiguous_format);  expand_51 = None
        view_186: "i64[196, 196]" = torch.ops.aten.reshape.default(clone_101, [196, 196]);  clone_101 = None
        
         # File: /home/sahanp/.conda/envs/pytorch-benchmarks/lib/python3.12/site-packages/timm/models/convit.py:123 in get_rel_indices, code: indd = indx ** 2 + indy ** 2
        pow_16: "i64[196, 196]" = torch.ops.aten.pow.Tensor_Scalar(view_186, 2)
        add_73: "i64[196, 196]" = torch.ops.aten.add.Tensor(pow_15, pow_16);  pow_15 = pow_16 = None
        
         # File: /home/sahanp/.conda/envs/pytorch-benchmarks/lib/python3.12/site-packages/timm/models/convit.py:124 in get_rel_indices, code: rel_indices[:, :, :, 2] = indd.unsqueeze(0)
        unsqueeze_44: "i64[1, 196, 196]" = torch.ops.aten.unsqueeze.default(add_73, 0);  add_73 = None
        copy_21: "f32[1, 196, 196]" = torch.ops.aten.copy.default(select_70, unsqueeze_44);  select_70 = unsqueeze_44 = None
        
        # No stacktrace found for following nodes
        select_scatter_default_21: "f32[1, 196, 196, 3]" = torch.ops.aten.select_scatter.default(full_default_7, copy_21, 3, 2);  full_default_7 = copy_21 = None
        
         # File: /home/sahanp/.conda/envs/pytorch-benchmarks/lib/python3.12/site-packages/timm/models/convit.py:125 in get_rel_indices, code: rel_indices[:, :, :, 1] = indy.unsqueeze(0)
        select_73: "f32[1, 196, 196]" = torch.ops.aten.select.int(select_scatter_default_21, 3, 1)
        unsqueeze_45: "i64[1, 196, 196]" = torch.ops.aten.unsqueeze.default(view_186, 0);  view_186 = None
        copy_22: "f32[1, 196, 196]" = torch.ops.aten.copy.default(select_73, unsqueeze_45);  select_73 = unsqueeze_45 = None
        
        # No stacktrace found for following nodes
        select_scatter_default_22: "f32[1, 196, 196, 3]" = torch.ops.aten.select_scatter.default(select_scatter_default_21, copy_22, 3, 1);  select_scatter_default_21 = copy_22 = None
        
         # File: /home/sahanp/.conda/envs/pytorch-benchmarks/lib/python3.12/site-packages/timm/models/convit.py:126 in get_rel_indices, code: rel_indices[:, :, :, 0] = indx.unsqueeze(0)
        select_76: "f32[1, 196, 196]" = torch.ops.aten.select.int(select_scatter_default_22, 3, 0)
        unsqueeze_46: "i64[1, 196, 196]" = torch.ops.aten.unsqueeze.default(repeat_7, 0);  repeat_7 = None
        copy_23: "f32[1, 196, 196]" = torch.ops.aten.copy.default(select_76, unsqueeze_46);  select_76 = unsqueeze_46 = None
        
        # No stacktrace found for following nodes
        select_scatter_default_23: "f32[1, 196, 196, 3]" = torch.ops.aten.select_scatter.default(select_scatter_default_22, copy_23, 3, 0);  select_scatter_default_22 = copy_23 = None
        
         # File: /home/sahanp/.conda/envs/pytorch-benchmarks/lib/python3.12/site-packages/timm/models/convit.py:128 in get_rel_indices, code: return rel_indices.to(device)
        device_put_7: "f32[1, 196, 196, 3]" = torch.ops.prims.device_put.default(select_scatter_default_23, device(type='cuda', index=0));  select_scatter_default_23 = None
        
         # File: /home/sahanp/.conda/envs/pytorch-benchmarks/lib/python3.12/site-packages/timm/models/convit.py:82 in get_attention, code: pos_score = self.rel_indices.expand(B, -1, -1, -1)
        expand_115: "f32[8, 196, 196, 3]" = torch.ops.aten.expand.default(device_put_7, [8, -1, -1, -1])
        
         # File: /home/sahanp/.conda/envs/pytorch-benchmarks/lib/python3.12/site-packages/timm/models/convit.py:83 in get_attention, code: pos_score = self.pos_proj(pos_score).permute(0, 3, 1, 2)
        clone_244: "f32[8, 196, 196, 3]" = torch.ops.aten.clone.default(expand_115, memory_format = torch.contiguous_format);  expand_115 = None
        view_451: "f32[307328, 3]" = torch.ops.aten.reshape.default(clone_244, [307328, 3]);  clone_244 = None
        permute_206: "f32[3, 16]" = torch.ops.aten.permute.default(arg113_1, [1, 0]);  arg113_1 = None
        mm_54: "f32[307328, 16]" = torch.ops.aten.mm.default(view_451, permute_206);  view_451 = permute_206 = None
        view_452: "f32[8, 196, 196, 16]" = torch.ops.aten.reshape.default(mm_54, [8, 196, 196, 16]);  mm_54 = None
        add_183: "f32[8, 196, 196, 16]" = torch.ops.aten.add.Tensor(view_452, arg114_1);  view_452 = arg114_1 = None
        permute_207: "f32[8, 16, 196, 196]" = torch.ops.aten.permute.default(add_183, [0, 3, 1, 2]);  add_183 = None
        
         # File: /home/sahanp/.conda/envs/pytorch-benchmarks/lib/python3.12/site-packages/timm/models/convit.py:86 in get_attention, code: pos_score = pos_score.softmax(dim=-1)
        clone_247: "f32[8, 16, 196, 196]" = torch.ops.aten.clone.default(permute_207, memory_format = torch.contiguous_format);  permute_207 = None
        amax_37: "f32[8, 16, 196, 1]" = torch.ops.aten.amax.default(clone_247, [-1], True)
        sub_104: "f32[8, 16, 196, 196]" = torch.ops.aten.sub.Tensor(clone_247, amax_37);  clone_247 = amax_37 = None
        exp_37: "f32[8, 16, 196, 196]" = torch.ops.aten.exp.default(sub_104);  sub_104 = None
        sum_55: "f32[8, 16, 196, 1]" = torch.ops.aten.sum.dim_IntList(exp_37, [-1], True)
        div_54: "f32[8, 16, 196, 196]" = torch.ops.aten.div.Tensor(exp_37, sum_55);  exp_37 = sum_55 = None
        
         # File: /home/sahanp/.conda/envs/pytorch-benchmarks/lib/python3.12/site-packages/timm/models/convit.py:89 in get_attention, code: attn = (1. - torch.sigmoid(gating)) * patch_score + torch.sigmoid(gating) * pos_score
        mul_192: "f32[8, 16, 196, 196]" = torch.ops.aten.mul.Tensor(sigmoid_35, div_54);  sigmoid_35 = div_54 = None
        add_184: "f32[8, 16, 196, 196]" = torch.ops.aten.add.Tensor(mul_191, mul_192);  mul_191 = mul_192 = None
        
         # File: /home/sahanp/.conda/envs/pytorch-benchmarks/lib/python3.12/site-packages/timm/models/convit.py:90 in get_attention, code: attn /= attn.sum(dim=-1).unsqueeze(-1)
        sum_56: "f32[8, 16, 196]" = torch.ops.aten.sum.dim_IntList(add_184, [-1])
        unsqueeze_67: "f32[8, 16, 196, 1]" = torch.ops.aten.unsqueeze.default(sum_56, -1);  sum_56 = None
        div_55: "f32[8, 16, 196, 196]" = torch.ops.aten.div.Tensor(add_184, unsqueeze_67);  add_184 = unsqueeze_67 = None
        
         # File: /home/sahanp/.conda/envs/pytorch-benchmarks/lib/python3.12/site-packages/timm/models/convit.py:73 in forward, code: x = (attn @ v).transpose(1, 2).reshape(B, N, C)
        expand_118: "f32[8, 16, 196, 196]" = torch.ops.aten.expand.default(div_55, [8, 16, 196, 196]);  div_55 = None
        view_460: "f32[128, 196, 196]" = torch.ops.aten.reshape.default(expand_118, [128, 196, 196]);  expand_118 = None
        
         # File: /home/sahanp/.conda/envs/pytorch-benchmarks/lib/python3.12/site-packages/timm/models/convit.py:72 in forward, code: v = self.v(x).reshape(B, N, self.num_heads, C // self.num_heads).permute(0, 2, 1, 3)
        view_457: "f32[1568, 768]" = torch.ops.aten.reshape.default(add_182, [1568, 768]);  add_182 = None
        permute_209: "f32[768, 768]" = torch.ops.aten.permute.default(arg116_1, [1, 0]);  arg116_1 = None
        mm_55: "f32[1568, 768]" = torch.ops.aten.mm.default(view_457, permute_209);  view_457 = permute_209 = None
        view_458: "f32[8, 196, 768]" = torch.ops.aten.reshape.default(mm_55, [8, 196, 768]);  mm_55 = None
        view_459: "f32[8, 196, 16, 48]" = torch.ops.aten.reshape.default(view_458, [8, 196, 16, 48]);  view_458 = None
        permute_210: "f32[8, 16, 196, 48]" = torch.ops.aten.permute.default(view_459, [0, 2, 1, 3]);  view_459 = None
        
         # File: /home/sahanp/.conda/envs/pytorch-benchmarks/lib/python3.12/site-packages/timm/models/convit.py:73 in forward, code: x = (attn @ v).transpose(1, 2).reshape(B, N, C)
        expand_119: "f32[8, 16, 196, 48]" = torch.ops.aten.expand.default(permute_210, [8, 16, 196, 48]);  permute_210 = None
        clone_249: "f32[8, 16, 196, 48]" = torch.ops.aten.clone.default(expand_119, memory_format = torch.contiguous_format);  expand_119 = None
        view_461: "f32[128, 196, 48]" = torch.ops.aten.reshape.default(clone_249, [128, 196, 48]);  clone_249 = None
        bmm_39: "f32[128, 196, 48]" = torch.ops.aten.bmm.default(view_460, view_461);  view_460 = view_461 = None
        view_462: "f32[8, 16, 196, 48]" = torch.ops.aten.reshape.default(bmm_39, [8, 16, 196, 48]);  bmm_39 = None
        permute_211: "f32[8, 196, 16, 48]" = torch.ops.aten.permute.default(view_462, [0, 2, 1, 3]);  view_462 = None
        clone_250: "f32[8, 196, 16, 48]" = torch.ops.aten.clone.default(permute_211, memory_format = torch.contiguous_format);  permute_211 = None
        view_463: "f32[8, 196, 768]" = torch.ops.aten.reshape.default(clone_250, [8, 196, 768]);  clone_250 = None
        
         # File: /home/sahanp/.conda/envs/pytorch-benchmarks/lib/python3.12/site-packages/timm/models/convit.py:74 in forward, code: x = self.proj(x)
        view_464: "f32[1568, 768]" = torch.ops.aten.reshape.default(view_463, [1568, 768]);  view_463 = None
        permute_212: "f32[768, 768]" = torch.ops.aten.permute.default(arg117_1, [1, 0]);  arg117_1 = None
        
        # No stacktrace found for following nodes
        mm_default_14: "f32[1568, 768]" = torch.ops.aten.mm.default(view_464, permute_212);  view_464 = permute_212 = None
        add_tensor_14: "f32[1568, 768]" = torch.ops.aten.add.Tensor(mm_default_14, arg118_1);  mm_default_14 = arg118_1 = None
        
         # File: /home/sahanp/.conda/envs/pytorch-benchmarks/lib/python3.12/site-packages/timm/models/convit.py:74 in forward, code: x = self.proj(x)
        view_465: "f32[8, 196, 768]" = torch.ops.aten.reshape.default(add_tensor_14, [8, 196, 768]);  add_tensor_14 = None
        
         # File: /home/sahanp/.conda/envs/pytorch-benchmarks/lib/python3.12/site-packages/timm/models/convit.py:233 in forward, code: x = x + self.drop_path(self.attn(self.norm1(x)))
        add_185: "f32[8, 196, 768]" = torch.ops.aten.add.Tensor(add_180, view_465);  add_180 = view_465 = None
        
         # File: /home/sahanp/.conda/envs/pytorch-benchmarks/lib/python3.12/site-packages/timm/layers/norm.py:57 in forward, code: x = F.layer_norm(x, self.normalized_shape, self.weight, self.bias, self.eps)
        clone_252: "f32[8, 196, 768]" = torch.ops.aten.clone.default(add_185, memory_format = torch.contiguous_format)
        var_mean_40 = torch.ops.aten.var_mean.correction(clone_252, [2], correction = 0, keepdim = True)
        getitem_86: "f32[8, 196, 1]" = var_mean_40[0]
        getitem_87: "f32[8, 196, 1]" = var_mean_40[1];  var_mean_40 = None
        sub_106: "f32[8, 196, 768]" = torch.ops.aten.sub.Tensor(clone_252, getitem_87);  clone_252 = getitem_87 = None
        add_186: "f32[8, 196, 1]" = torch.ops.aten.add.Tensor(getitem_86, 1e-06);  getitem_86 = None
        rsqrt_40: "f32[8, 196, 1]" = torch.ops.aten.rsqrt.default(add_186);  add_186 = None
        mul_193: "f32[8, 196, 768]" = torch.ops.aten.mul.Tensor(sub_106, rsqrt_40);  sub_106 = rsqrt_40 = None
        mul_194: "f32[8, 196, 768]" = torch.ops.aten.mul.Tensor(mul_193, arg119_1);  mul_193 = arg119_1 = None
        add_187: "f32[8, 196, 768]" = torch.ops.aten.add.Tensor(mul_194, arg120_1);  mul_194 = arg120_1 = None
        
         # File: /home/sahanp/.conda/envs/pytorch-benchmarks/lib/python3.12/site-packages/timm/layers/mlp.py:42 in forward, code: x = self.fc1(x)
        view_466: "f32[1568, 768]" = torch.ops.aten.reshape.default(add_187, [1568, 768]);  add_187 = None
        permute_213: "f32[768, 3072]" = torch.ops.aten.permute.default(arg121_1, [1, 0]);  arg121_1 = None
        
        # No stacktrace found for following nodes
        mm_default_13: "f32[1568, 3072]" = torch.ops.aten.mm.default(view_466, permute_213);  view_466 = permute_213 = None
        add_tensor_13: "f32[1568, 3072]" = torch.ops.aten.add.Tensor(mm_default_13, arg122_1);  mm_default_13 = arg122_1 = None
        
         # File: /home/sahanp/.conda/envs/pytorch-benchmarks/lib/python3.12/site-packages/timm/layers/mlp.py:42 in forward, code: x = self.fc1(x)
        view_467: "f32[8, 196, 3072]" = torch.ops.aten.reshape.default(add_tensor_13, [8, 196, 3072]);  add_tensor_13 = None
        
         # File: /home/sahanp/.conda/envs/pytorch-benchmarks/lib/python3.12/site-packages/timm/layers/mlp.py:43 in forward, code: x = self.act(x)
        mul_195: "f32[8, 196, 3072]" = torch.ops.aten.mul.Tensor(view_467, 0.5)
        mul_196: "f32[8, 196, 3072]" = torch.ops.aten.mul.Tensor(view_467, 0.7071067811865476);  view_467 = None
        erf_19: "f32[8, 196, 3072]" = torch.ops.aten.erf.default(mul_196);  mul_196 = None
        add_188: "f32[8, 196, 3072]" = torch.ops.aten.add.Tensor(erf_19, 1);  erf_19 = None
        mul_197: "f32[8, 196, 3072]" = torch.ops.aten.mul.Tensor(mul_195, add_188);  mul_195 = add_188 = None
        
         # File: /home/sahanp/.conda/envs/pytorch-benchmarks/lib/python3.12/site-packages/timm/layers/mlp.py:46 in forward, code: x = self.fc2(x)
        view_468: "f32[1568, 3072]" = torch.ops.aten.reshape.default(mul_197, [1568, 3072]);  mul_197 = None
        permute_214: "f32[3072, 768]" = torch.ops.aten.permute.default(arg123_1, [1, 0]);  arg123_1 = None
        
        # No stacktrace found for following nodes
        mm_default_12: "f32[1568, 768]" = torch.ops.aten.mm.default(view_468, permute_214);  view_468 = permute_214 = None
        add_tensor_12: "f32[1568, 768]" = torch.ops.aten.add.Tensor(mm_default_12, arg124_1);  mm_default_12 = arg124_1 = None
        
         # File: /home/sahanp/.conda/envs/pytorch-benchmarks/lib/python3.12/site-packages/timm/layers/mlp.py:46 in forward, code: x = self.fc2(x)
        view_469: "f32[8, 196, 768]" = torch.ops.aten.reshape.default(add_tensor_12, [8, 196, 768]);  add_tensor_12 = None
        
         # File: /home/sahanp/.conda/envs/pytorch-benchmarks/lib/python3.12/site-packages/timm/models/convit.py:234 in forward, code: x = x + self.drop_path(self.mlp(self.norm2(x)))
        add_189: "f32[8, 196, 768]" = torch.ops.aten.add.Tensor(add_185, view_469);  add_185 = view_469 = None
        
         # File: /home/sahanp/.conda/envs/pytorch-benchmarks/lib/python3.12/site-packages/timm/layers/norm.py:57 in forward, code: x = F.layer_norm(x, self.normalized_shape, self.weight, self.bias, self.eps)
        clone_255: "f32[8, 196, 768]" = torch.ops.aten.clone.default(add_189, memory_format = torch.contiguous_format)
        var_mean_41 = torch.ops.aten.var_mean.correction(clone_255, [2], correction = 0, keepdim = True)
        getitem_88: "f32[8, 196, 1]" = var_mean_41[0]
        getitem_89: "f32[8, 196, 1]" = var_mean_41[1];  var_mean_41 = None
        
         # File: /home/sahanp/.conda/envs/pytorch-benchmarks/lib/python3.12/site-packages/timm/models/convit.py:88 in get_attention, code: gating = self.gating_param.view(1, -1, 1, 1)
        view_478: "f32[1, 16, 1, 1]" = torch.ops.aten.reshape.default(arg130_1, [1, -1, 1, 1]);  arg130_1 = None
        
         # File: /home/sahanp/.conda/envs/pytorch-benchmarks/lib/python3.12/site-packages/timm/models/convit.py:89 in get_attention, code: attn = (1. - torch.sigmoid(gating)) * patch_score + torch.sigmoid(gating) * pos_score
        sigmoid_36: "f32[1, 16, 1, 1]" = torch.ops.aten.sigmoid.default(view_478)
        sub_110: "f32[1, 16, 1, 1]" = torch.ops.aten.sub.Tensor(1.0, sigmoid_36);  sigmoid_36 = None
        
         # File: /home/sahanp/.conda/envs/pytorch-benchmarks/lib/python3.12/site-packages/timm/layers/norm.py:57 in forward, code: x = F.layer_norm(x, self.normalized_shape, self.weight, self.bias, self.eps)
        sub_107: "f32[8, 196, 768]" = torch.ops.aten.sub.Tensor(clone_255, getitem_89);  clone_255 = getitem_89 = None
        add_190: "f32[8, 196, 1]" = torch.ops.aten.add.Tensor(getitem_88, 1e-06);  getitem_88 = None
        rsqrt_41: "f32[8, 196, 1]" = torch.ops.aten.rsqrt.default(add_190);  add_190 = None
        mul_198: "f32[8, 196, 768]" = torch.ops.aten.mul.Tensor(sub_107, rsqrt_41);  sub_107 = rsqrt_41 = None
        mul_199: "f32[8, 196, 768]" = torch.ops.aten.mul.Tensor(mul_198, arg125_1);  mul_198 = arg125_1 = None
        add_191: "f32[8, 196, 768]" = torch.ops.aten.add.Tensor(mul_199, arg126_1);  mul_199 = arg126_1 = None
        
         # File: /home/sahanp/.conda/envs/pytorch-benchmarks/lib/python3.12/site-packages/timm/models/convit.py:80 in get_attention, code: qk = self.qk(x).reshape(B, N, 2, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)
        view_470: "f32[1568, 768]" = torch.ops.aten.reshape.default(add_191, [1568, 768])
        permute_215: "f32[768, 1536]" = torch.ops.aten.permute.default(arg127_1, [1, 0]);  arg127_1 = None
        mm_56: "f32[1568, 1536]" = torch.ops.aten.mm.default(view_470, permute_215);  view_470 = permute_215 = None
        view_471: "f32[8, 196, 1536]" = torch.ops.aten.reshape.default(mm_56, [8, 196, 1536]);  mm_56 = None
        view_472: "f32[8, 196, 2, 16, 48]" = torch.ops.aten.reshape.default(view_471, [8, 196, 2, 16, 48]);  view_471 = None
        permute_216: "f32[2, 8, 16, 196, 48]" = torch.ops.aten.permute.default(view_472, [2, 0, 3, 1, 4]);  view_472 = None
        
         # File: /home/sahanp/.conda/envs/pytorch-benchmarks/lib/python3.12/site-packages/timm/models/convit.py:81 in get_attention, code: q, k = qk[0], qk[1]
        select_117: "f32[8, 16, 196, 48]" = torch.ops.aten.select.int(permute_216, 0, 0)
        
         # File: /home/sahanp/.conda/envs/pytorch-benchmarks/lib/python3.12/site-packages/timm/models/convit.py:84 in get_attention, code: patch_score = (q @ k.transpose(-2, -1)) * self.scale
        expand_121: "f32[8, 16, 196, 48]" = torch.ops.aten.expand.default(select_117, [8, 16, 196, 48]);  select_117 = None
        clone_257: "f32[8, 16, 196, 48]" = torch.ops.aten.clone.default(expand_121, memory_format = torch.contiguous_format);  expand_121 = None
        view_475: "f32[128, 196, 48]" = torch.ops.aten.reshape.default(clone_257, [128, 196, 48]);  clone_257 = None
        
         # File: /home/sahanp/.conda/envs/pytorch-benchmarks/lib/python3.12/site-packages/timm/models/convit.py:81 in get_attention, code: q, k = qk[0], qk[1]
        select_118: "f32[8, 16, 196, 48]" = torch.ops.aten.select.int(permute_216, 0, 1);  permute_216 = None
        
         # File: /home/sahanp/.conda/envs/pytorch-benchmarks/lib/python3.12/site-packages/timm/models/convit.py:84 in get_attention, code: patch_score = (q @ k.transpose(-2, -1)) * self.scale
        permute_219: "f32[8, 16, 48, 196]" = torch.ops.aten.permute.default(select_118, [0, 1, 3, 2]);  select_118 = None
        expand_122: "f32[8, 16, 48, 196]" = torch.ops.aten.expand.default(permute_219, [8, 16, 48, 196]);  permute_219 = None
        clone_258: "f32[8, 16, 48, 196]" = torch.ops.aten.clone.default(expand_122, memory_format = torch.contiguous_format);  expand_122 = None
        view_476: "f32[128, 48, 196]" = torch.ops.aten.reshape.default(clone_258, [128, 48, 196]);  clone_258 = None
        bmm_40: "f32[128, 196, 196]" = torch.ops.aten.bmm.default(view_475, view_476);  view_475 = view_476 = None
        view_477: "f32[8, 16, 196, 196]" = torch.ops.aten.reshape.default(bmm_40, [8, 16, 196, 196]);  bmm_40 = None
        
        # No stacktrace found for following nodes
        mul_tensor_6: "f32[8, 16, 196, 196]" = torch.ops.aten.mul.Tensor(view_477, 1);  view_477 = None
        amax_default_3: "f32[8, 16, 196, 1]" = torch.ops.aten.amax.default(mul_tensor_6, [-1], True)
        sub_tensor_3: "f32[8, 16, 196, 196]" = torch.ops.aten.sub.Tensor(mul_tensor_6, amax_default_3);  mul_tensor_6 = amax_default_3 = None
        mul_tensor_7: "f32[8, 16, 196, 196]" = torch.ops.aten.mul.Tensor(sub_tensor_3, 0.14433756729740643);  sub_tensor_3 = None
        
         # File: /home/sahanp/.conda/envs/pytorch-benchmarks/lib/python3.12/site-packages/timm/models/convit.py:85 in get_attention, code: patch_score = patch_score.softmax(dim=-1)
        exp_38: "f32[8, 16, 196, 196]" = torch.ops.aten.exp.default(mul_tensor_7);  mul_tensor_7 = None
        sum_57: "f32[8, 16, 196, 1]" = torch.ops.aten.sum.dim_IntList(exp_38, [-1], True)
        div_56: "f32[8, 16, 196, 196]" = torch.ops.aten.div.Tensor(exp_38, sum_57);  exp_38 = sum_57 = None
        
         # File: /home/sahanp/.conda/envs/pytorch-benchmarks/lib/python3.12/site-packages/timm/models/convit.py:89 in get_attention, code: attn = (1. - torch.sigmoid(gating)) * patch_score + torch.sigmoid(gating) * pos_score
        mul_201: "f32[8, 16, 196, 196]" = torch.ops.aten.mul.Tensor(sub_110, div_56);  sub_110 = div_56 = None
        sigmoid_37: "f32[1, 16, 1, 1]" = torch.ops.aten.sigmoid.default(view_478);  view_478 = None
        
         # File: /home/sahanp/.conda/envs/pytorch-benchmarks/lib/python3.12/site-packages/timm/models/convit.py:119 in get_rel_indices, code: rel_indices = torch.zeros(1, num_patches, num_patches, 3)
        full_default_8: "f32[1, 196, 196, 3]" = torch.ops.aten.full.default([1, 196, 196, 3], 0, dtype = torch.float32, layout = torch.strided, device = device(type='cpu'), pin_memory = False)
        
         # File: /home/sahanp/.conda/envs/pytorch-benchmarks/lib/python3.12/site-packages/timm/models/convit.py:124 in get_rel_indices, code: rel_indices[:, :, :, 2] = indd.unsqueeze(0)
        select_80: "f32[1, 196, 196]" = torch.ops.aten.select.int(full_default_8, 3, 2)
        
         # File: /home/sahanp/.conda/envs/pytorch-benchmarks/lib/python3.12/site-packages/timm/models/convit.py:120 in get_rel_indices, code: ind = torch.arange(img_size).view(1, -1) - torch.arange(img_size).view(-1, 1)
        iota_16: "i64[14]" = torch.ops.prims.iota.default(14, start = 0, step = 1, dtype = torch.int64, device = device(type='cpu'), requires_grad = False)
        view_209: "i64[1, 14]" = torch.ops.aten.reshape.default(iota_16, [1, -1]);  iota_16 = None
        iota_17: "i64[14]" = torch.ops.prims.iota.default(14, start = 0, step = 1, dtype = torch.int64, device = device(type='cpu'), requires_grad = False)
        view_210: "i64[14, 1]" = torch.ops.aten.reshape.default(iota_17, [-1, 1]);  iota_17 = None
        sub_49: "i64[14, 14]" = torch.ops.aten.sub.Tensor(view_209, view_210);  view_209 = view_210 = None
        
         # File: /home/sahanp/.conda/envs/pytorch-benchmarks/lib/python3.12/site-packages/timm/models/convit.py:121 in get_rel_indices, code: indx = ind.repeat(img_size, img_size)
        repeat_8: "i64[196, 196]" = torch.ops.aten.repeat.default(sub_49, [14, 14])
        
         # File: /home/sahanp/.conda/envs/pytorch-benchmarks/lib/python3.12/site-packages/timm/models/convit.py:123 in get_rel_indices, code: indd = indx ** 2 + indy ** 2
        pow_17: "i64[196, 196]" = torch.ops.aten.pow.Tensor_Scalar(repeat_8, 2)
        
         # File: /home/sahanp/.conda/envs/pytorch-benchmarks/lib/python3.12/site-packages/timm/models/convit.py:122 in get_rel_indices, code: indy = ind.repeat_interleave(img_size, dim=0).repeat_interleave(img_size, dim=1)
        unsqueeze_48: "i64[14, 1, 14]" = torch.ops.aten.unsqueeze.default(sub_49, 1);  sub_49 = None
        expand_57: "i64[14, 14, 14]" = torch.ops.aten.expand.default(unsqueeze_48, [14, 14, 14]);  unsqueeze_48 = None
        clone_114: "i64[14, 14, 14]" = torch.ops.aten.clone.default(expand_57, memory_format = torch.contiguous_format);  expand_57 = None
        view_211: "i64[196, 14]" = torch.ops.aten.reshape.default(clone_114, [196, 14]);  clone_114 = None
        unsqueeze_49: "i64[196, 14, 1]" = torch.ops.aten.unsqueeze.default(view_211, 2);  view_211 = None
        expand_58: "i64[196, 14, 14]" = torch.ops.aten.expand.default(unsqueeze_49, [196, 14, 14]);  unsqueeze_49 = None
        clone_115: "i64[196, 14, 14]" = torch.ops.aten.clone.default(expand_58, memory_format = torch.contiguous_format);  expand_58 = None
        view_212: "i64[196, 196]" = torch.ops.aten.reshape.default(clone_115, [196, 196]);  clone_115 = None
        
         # File: /home/sahanp/.conda/envs/pytorch-benchmarks/lib/python3.12/site-packages/timm/models/convit.py:123 in get_rel_indices, code: indd = indx ** 2 + indy ** 2
        pow_18: "i64[196, 196]" = torch.ops.aten.pow.Tensor_Scalar(view_212, 2)
        add_83: "i64[196, 196]" = torch.ops.aten.add.Tensor(pow_17, pow_18);  pow_17 = pow_18 = None
        
         # File: /home/sahanp/.conda/envs/pytorch-benchmarks/lib/python3.12/site-packages/timm/models/convit.py:124 in get_rel_indices, code: rel_indices[:, :, :, 2] = indd.unsqueeze(0)
        unsqueeze_50: "i64[1, 196, 196]" = torch.ops.aten.unsqueeze.default(add_83, 0);  add_83 = None
        copy_24: "f32[1, 196, 196]" = torch.ops.aten.copy.default(select_80, unsqueeze_50);  select_80 = unsqueeze_50 = None
        
        # No stacktrace found for following nodes
        select_scatter_default_24: "f32[1, 196, 196, 3]" = torch.ops.aten.select_scatter.default(full_default_8, copy_24, 3, 2);  full_default_8 = copy_24 = None
        
         # File: /home/sahanp/.conda/envs/pytorch-benchmarks/lib/python3.12/site-packages/timm/models/convit.py:125 in get_rel_indices, code: rel_indices[:, :, :, 1] = indy.unsqueeze(0)
        select_83: "f32[1, 196, 196]" = torch.ops.aten.select.int(select_scatter_default_24, 3, 1)
        unsqueeze_51: "i64[1, 196, 196]" = torch.ops.aten.unsqueeze.default(view_212, 0);  view_212 = None
        copy_25: "f32[1, 196, 196]" = torch.ops.aten.copy.default(select_83, unsqueeze_51);  select_83 = unsqueeze_51 = None
        
        # No stacktrace found for following nodes
        select_scatter_default_25: "f32[1, 196, 196, 3]" = torch.ops.aten.select_scatter.default(select_scatter_default_24, copy_25, 3, 1);  select_scatter_default_24 = copy_25 = None
        
         # File: /home/sahanp/.conda/envs/pytorch-benchmarks/lib/python3.12/site-packages/timm/models/convit.py:126 in get_rel_indices, code: rel_indices[:, :, :, 0] = indx.unsqueeze(0)
        select_86: "f32[1, 196, 196]" = torch.ops.aten.select.int(select_scatter_default_25, 3, 0)
        unsqueeze_52: "i64[1, 196, 196]" = torch.ops.aten.unsqueeze.default(repeat_8, 0);  repeat_8 = None
        copy_26: "f32[1, 196, 196]" = torch.ops.aten.copy.default(select_86, unsqueeze_52);  select_86 = unsqueeze_52 = None
        
        # No stacktrace found for following nodes
        select_scatter_default_26: "f32[1, 196, 196, 3]" = torch.ops.aten.select_scatter.default(select_scatter_default_25, copy_26, 3, 0);  select_scatter_default_25 = copy_26 = None
        
         # File: /home/sahanp/.conda/envs/pytorch-benchmarks/lib/python3.12/site-packages/timm/models/convit.py:128 in get_rel_indices, code: return rel_indices.to(device)
        device_put_8: "f32[1, 196, 196, 3]" = torch.ops.prims.device_put.default(select_scatter_default_26, device(type='cuda', index=0));  select_scatter_default_26 = None
        
         # File: /home/sahanp/.conda/envs/pytorch-benchmarks/lib/python3.12/site-packages/timm/models/convit.py:82 in get_attention, code: pos_score = self.rel_indices.expand(B, -1, -1, -1)
        expand_120: "f32[8, 196, 196, 3]" = torch.ops.aten.expand.default(device_put_8, [8, -1, -1, -1])
        
         # File: /home/sahanp/.conda/envs/pytorch-benchmarks/lib/python3.12/site-packages/timm/models/convit.py:83 in get_attention, code: pos_score = self.pos_proj(pos_score).permute(0, 3, 1, 2)
        clone_256: "f32[8, 196, 196, 3]" = torch.ops.aten.clone.default(expand_120, memory_format = torch.contiguous_format);  expand_120 = None
        view_473: "f32[307328, 3]" = torch.ops.aten.reshape.default(clone_256, [307328, 3]);  clone_256 = None
        permute_217: "f32[3, 16]" = torch.ops.aten.permute.default(arg128_1, [1, 0]);  arg128_1 = None
        mm_57: "f32[307328, 16]" = torch.ops.aten.mm.default(view_473, permute_217);  view_473 = permute_217 = None
        view_474: "f32[8, 196, 196, 16]" = torch.ops.aten.reshape.default(mm_57, [8, 196, 196, 16]);  mm_57 = None
        add_192: "f32[8, 196, 196, 16]" = torch.ops.aten.add.Tensor(view_474, arg129_1);  view_474 = arg129_1 = None
        permute_218: "f32[8, 16, 196, 196]" = torch.ops.aten.permute.default(add_192, [0, 3, 1, 2]);  add_192 = None
        
         # File: /home/sahanp/.conda/envs/pytorch-benchmarks/lib/python3.12/site-packages/timm/models/convit.py:86 in get_attention, code: pos_score = pos_score.softmax(dim=-1)
        clone_259: "f32[8, 16, 196, 196]" = torch.ops.aten.clone.default(permute_218, memory_format = torch.contiguous_format);  permute_218 = None
        amax_39: "f32[8, 16, 196, 1]" = torch.ops.aten.amax.default(clone_259, [-1], True)
        sub_109: "f32[8, 16, 196, 196]" = torch.ops.aten.sub.Tensor(clone_259, amax_39);  clone_259 = amax_39 = None
        exp_39: "f32[8, 16, 196, 196]" = torch.ops.aten.exp.default(sub_109);  sub_109 = None
        sum_58: "f32[8, 16, 196, 1]" = torch.ops.aten.sum.dim_IntList(exp_39, [-1], True)
        div_57: "f32[8, 16, 196, 196]" = torch.ops.aten.div.Tensor(exp_39, sum_58);  exp_39 = sum_58 = None
        
         # File: /home/sahanp/.conda/envs/pytorch-benchmarks/lib/python3.12/site-packages/timm/models/convit.py:89 in get_attention, code: attn = (1. - torch.sigmoid(gating)) * patch_score + torch.sigmoid(gating) * pos_score
        mul_202: "f32[8, 16, 196, 196]" = torch.ops.aten.mul.Tensor(sigmoid_37, div_57);  sigmoid_37 = div_57 = None
        add_193: "f32[8, 16, 196, 196]" = torch.ops.aten.add.Tensor(mul_201, mul_202);  mul_201 = mul_202 = None
        
         # File: /home/sahanp/.conda/envs/pytorch-benchmarks/lib/python3.12/site-packages/timm/models/convit.py:90 in get_attention, code: attn /= attn.sum(dim=-1).unsqueeze(-1)
        sum_59: "f32[8, 16, 196]" = torch.ops.aten.sum.dim_IntList(add_193, [-1])
        unsqueeze_68: "f32[8, 16, 196, 1]" = torch.ops.aten.unsqueeze.default(sum_59, -1);  sum_59 = None
        div_58: "f32[8, 16, 196, 196]" = torch.ops.aten.div.Tensor(add_193, unsqueeze_68);  add_193 = unsqueeze_68 = None
        
         # File: /home/sahanp/.conda/envs/pytorch-benchmarks/lib/python3.12/site-packages/timm/models/convit.py:73 in forward, code: x = (attn @ v).transpose(1, 2).reshape(B, N, C)
        expand_123: "f32[8, 16, 196, 196]" = torch.ops.aten.expand.default(div_58, [8, 16, 196, 196]);  div_58 = None
        view_482: "f32[128, 196, 196]" = torch.ops.aten.reshape.default(expand_123, [128, 196, 196]);  expand_123 = None
        
         # File: /home/sahanp/.conda/envs/pytorch-benchmarks/lib/python3.12/site-packages/timm/models/convit.py:72 in forward, code: v = self.v(x).reshape(B, N, self.num_heads, C // self.num_heads).permute(0, 2, 1, 3)
        view_479: "f32[1568, 768]" = torch.ops.aten.reshape.default(add_191, [1568, 768]);  add_191 = None
        permute_220: "f32[768, 768]" = torch.ops.aten.permute.default(arg131_1, [1, 0]);  arg131_1 = None
        mm_58: "f32[1568, 768]" = torch.ops.aten.mm.default(view_479, permute_220);  view_479 = permute_220 = None
        view_480: "f32[8, 196, 768]" = torch.ops.aten.reshape.default(mm_58, [8, 196, 768]);  mm_58 = None
        view_481: "f32[8, 196, 16, 48]" = torch.ops.aten.reshape.default(view_480, [8, 196, 16, 48]);  view_480 = None
        permute_221: "f32[8, 16, 196, 48]" = torch.ops.aten.permute.default(view_481, [0, 2, 1, 3]);  view_481 = None
        
         # File: /home/sahanp/.conda/envs/pytorch-benchmarks/lib/python3.12/site-packages/timm/models/convit.py:73 in forward, code: x = (attn @ v).transpose(1, 2).reshape(B, N, C)
        expand_124: "f32[8, 16, 196, 48]" = torch.ops.aten.expand.default(permute_221, [8, 16, 196, 48]);  permute_221 = None
        clone_261: "f32[8, 16, 196, 48]" = torch.ops.aten.clone.default(expand_124, memory_format = torch.contiguous_format);  expand_124 = None
        view_483: "f32[128, 196, 48]" = torch.ops.aten.reshape.default(clone_261, [128, 196, 48]);  clone_261 = None
        bmm_41: "f32[128, 196, 48]" = torch.ops.aten.bmm.default(view_482, view_483);  view_482 = view_483 = None
        view_484: "f32[8, 16, 196, 48]" = torch.ops.aten.reshape.default(bmm_41, [8, 16, 196, 48]);  bmm_41 = None
        permute_222: "f32[8, 196, 16, 48]" = torch.ops.aten.permute.default(view_484, [0, 2, 1, 3]);  view_484 = None
        clone_262: "f32[8, 196, 16, 48]" = torch.ops.aten.clone.default(permute_222, memory_format = torch.contiguous_format);  permute_222 = None
        view_485: "f32[8, 196, 768]" = torch.ops.aten.reshape.default(clone_262, [8, 196, 768]);  clone_262 = None
        
         # File: /home/sahanp/.conda/envs/pytorch-benchmarks/lib/python3.12/site-packages/timm/models/convit.py:74 in forward, code: x = self.proj(x)
        view_486: "f32[1568, 768]" = torch.ops.aten.reshape.default(view_485, [1568, 768]);  view_485 = None
        permute_223: "f32[768, 768]" = torch.ops.aten.permute.default(arg132_1, [1, 0]);  arg132_1 = None
        
        # No stacktrace found for following nodes
        mm_default_11: "f32[1568, 768]" = torch.ops.aten.mm.default(view_486, permute_223);  view_486 = permute_223 = None
        add_tensor_11: "f32[1568, 768]" = torch.ops.aten.add.Tensor(mm_default_11, arg133_1);  mm_default_11 = arg133_1 = None
        
         # File: /home/sahanp/.conda/envs/pytorch-benchmarks/lib/python3.12/site-packages/timm/models/convit.py:74 in forward, code: x = self.proj(x)
        view_487: "f32[8, 196, 768]" = torch.ops.aten.reshape.default(add_tensor_11, [8, 196, 768]);  add_tensor_11 = None
        
         # File: /home/sahanp/.conda/envs/pytorch-benchmarks/lib/python3.12/site-packages/timm/models/convit.py:233 in forward, code: x = x + self.drop_path(self.attn(self.norm1(x)))
        add_194: "f32[8, 196, 768]" = torch.ops.aten.add.Tensor(add_189, view_487);  add_189 = view_487 = None
        
         # File: /home/sahanp/.conda/envs/pytorch-benchmarks/lib/python3.12/site-packages/timm/layers/norm.py:57 in forward, code: x = F.layer_norm(x, self.normalized_shape, self.weight, self.bias, self.eps)
        clone_264: "f32[8, 196, 768]" = torch.ops.aten.clone.default(add_194, memory_format = torch.contiguous_format)
        var_mean_42 = torch.ops.aten.var_mean.correction(clone_264, [2], correction = 0, keepdim = True)
        getitem_90: "f32[8, 196, 1]" = var_mean_42[0]
        getitem_91: "f32[8, 196, 1]" = var_mean_42[1];  var_mean_42 = None
        sub_111: "f32[8, 196, 768]" = torch.ops.aten.sub.Tensor(clone_264, getitem_91);  clone_264 = getitem_91 = None
        add_195: "f32[8, 196, 1]" = torch.ops.aten.add.Tensor(getitem_90, 1e-06);  getitem_90 = None
        rsqrt_42: "f32[8, 196, 1]" = torch.ops.aten.rsqrt.default(add_195);  add_195 = None
        mul_203: "f32[8, 196, 768]" = torch.ops.aten.mul.Tensor(sub_111, rsqrt_42);  sub_111 = rsqrt_42 = None
        mul_204: "f32[8, 196, 768]" = torch.ops.aten.mul.Tensor(mul_203, arg134_1);  mul_203 = arg134_1 = None
        add_196: "f32[8, 196, 768]" = torch.ops.aten.add.Tensor(mul_204, arg135_1);  mul_204 = arg135_1 = None
        
         # File: /home/sahanp/.conda/envs/pytorch-benchmarks/lib/python3.12/site-packages/timm/layers/mlp.py:42 in forward, code: x = self.fc1(x)
        view_488: "f32[1568, 768]" = torch.ops.aten.reshape.default(add_196, [1568, 768]);  add_196 = None
        permute_224: "f32[768, 3072]" = torch.ops.aten.permute.default(arg136_1, [1, 0]);  arg136_1 = None
        
        # No stacktrace found for following nodes
        mm_default_10: "f32[1568, 3072]" = torch.ops.aten.mm.default(view_488, permute_224);  view_488 = permute_224 = None
        add_tensor_10: "f32[1568, 3072]" = torch.ops.aten.add.Tensor(mm_default_10, arg137_1);  mm_default_10 = arg137_1 = None
        
         # File: /home/sahanp/.conda/envs/pytorch-benchmarks/lib/python3.12/site-packages/timm/layers/mlp.py:42 in forward, code: x = self.fc1(x)
        view_489: "f32[8, 196, 3072]" = torch.ops.aten.reshape.default(add_tensor_10, [8, 196, 3072]);  add_tensor_10 = None
        
         # File: /home/sahanp/.conda/envs/pytorch-benchmarks/lib/python3.12/site-packages/timm/layers/mlp.py:43 in forward, code: x = self.act(x)
        mul_205: "f32[8, 196, 3072]" = torch.ops.aten.mul.Tensor(view_489, 0.5)
        mul_206: "f32[8, 196, 3072]" = torch.ops.aten.mul.Tensor(view_489, 0.7071067811865476);  view_489 = None
        erf_20: "f32[8, 196, 3072]" = torch.ops.aten.erf.default(mul_206);  mul_206 = None
        add_197: "f32[8, 196, 3072]" = torch.ops.aten.add.Tensor(erf_20, 1);  erf_20 = None
        mul_207: "f32[8, 196, 3072]" = torch.ops.aten.mul.Tensor(mul_205, add_197);  mul_205 = add_197 = None
        
         # File: /home/sahanp/.conda/envs/pytorch-benchmarks/lib/python3.12/site-packages/timm/layers/mlp.py:46 in forward, code: x = self.fc2(x)
        view_490: "f32[1568, 3072]" = torch.ops.aten.reshape.default(mul_207, [1568, 3072]);  mul_207 = None
        permute_225: "f32[3072, 768]" = torch.ops.aten.permute.default(arg138_1, [1, 0]);  arg138_1 = None
        
        # No stacktrace found for following nodes
        mm_default_9: "f32[1568, 768]" = torch.ops.aten.mm.default(view_490, permute_225);  view_490 = permute_225 = None
        add_tensor_9: "f32[1568, 768]" = torch.ops.aten.add.Tensor(mm_default_9, arg139_1);  mm_default_9 = arg139_1 = None
        
         # File: /home/sahanp/.conda/envs/pytorch-benchmarks/lib/python3.12/site-packages/timm/layers/mlp.py:46 in forward, code: x = self.fc2(x)
        view_491: "f32[8, 196, 768]" = torch.ops.aten.reshape.default(add_tensor_9, [8, 196, 768]);  add_tensor_9 = None
        
         # File: /home/sahanp/.conda/envs/pytorch-benchmarks/lib/python3.12/site-packages/timm/models/convit.py:234 in forward, code: x = x + self.drop_path(self.mlp(self.norm2(x)))
        add_198: "f32[8, 196, 768]" = torch.ops.aten.add.Tensor(add_194, view_491);  add_194 = view_491 = None
        
         # File: /home/sahanp/.conda/envs/pytorch-benchmarks/lib/python3.12/site-packages/timm/layers/norm.py:57 in forward, code: x = F.layer_norm(x, self.normalized_shape, self.weight, self.bias, self.eps)
        clone_267: "f32[8, 196, 768]" = torch.ops.aten.clone.default(add_198, memory_format = torch.contiguous_format)
        var_mean_43 = torch.ops.aten.var_mean.correction(clone_267, [2], correction = 0, keepdim = True)
        getitem_92: "f32[8, 196, 1]" = var_mean_43[0]
        getitem_93: "f32[8, 196, 1]" = var_mean_43[1];  var_mean_43 = None
        
         # File: /home/sahanp/.conda/envs/pytorch-benchmarks/lib/python3.12/site-packages/timm/models/convit.py:88 in get_attention, code: gating = self.gating_param.view(1, -1, 1, 1)
        view_500: "f32[1, 16, 1, 1]" = torch.ops.aten.reshape.default(arg145_1, [1, -1, 1, 1]);  arg145_1 = None
        
         # File: /home/sahanp/.conda/envs/pytorch-benchmarks/lib/python3.12/site-packages/timm/models/convit.py:89 in get_attention, code: attn = (1. - torch.sigmoid(gating)) * patch_score + torch.sigmoid(gating) * pos_score
        sigmoid_38: "f32[1, 16, 1, 1]" = torch.ops.aten.sigmoid.default(view_500)
        sub_115: "f32[1, 16, 1, 1]" = torch.ops.aten.sub.Tensor(1.0, sigmoid_38);  sigmoid_38 = None
        
         # File: /home/sahanp/.conda/envs/pytorch-benchmarks/lib/python3.12/site-packages/timm/layers/norm.py:57 in forward, code: x = F.layer_norm(x, self.normalized_shape, self.weight, self.bias, self.eps)
        sub_112: "f32[8, 196, 768]" = torch.ops.aten.sub.Tensor(clone_267, getitem_93);  clone_267 = getitem_93 = None
        add_199: "f32[8, 196, 1]" = torch.ops.aten.add.Tensor(getitem_92, 1e-06);  getitem_92 = None
        rsqrt_43: "f32[8, 196, 1]" = torch.ops.aten.rsqrt.default(add_199);  add_199 = None
        mul_208: "f32[8, 196, 768]" = torch.ops.aten.mul.Tensor(sub_112, rsqrt_43);  sub_112 = rsqrt_43 = None
        mul_209: "f32[8, 196, 768]" = torch.ops.aten.mul.Tensor(mul_208, arg140_1);  mul_208 = arg140_1 = None
        add_200: "f32[8, 196, 768]" = torch.ops.aten.add.Tensor(mul_209, arg141_1);  mul_209 = arg141_1 = None
        
         # File: /home/sahanp/.conda/envs/pytorch-benchmarks/lib/python3.12/site-packages/timm/models/convit.py:80 in get_attention, code: qk = self.qk(x).reshape(B, N, 2, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)
        view_492: "f32[1568, 768]" = torch.ops.aten.reshape.default(add_200, [1568, 768])
        permute_226: "f32[768, 1536]" = torch.ops.aten.permute.default(arg142_1, [1, 0]);  arg142_1 = None
        mm_59: "f32[1568, 1536]" = torch.ops.aten.mm.default(view_492, permute_226);  view_492 = permute_226 = None
        view_493: "f32[8, 196, 1536]" = torch.ops.aten.reshape.default(mm_59, [8, 196, 1536]);  mm_59 = None
        view_494: "f32[8, 196, 2, 16, 48]" = torch.ops.aten.reshape.default(view_493, [8, 196, 2, 16, 48]);  view_493 = None
        permute_227: "f32[2, 8, 16, 196, 48]" = torch.ops.aten.permute.default(view_494, [2, 0, 3, 1, 4]);  view_494 = None
        
         # File: /home/sahanp/.conda/envs/pytorch-benchmarks/lib/python3.12/site-packages/timm/models/convit.py:81 in get_attention, code: q, k = qk[0], qk[1]
        select_119: "f32[8, 16, 196, 48]" = torch.ops.aten.select.int(permute_227, 0, 0)
        
         # File: /home/sahanp/.conda/envs/pytorch-benchmarks/lib/python3.12/site-packages/timm/models/convit.py:84 in get_attention, code: patch_score = (q @ k.transpose(-2, -1)) * self.scale
        expand_126: "f32[8, 16, 196, 48]" = torch.ops.aten.expand.default(select_119, [8, 16, 196, 48]);  select_119 = None
        clone_269: "f32[8, 16, 196, 48]" = torch.ops.aten.clone.default(expand_126, memory_format = torch.contiguous_format);  expand_126 = None
        view_497: "f32[128, 196, 48]" = torch.ops.aten.reshape.default(clone_269, [128, 196, 48]);  clone_269 = None
        
         # File: /home/sahanp/.conda/envs/pytorch-benchmarks/lib/python3.12/site-packages/timm/models/convit.py:81 in get_attention, code: q, k = qk[0], qk[1]
        select_120: "f32[8, 16, 196, 48]" = torch.ops.aten.select.int(permute_227, 0, 1);  permute_227 = None
        
         # File: /home/sahanp/.conda/envs/pytorch-benchmarks/lib/python3.12/site-packages/timm/models/convit.py:84 in get_attention, code: patch_score = (q @ k.transpose(-2, -1)) * self.scale
        permute_230: "f32[8, 16, 48, 196]" = torch.ops.aten.permute.default(select_120, [0, 1, 3, 2]);  select_120 = None
        expand_127: "f32[8, 16, 48, 196]" = torch.ops.aten.expand.default(permute_230, [8, 16, 48, 196]);  permute_230 = None
        clone_270: "f32[8, 16, 48, 196]" = torch.ops.aten.clone.default(expand_127, memory_format = torch.contiguous_format);  expand_127 = None
        view_498: "f32[128, 48, 196]" = torch.ops.aten.reshape.default(clone_270, [128, 48, 196]);  clone_270 = None
        bmm_42: "f32[128, 196, 196]" = torch.ops.aten.bmm.default(view_497, view_498);  view_497 = view_498 = None
        view_499: "f32[8, 16, 196, 196]" = torch.ops.aten.reshape.default(bmm_42, [8, 16, 196, 196]);  bmm_42 = None
        
        # No stacktrace found for following nodes
        mul_tensor_4: "f32[8, 16, 196, 196]" = torch.ops.aten.mul.Tensor(view_499, 1);  view_499 = None
        amax_default_2: "f32[8, 16, 196, 1]" = torch.ops.aten.amax.default(mul_tensor_4, [-1], True)
        sub_tensor_2: "f32[8, 16, 196, 196]" = torch.ops.aten.sub.Tensor(mul_tensor_4, amax_default_2);  mul_tensor_4 = amax_default_2 = None
        mul_tensor_5: "f32[8, 16, 196, 196]" = torch.ops.aten.mul.Tensor(sub_tensor_2, 0.14433756729740643);  sub_tensor_2 = None
        
         # File: /home/sahanp/.conda/envs/pytorch-benchmarks/lib/python3.12/site-packages/timm/models/convit.py:85 in get_attention, code: patch_score = patch_score.softmax(dim=-1)
        exp_40: "f32[8, 16, 196, 196]" = torch.ops.aten.exp.default(mul_tensor_5);  mul_tensor_5 = None
        sum_60: "f32[8, 16, 196, 1]" = torch.ops.aten.sum.dim_IntList(exp_40, [-1], True)
        div_59: "f32[8, 16, 196, 196]" = torch.ops.aten.div.Tensor(exp_40, sum_60);  exp_40 = sum_60 = None
        
         # File: /home/sahanp/.conda/envs/pytorch-benchmarks/lib/python3.12/site-packages/timm/models/convit.py:89 in get_attention, code: attn = (1. - torch.sigmoid(gating)) * patch_score + torch.sigmoid(gating) * pos_score
        mul_211: "f32[8, 16, 196, 196]" = torch.ops.aten.mul.Tensor(sub_115, div_59);  sub_115 = div_59 = None
        sigmoid_39: "f32[1, 16, 1, 1]" = torch.ops.aten.sigmoid.default(view_500);  view_500 = None
        
         # File: /home/sahanp/.conda/envs/pytorch-benchmarks/lib/python3.12/site-packages/timm/models/convit.py:119 in get_rel_indices, code: rel_indices = torch.zeros(1, num_patches, num_patches, 3)
        full_default_9: "f32[1, 196, 196, 3]" = torch.ops.aten.full.default([1, 196, 196, 3], 0, dtype = torch.float32, layout = torch.strided, device = device(type='cpu'), pin_memory = False)
        
         # File: /home/sahanp/.conda/envs/pytorch-benchmarks/lib/python3.12/site-packages/timm/models/convit.py:124 in get_rel_indices, code: rel_indices[:, :, :, 2] = indd.unsqueeze(0)
        select_90: "f32[1, 196, 196]" = torch.ops.aten.select.int(full_default_9, 3, 2)
        
         # File: /home/sahanp/.conda/envs/pytorch-benchmarks/lib/python3.12/site-packages/timm/models/convit.py:120 in get_rel_indices, code: ind = torch.arange(img_size).view(1, -1) - torch.arange(img_size).view(-1, 1)
        iota_18: "i64[14]" = torch.ops.prims.iota.default(14, start = 0, step = 1, dtype = torch.int64, device = device(type='cpu'), requires_grad = False)
        view_235: "i64[1, 14]" = torch.ops.aten.reshape.default(iota_18, [1, -1]);  iota_18 = None
        iota_19: "i64[14]" = torch.ops.prims.iota.default(14, start = 0, step = 1, dtype = torch.int64, device = device(type='cpu'), requires_grad = False)
        view_236: "i64[14, 1]" = torch.ops.aten.reshape.default(iota_19, [-1, 1]);  iota_19 = None
        sub_55: "i64[14, 14]" = torch.ops.aten.sub.Tensor(view_235, view_236);  view_235 = view_236 = None
        
         # File: /home/sahanp/.conda/envs/pytorch-benchmarks/lib/python3.12/site-packages/timm/models/convit.py:121 in get_rel_indices, code: indx = ind.repeat(img_size, img_size)
        repeat_9: "i64[196, 196]" = torch.ops.aten.repeat.default(sub_55, [14, 14])
        
         # File: /home/sahanp/.conda/envs/pytorch-benchmarks/lib/python3.12/site-packages/timm/models/convit.py:123 in get_rel_indices, code: indd = indx ** 2 + indy ** 2
        pow_19: "i64[196, 196]" = torch.ops.aten.pow.Tensor_Scalar(repeat_9, 2)
        
         # File: /home/sahanp/.conda/envs/pytorch-benchmarks/lib/python3.12/site-packages/timm/models/convit.py:122 in get_rel_indices, code: indy = ind.repeat_interleave(img_size, dim=0).repeat_interleave(img_size, dim=1)
        unsqueeze_54: "i64[14, 1, 14]" = torch.ops.aten.unsqueeze.default(sub_55, 1);  sub_55 = None
        expand_64: "i64[14, 14, 14]" = torch.ops.aten.expand.default(unsqueeze_54, [14, 14, 14]);  unsqueeze_54 = None
        clone_128: "i64[14, 14, 14]" = torch.ops.aten.clone.default(expand_64, memory_format = torch.contiguous_format);  expand_64 = None
        view_237: "i64[196, 14]" = torch.ops.aten.reshape.default(clone_128, [196, 14]);  clone_128 = None
        unsqueeze_55: "i64[196, 14, 1]" = torch.ops.aten.unsqueeze.default(view_237, 2);  view_237 = None
        expand_65: "i64[196, 14, 14]" = torch.ops.aten.expand.default(unsqueeze_55, [196, 14, 14]);  unsqueeze_55 = None
        clone_129: "i64[196, 14, 14]" = torch.ops.aten.clone.default(expand_65, memory_format = torch.contiguous_format);  expand_65 = None
        view_238: "i64[196, 196]" = torch.ops.aten.reshape.default(clone_129, [196, 196]);  clone_129 = None
        
         # File: /home/sahanp/.conda/envs/pytorch-benchmarks/lib/python3.12/site-packages/timm/models/convit.py:123 in get_rel_indices, code: indd = indx ** 2 + indy ** 2
        pow_20: "i64[196, 196]" = torch.ops.aten.pow.Tensor_Scalar(view_238, 2)
        add_93: "i64[196, 196]" = torch.ops.aten.add.Tensor(pow_19, pow_20);  pow_19 = pow_20 = None
        
         # File: /home/sahanp/.conda/envs/pytorch-benchmarks/lib/python3.12/site-packages/timm/models/convit.py:124 in get_rel_indices, code: rel_indices[:, :, :, 2] = indd.unsqueeze(0)
        unsqueeze_56: "i64[1, 196, 196]" = torch.ops.aten.unsqueeze.default(add_93, 0);  add_93 = None
        copy_27: "f32[1, 196, 196]" = torch.ops.aten.copy.default(select_90, unsqueeze_56);  select_90 = unsqueeze_56 = None
        
        # No stacktrace found for following nodes
        select_scatter_default_27: "f32[1, 196, 196, 3]" = torch.ops.aten.select_scatter.default(full_default_9, copy_27, 3, 2);  full_default_9 = copy_27 = None
        
         # File: /home/sahanp/.conda/envs/pytorch-benchmarks/lib/python3.12/site-packages/timm/models/convit.py:125 in get_rel_indices, code: rel_indices[:, :, :, 1] = indy.unsqueeze(0)
        select_93: "f32[1, 196, 196]" = torch.ops.aten.select.int(select_scatter_default_27, 3, 1)
        unsqueeze_57: "i64[1, 196, 196]" = torch.ops.aten.unsqueeze.default(view_238, 0);  view_238 = None
        copy_28: "f32[1, 196, 196]" = torch.ops.aten.copy.default(select_93, unsqueeze_57);  select_93 = unsqueeze_57 = None
        
        # No stacktrace found for following nodes
        select_scatter_default_28: "f32[1, 196, 196, 3]" = torch.ops.aten.select_scatter.default(select_scatter_default_27, copy_28, 3, 1);  select_scatter_default_27 = copy_28 = None
        
         # File: /home/sahanp/.conda/envs/pytorch-benchmarks/lib/python3.12/site-packages/timm/models/convit.py:126 in get_rel_indices, code: rel_indices[:, :, :, 0] = indx.unsqueeze(0)
        select_96: "f32[1, 196, 196]" = torch.ops.aten.select.int(select_scatter_default_28, 3, 0)
        unsqueeze_58: "i64[1, 196, 196]" = torch.ops.aten.unsqueeze.default(repeat_9, 0);  repeat_9 = None
        copy_29: "f32[1, 196, 196]" = torch.ops.aten.copy.default(select_96, unsqueeze_58);  select_96 = unsqueeze_58 = None
        
        # No stacktrace found for following nodes
        select_scatter_default_29: "f32[1, 196, 196, 3]" = torch.ops.aten.select_scatter.default(select_scatter_default_28, copy_29, 3, 0);  select_scatter_default_28 = copy_29 = None
        
         # File: /home/sahanp/.conda/envs/pytorch-benchmarks/lib/python3.12/site-packages/timm/models/convit.py:128 in get_rel_indices, code: return rel_indices.to(device)
        device_put_9: "f32[1, 196, 196, 3]" = torch.ops.prims.device_put.default(select_scatter_default_29, device(type='cuda', index=0));  select_scatter_default_29 = None
        
         # File: /home/sahanp/.conda/envs/pytorch-benchmarks/lib/python3.12/site-packages/timm/models/convit.py:82 in get_attention, code: pos_score = self.rel_indices.expand(B, -1, -1, -1)
        expand_125: "f32[8, 196, 196, 3]" = torch.ops.aten.expand.default(device_put_9, [8, -1, -1, -1])
        
         # File: /home/sahanp/.conda/envs/pytorch-benchmarks/lib/python3.12/site-packages/timm/models/convit.py:83 in get_attention, code: pos_score = self.pos_proj(pos_score).permute(0, 3, 1, 2)
        clone_268: "f32[8, 196, 196, 3]" = torch.ops.aten.clone.default(expand_125, memory_format = torch.contiguous_format);  expand_125 = None
        view_495: "f32[307328, 3]" = torch.ops.aten.reshape.default(clone_268, [307328, 3]);  clone_268 = None
        permute_228: "f32[3, 16]" = torch.ops.aten.permute.default(arg143_1, [1, 0]);  arg143_1 = None
        mm_60: "f32[307328, 16]" = torch.ops.aten.mm.default(view_495, permute_228);  view_495 = permute_228 = None
        view_496: "f32[8, 196, 196, 16]" = torch.ops.aten.reshape.default(mm_60, [8, 196, 196, 16]);  mm_60 = None
        add_201: "f32[8, 196, 196, 16]" = torch.ops.aten.add.Tensor(view_496, arg144_1);  view_496 = arg144_1 = None
        permute_229: "f32[8, 16, 196, 196]" = torch.ops.aten.permute.default(add_201, [0, 3, 1, 2]);  add_201 = None
        
         # File: /home/sahanp/.conda/envs/pytorch-benchmarks/lib/python3.12/site-packages/timm/models/convit.py:86 in get_attention, code: pos_score = pos_score.softmax(dim=-1)
        clone_271: "f32[8, 16, 196, 196]" = torch.ops.aten.clone.default(permute_229, memory_format = torch.contiguous_format);  permute_229 = None
        amax_41: "f32[8, 16, 196, 1]" = torch.ops.aten.amax.default(clone_271, [-1], True)
        sub_114: "f32[8, 16, 196, 196]" = torch.ops.aten.sub.Tensor(clone_271, amax_41);  clone_271 = amax_41 = None
        exp_41: "f32[8, 16, 196, 196]" = torch.ops.aten.exp.default(sub_114);  sub_114 = None
        sum_61: "f32[8, 16, 196, 1]" = torch.ops.aten.sum.dim_IntList(exp_41, [-1], True)
        div_60: "f32[8, 16, 196, 196]" = torch.ops.aten.div.Tensor(exp_41, sum_61);  exp_41 = sum_61 = None
        
         # File: /home/sahanp/.conda/envs/pytorch-benchmarks/lib/python3.12/site-packages/timm/models/convit.py:89 in get_attention, code: attn = (1. - torch.sigmoid(gating)) * patch_score + torch.sigmoid(gating) * pos_score
        mul_212: "f32[8, 16, 196, 196]" = torch.ops.aten.mul.Tensor(sigmoid_39, div_60);  sigmoid_39 = div_60 = None
        add_202: "f32[8, 16, 196, 196]" = torch.ops.aten.add.Tensor(mul_211, mul_212);  mul_211 = mul_212 = None
        
         # File: /home/sahanp/.conda/envs/pytorch-benchmarks/lib/python3.12/site-packages/timm/models/convit.py:90 in get_attention, code: attn /= attn.sum(dim=-1).unsqueeze(-1)
        sum_62: "f32[8, 16, 196]" = torch.ops.aten.sum.dim_IntList(add_202, [-1])
        unsqueeze_69: "f32[8, 16, 196, 1]" = torch.ops.aten.unsqueeze.default(sum_62, -1);  sum_62 = None
        div_61: "f32[8, 16, 196, 196]" = torch.ops.aten.div.Tensor(add_202, unsqueeze_69);  add_202 = unsqueeze_69 = None
        
         # File: /home/sahanp/.conda/envs/pytorch-benchmarks/lib/python3.12/site-packages/timm/models/convit.py:73 in forward, code: x = (attn @ v).transpose(1, 2).reshape(B, N, C)
        expand_128: "f32[8, 16, 196, 196]" = torch.ops.aten.expand.default(div_61, [8, 16, 196, 196]);  div_61 = None
        view_504: "f32[128, 196, 196]" = torch.ops.aten.reshape.default(expand_128, [128, 196, 196]);  expand_128 = None
        
         # File: /home/sahanp/.conda/envs/pytorch-benchmarks/lib/python3.12/site-packages/timm/models/convit.py:72 in forward, code: v = self.v(x).reshape(B, N, self.num_heads, C // self.num_heads).permute(0, 2, 1, 3)
        view_501: "f32[1568, 768]" = torch.ops.aten.reshape.default(add_200, [1568, 768]);  add_200 = None
        permute_231: "f32[768, 768]" = torch.ops.aten.permute.default(arg146_1, [1, 0]);  arg146_1 = None
        mm_61: "f32[1568, 768]" = torch.ops.aten.mm.default(view_501, permute_231);  view_501 = permute_231 = None
        view_502: "f32[8, 196, 768]" = torch.ops.aten.reshape.default(mm_61, [8, 196, 768]);  mm_61 = None
        view_503: "f32[8, 196, 16, 48]" = torch.ops.aten.reshape.default(view_502, [8, 196, 16, 48]);  view_502 = None
        permute_232: "f32[8, 16, 196, 48]" = torch.ops.aten.permute.default(view_503, [0, 2, 1, 3]);  view_503 = None
        
         # File: /home/sahanp/.conda/envs/pytorch-benchmarks/lib/python3.12/site-packages/timm/models/convit.py:73 in forward, code: x = (attn @ v).transpose(1, 2).reshape(B, N, C)
        expand_129: "f32[8, 16, 196, 48]" = torch.ops.aten.expand.default(permute_232, [8, 16, 196, 48]);  permute_232 = None
        clone_273: "f32[8, 16, 196, 48]" = torch.ops.aten.clone.default(expand_129, memory_format = torch.contiguous_format);  expand_129 = None
        view_505: "f32[128, 196, 48]" = torch.ops.aten.reshape.default(clone_273, [128, 196, 48]);  clone_273 = None
        bmm_43: "f32[128, 196, 48]" = torch.ops.aten.bmm.default(view_504, view_505);  view_504 = view_505 = None
        view_506: "f32[8, 16, 196, 48]" = torch.ops.aten.reshape.default(bmm_43, [8, 16, 196, 48]);  bmm_43 = None
        permute_233: "f32[8, 196, 16, 48]" = torch.ops.aten.permute.default(view_506, [0, 2, 1, 3]);  view_506 = None
        clone_274: "f32[8, 196, 16, 48]" = torch.ops.aten.clone.default(permute_233, memory_format = torch.contiguous_format);  permute_233 = None
        view_507: "f32[8, 196, 768]" = torch.ops.aten.reshape.default(clone_274, [8, 196, 768]);  clone_274 = None
        
         # File: /home/sahanp/.conda/envs/pytorch-benchmarks/lib/python3.12/site-packages/timm/models/convit.py:74 in forward, code: x = self.proj(x)
        view_508: "f32[1568, 768]" = torch.ops.aten.reshape.default(view_507, [1568, 768]);  view_507 = None
        permute_234: "f32[768, 768]" = torch.ops.aten.permute.default(arg147_1, [1, 0]);  arg147_1 = None
        
        # No stacktrace found for following nodes
        mm_default_8: "f32[1568, 768]" = torch.ops.aten.mm.default(view_508, permute_234);  view_508 = permute_234 = None
        add_tensor_8: "f32[1568, 768]" = torch.ops.aten.add.Tensor(mm_default_8, arg148_1);  mm_default_8 = arg148_1 = None
        
         # File: /home/sahanp/.conda/envs/pytorch-benchmarks/lib/python3.12/site-packages/timm/models/convit.py:74 in forward, code: x = self.proj(x)
        view_509: "f32[8, 196, 768]" = torch.ops.aten.reshape.default(add_tensor_8, [8, 196, 768]);  add_tensor_8 = None
        
         # File: /home/sahanp/.conda/envs/pytorch-benchmarks/lib/python3.12/site-packages/timm/models/convit.py:233 in forward, code: x = x + self.drop_path(self.attn(self.norm1(x)))
        add_203: "f32[8, 196, 768]" = torch.ops.aten.add.Tensor(add_198, view_509);  add_198 = view_509 = None
        
         # File: /home/sahanp/.conda/envs/pytorch-benchmarks/lib/python3.12/site-packages/timm/layers/norm.py:57 in forward, code: x = F.layer_norm(x, self.normalized_shape, self.weight, self.bias, self.eps)
        clone_276: "f32[8, 196, 768]" = torch.ops.aten.clone.default(add_203, memory_format = torch.contiguous_format)
        var_mean_44 = torch.ops.aten.var_mean.correction(clone_276, [2], correction = 0, keepdim = True)
        getitem_94: "f32[8, 196, 1]" = var_mean_44[0]
        getitem_95: "f32[8, 196, 1]" = var_mean_44[1];  var_mean_44 = None
        
         # File: /home/sahanp/.conda/envs/pytorch-benchmarks/lib/python3.12/site-packages/timm/models/convit.py:362 in forward_features, code: cls_tokens = self.cls_token.expand(x.shape[0], -1, -1)
        expand_79: "f32[8, 1, 768]" = torch.ops.aten.expand.default(arg4_1, [8, -1, -1]);  arg4_1 = None
        
         # File: /home/sahanp/.conda/envs/pytorch-benchmarks/lib/python3.12/site-packages/timm/layers/norm.py:57 in forward, code: x = F.layer_norm(x, self.normalized_shape, self.weight, self.bias, self.eps)
        sub_116: "f32[8, 196, 768]" = torch.ops.aten.sub.Tensor(clone_276, getitem_95);  clone_276 = getitem_95 = None
        add_204: "f32[8, 196, 1]" = torch.ops.aten.add.Tensor(getitem_94, 1e-06);  getitem_94 = None
        rsqrt_44: "f32[8, 196, 1]" = torch.ops.aten.rsqrt.default(add_204);  add_204 = None
        mul_213: "f32[8, 196, 768]" = torch.ops.aten.mul.Tensor(sub_116, rsqrt_44);  sub_116 = rsqrt_44 = None
        mul_214: "f32[8, 196, 768]" = torch.ops.aten.mul.Tensor(mul_213, arg149_1);  mul_213 = arg149_1 = None
        add_205: "f32[8, 196, 768]" = torch.ops.aten.add.Tensor(mul_214, arg150_1);  mul_214 = arg150_1 = None
        
         # File: /home/sahanp/.conda/envs/pytorch-benchmarks/lib/python3.12/site-packages/timm/layers/mlp.py:42 in forward, code: x = self.fc1(x)
        view_510: "f32[1568, 768]" = torch.ops.aten.reshape.default(add_205, [1568, 768]);  add_205 = None
        permute_235: "f32[768, 3072]" = torch.ops.aten.permute.default(arg151_1, [1, 0]);  arg151_1 = None
        
        # No stacktrace found for following nodes
        mm_default_7: "f32[1568, 3072]" = torch.ops.aten.mm.default(view_510, permute_235);  view_510 = permute_235 = None
        add_tensor_7: "f32[1568, 3072]" = torch.ops.aten.add.Tensor(mm_default_7, arg152_1);  mm_default_7 = arg152_1 = None
        
         # File: /home/sahanp/.conda/envs/pytorch-benchmarks/lib/python3.12/site-packages/timm/layers/mlp.py:42 in forward, code: x = self.fc1(x)
        view_511: "f32[8, 196, 3072]" = torch.ops.aten.reshape.default(add_tensor_7, [8, 196, 3072]);  add_tensor_7 = None
        
         # File: /home/sahanp/.conda/envs/pytorch-benchmarks/lib/python3.12/site-packages/timm/layers/mlp.py:43 in forward, code: x = self.act(x)
        mul_215: "f32[8, 196, 3072]" = torch.ops.aten.mul.Tensor(view_511, 0.5)
        mul_216: "f32[8, 196, 3072]" = torch.ops.aten.mul.Tensor(view_511, 0.7071067811865476);  view_511 = None
        erf_21: "f32[8, 196, 3072]" = torch.ops.aten.erf.default(mul_216);  mul_216 = None
        add_206: "f32[8, 196, 3072]" = torch.ops.aten.add.Tensor(erf_21, 1);  erf_21 = None
        mul_217: "f32[8, 196, 3072]" = torch.ops.aten.mul.Tensor(mul_215, add_206);  mul_215 = add_206 = None
        
         # File: /home/sahanp/.conda/envs/pytorch-benchmarks/lib/python3.12/site-packages/timm/layers/mlp.py:46 in forward, code: x = self.fc2(x)
        view_512: "f32[1568, 3072]" = torch.ops.aten.reshape.default(mul_217, [1568, 3072]);  mul_217 = None
        permute_236: "f32[3072, 768]" = torch.ops.aten.permute.default(arg153_1, [1, 0]);  arg153_1 = None
        
        # No stacktrace found for following nodes
        mm_default_6: "f32[1568, 768]" = torch.ops.aten.mm.default(view_512, permute_236);  view_512 = permute_236 = None
        add_tensor_6: "f32[1568, 768]" = torch.ops.aten.add.Tensor(mm_default_6, arg154_1);  mm_default_6 = arg154_1 = None
        
         # File: /home/sahanp/.conda/envs/pytorch-benchmarks/lib/python3.12/site-packages/timm/layers/mlp.py:46 in forward, code: x = self.fc2(x)
        view_513: "f32[8, 196, 768]" = torch.ops.aten.reshape.default(add_tensor_6, [8, 196, 768]);  add_tensor_6 = None
        
         # File: /home/sahanp/.conda/envs/pytorch-benchmarks/lib/python3.12/site-packages/timm/models/convit.py:234 in forward, code: x = x + self.drop_path(self.mlp(self.norm2(x)))
        add_207: "f32[8, 196, 768]" = torch.ops.aten.add.Tensor(add_203, view_513);  add_203 = view_513 = None
        
         # File: /home/sahanp/.conda/envs/pytorch-benchmarks/lib/python3.12/site-packages/timm/models/convit.py:365 in forward_features, code: x = torch.cat((cls_tokens, x), dim=1)
        cat_1: "f32[8, 197, 768]" = torch.ops.aten.cat.default([expand_79, add_207], 1);  expand_79 = add_207 = None
        
         # File: /home/sahanp/.conda/envs/pytorch-benchmarks/lib/python3.12/site-packages/timm/layers/norm.py:57 in forward, code: x = F.layer_norm(x, self.normalized_shape, self.weight, self.bias, self.eps)
        var_mean_45 = torch.ops.aten.var_mean.correction(cat_1, [2], correction = 0, keepdim = True)
        getitem_96: "f32[8, 197, 1]" = var_mean_45[0]
        getitem_97: "f32[8, 197, 1]" = var_mean_45[1];  var_mean_45 = None
        sub_117: "f32[8, 197, 768]" = torch.ops.aten.sub.Tensor(cat_1, getitem_97);  getitem_97 = None
        add_208: "f32[8, 197, 1]" = torch.ops.aten.add.Tensor(getitem_96, 1e-06);  getitem_96 = None
        rsqrt_45: "f32[8, 197, 1]" = torch.ops.aten.rsqrt.default(add_208);  add_208 = None
        mul_218: "f32[8, 197, 768]" = torch.ops.aten.mul.Tensor(sub_117, rsqrt_45);  sub_117 = rsqrt_45 = None
        mul_219: "f32[8, 197, 768]" = torch.ops.aten.mul.Tensor(mul_218, arg155_1);  mul_218 = arg155_1 = None
        add_209: "f32[8, 197, 768]" = torch.ops.aten.add.Tensor(mul_219, arg156_1);  mul_219 = arg156_1 = None
        
         # File: /home/sahanp/.conda/envs/pytorch-benchmarks/lib/python3.12/site-packages/timm/models/convit.py:173 in forward, code: qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)
        view_514: "f32[1576, 768]" = torch.ops.aten.reshape.default(add_209, [1576, 768]);  add_209 = None
        permute_237: "f32[768, 2304]" = torch.ops.aten.permute.default(arg157_1, [1, 0]);  arg157_1 = None
        mm_62: "f32[1576, 2304]" = torch.ops.aten.mm.default(view_514, permute_237);  view_514 = permute_237 = None
        view_515: "f32[8, 197, 2304]" = torch.ops.aten.reshape.default(mm_62, [8, 197, 2304]);  mm_62 = None
        view_516: "f32[8, 197, 3, 16, 48]" = torch.ops.aten.reshape.default(view_515, [8, 197, 3, 16, 48]);  view_515 = None
        permute_238: "f32[3, 8, 16, 197, 48]" = torch.ops.aten.permute.default(view_516, [2, 0, 3, 1, 4]);  view_516 = None
        
         # File: /home/sahanp/.conda/envs/pytorch-benchmarks/lib/python3.12/site-packages/timm/models/convit.py:174 in forward, code: q, k, v = qkv.unbind(0)
        unbind_2 = torch.ops.aten.unbind.int(permute_238);  permute_238 = None
        getitem_98: "f32[8, 16, 197, 48]" = unbind_2[0]
        getitem_99: "f32[8, 16, 197, 48]" = unbind_2[1]
        getitem_100: "f32[8, 16, 197, 48]" = unbind_2[2];  unbind_2 = None
        
         # File: /home/sahanp/.conda/envs/pytorch-benchmarks/lib/python3.12/site-packages/timm/models/convit.py:176 in forward, code: attn = (q @ k.transpose(-2, -1)) * self.scale
        expand_130: "f32[8, 16, 197, 48]" = torch.ops.aten.expand.default(getitem_98, [8, 16, 197, 48]);  getitem_98 = None
        clone_279: "f32[8, 16, 197, 48]" = torch.ops.aten.clone.default(expand_130, memory_format = torch.contiguous_format);  expand_130 = None
        view_517: "f32[128, 197, 48]" = torch.ops.aten.reshape.default(clone_279, [128, 197, 48]);  clone_279 = None
        permute_239: "f32[8, 16, 48, 197]" = torch.ops.aten.permute.default(getitem_99, [0, 1, 3, 2]);  getitem_99 = None
        expand_131: "f32[8, 16, 48, 197]" = torch.ops.aten.expand.default(permute_239, [8, 16, 48, 197]);  permute_239 = None
        clone_280: "f32[8, 16, 48, 197]" = torch.ops.aten.clone.default(expand_131, memory_format = torch.contiguous_format);  expand_131 = None
        view_518: "f32[128, 48, 197]" = torch.ops.aten.reshape.default(clone_280, [128, 48, 197]);  clone_280 = None
        bmm_44: "f32[128, 197, 197]" = torch.ops.aten.bmm.default(view_517, view_518);  view_517 = view_518 = None
        view_519: "f32[8, 16, 197, 197]" = torch.ops.aten.reshape.default(bmm_44, [8, 16, 197, 197]);  bmm_44 = None
        
        # No stacktrace found for following nodes
        mul_tensor_2: "f32[8, 16, 197, 197]" = torch.ops.aten.mul.Tensor(view_519, 1);  view_519 = None
        amax_default_1: "f32[8, 16, 197, 1]" = torch.ops.aten.amax.default(mul_tensor_2, [-1], True)
        sub_tensor_1: "f32[8, 16, 197, 197]" = torch.ops.aten.sub.Tensor(mul_tensor_2, amax_default_1);  mul_tensor_2 = amax_default_1 = None
        mul_tensor_3: "f32[8, 16, 197, 197]" = torch.ops.aten.mul.Tensor(sub_tensor_1, 0.14433756729740643);  sub_tensor_1 = None
        
         # File: /home/sahanp/.conda/envs/pytorch-benchmarks/lib/python3.12/site-packages/timm/models/convit.py:177 in forward, code: attn = attn.softmax(dim=-1)
        exp_42: "f32[8, 16, 197, 197]" = torch.ops.aten.exp.default(mul_tensor_3);  mul_tensor_3 = None
        sum_63: "f32[8, 16, 197, 1]" = torch.ops.aten.sum.dim_IntList(exp_42, [-1], True)
        div_62: "f32[8, 16, 197, 197]" = torch.ops.aten.div.Tensor(exp_42, sum_63);  exp_42 = sum_63 = None
        
         # File: /home/sahanp/.conda/envs/pytorch-benchmarks/lib/python3.12/site-packages/timm/models/convit.py:180 in forward, code: x = (attn @ v).transpose(1, 2).reshape(B, N, C)
        expand_132: "f32[8, 16, 197, 197]" = torch.ops.aten.expand.default(div_62, [8, 16, 197, 197]);  div_62 = None
        view_520: "f32[128, 197, 197]" = torch.ops.aten.reshape.default(expand_132, [128, 197, 197]);  expand_132 = None
        expand_133: "f32[8, 16, 197, 48]" = torch.ops.aten.expand.default(getitem_100, [8, 16, 197, 48]);  getitem_100 = None
        clone_282: "f32[8, 16, 197, 48]" = torch.ops.aten.clone.default(expand_133, memory_format = torch.contiguous_format);  expand_133 = None
        view_521: "f32[128, 197, 48]" = torch.ops.aten.reshape.default(clone_282, [128, 197, 48]);  clone_282 = None
        bmm_45: "f32[128, 197, 48]" = torch.ops.aten.bmm.default(view_520, view_521);  view_520 = view_521 = None
        view_522: "f32[8, 16, 197, 48]" = torch.ops.aten.reshape.default(bmm_45, [8, 16, 197, 48]);  bmm_45 = None
        permute_240: "f32[8, 197, 16, 48]" = torch.ops.aten.permute.default(view_522, [0, 2, 1, 3]);  view_522 = None
        clone_283: "f32[8, 197, 16, 48]" = torch.ops.aten.clone.default(permute_240, memory_format = torch.contiguous_format);  permute_240 = None
        view_523: "f32[8, 197, 768]" = torch.ops.aten.reshape.default(clone_283, [8, 197, 768]);  clone_283 = None
        
         # File: /home/sahanp/.conda/envs/pytorch-benchmarks/lib/python3.12/site-packages/timm/models/convit.py:181 in forward, code: x = self.proj(x)
        view_524: "f32[1576, 768]" = torch.ops.aten.reshape.default(view_523, [1576, 768]);  view_523 = None
        permute_241: "f32[768, 768]" = torch.ops.aten.permute.default(arg158_1, [1, 0]);  arg158_1 = None
        
        # No stacktrace found for following nodes
        mm_default_5: "f32[1576, 768]" = torch.ops.aten.mm.default(view_524, permute_241);  view_524 = permute_241 = None
        add_tensor_5: "f32[1576, 768]" = torch.ops.aten.add.Tensor(mm_default_5, arg159_1);  mm_default_5 = arg159_1 = None
        
         # File: /home/sahanp/.conda/envs/pytorch-benchmarks/lib/python3.12/site-packages/timm/models/convit.py:181 in forward, code: x = self.proj(x)
        view_525: "f32[8, 197, 768]" = torch.ops.aten.reshape.default(add_tensor_5, [8, 197, 768]);  add_tensor_5 = None
        
         # File: /home/sahanp/.conda/envs/pytorch-benchmarks/lib/python3.12/site-packages/timm/models/convit.py:233 in forward, code: x = x + self.drop_path(self.attn(self.norm1(x)))
        add_210: "f32[8, 197, 768]" = torch.ops.aten.add.Tensor(cat_1, view_525);  cat_1 = view_525 = None
        
         # File: /home/sahanp/.conda/envs/pytorch-benchmarks/lib/python3.12/site-packages/timm/layers/norm.py:57 in forward, code: x = F.layer_norm(x, self.normalized_shape, self.weight, self.bias, self.eps)
        var_mean_46 = torch.ops.aten.var_mean.correction(add_210, [2], correction = 0, keepdim = True)
        getitem_101: "f32[8, 197, 1]" = var_mean_46[0]
        getitem_102: "f32[8, 197, 1]" = var_mean_46[1];  var_mean_46 = None
        sub_119: "f32[8, 197, 768]" = torch.ops.aten.sub.Tensor(add_210, getitem_102);  getitem_102 = None
        add_211: "f32[8, 197, 1]" = torch.ops.aten.add.Tensor(getitem_101, 1e-06);  getitem_101 = None
        rsqrt_46: "f32[8, 197, 1]" = torch.ops.aten.rsqrt.default(add_211);  add_211 = None
        mul_221: "f32[8, 197, 768]" = torch.ops.aten.mul.Tensor(sub_119, rsqrt_46);  sub_119 = rsqrt_46 = None
        mul_222: "f32[8, 197, 768]" = torch.ops.aten.mul.Tensor(mul_221, arg160_1);  mul_221 = arg160_1 = None
        add_212: "f32[8, 197, 768]" = torch.ops.aten.add.Tensor(mul_222, arg161_1);  mul_222 = arg161_1 = None
        
         # File: /home/sahanp/.conda/envs/pytorch-benchmarks/lib/python3.12/site-packages/timm/layers/mlp.py:42 in forward, code: x = self.fc1(x)
        view_526: "f32[1576, 768]" = torch.ops.aten.reshape.default(add_212, [1576, 768]);  add_212 = None
        permute_242: "f32[768, 3072]" = torch.ops.aten.permute.default(arg162_1, [1, 0]);  arg162_1 = None
        
        # No stacktrace found for following nodes
        mm_default_4: "f32[1576, 3072]" = torch.ops.aten.mm.default(view_526, permute_242);  view_526 = permute_242 = None
        add_tensor_4: "f32[1576, 3072]" = torch.ops.aten.add.Tensor(mm_default_4, arg163_1);  mm_default_4 = arg163_1 = None
        
         # File: /home/sahanp/.conda/envs/pytorch-benchmarks/lib/python3.12/site-packages/timm/layers/mlp.py:42 in forward, code: x = self.fc1(x)
        view_527: "f32[8, 197, 3072]" = torch.ops.aten.reshape.default(add_tensor_4, [8, 197, 3072]);  add_tensor_4 = None
        
         # File: /home/sahanp/.conda/envs/pytorch-benchmarks/lib/python3.12/site-packages/timm/layers/mlp.py:43 in forward, code: x = self.act(x)
        mul_223: "f32[8, 197, 3072]" = torch.ops.aten.mul.Tensor(view_527, 0.5)
        mul_224: "f32[8, 197, 3072]" = torch.ops.aten.mul.Tensor(view_527, 0.7071067811865476);  view_527 = None
        erf_22: "f32[8, 197, 3072]" = torch.ops.aten.erf.default(mul_224);  mul_224 = None
        add_213: "f32[8, 197, 3072]" = torch.ops.aten.add.Tensor(erf_22, 1);  erf_22 = None
        mul_225: "f32[8, 197, 3072]" = torch.ops.aten.mul.Tensor(mul_223, add_213);  mul_223 = add_213 = None
        
         # File: /home/sahanp/.conda/envs/pytorch-benchmarks/lib/python3.12/site-packages/timm/layers/mlp.py:46 in forward, code: x = self.fc2(x)
        view_528: "f32[1576, 3072]" = torch.ops.aten.reshape.default(mul_225, [1576, 3072]);  mul_225 = None
        permute_243: "f32[3072, 768]" = torch.ops.aten.permute.default(arg164_1, [1, 0]);  arg164_1 = None
        
        # No stacktrace found for following nodes
        mm_default_3: "f32[1576, 768]" = torch.ops.aten.mm.default(view_528, permute_243);  view_528 = permute_243 = None
        add_tensor_3: "f32[1576, 768]" = torch.ops.aten.add.Tensor(mm_default_3, arg165_1);  mm_default_3 = arg165_1 = None
        
         # File: /home/sahanp/.conda/envs/pytorch-benchmarks/lib/python3.12/site-packages/timm/layers/mlp.py:46 in forward, code: x = self.fc2(x)
        view_529: "f32[8, 197, 768]" = torch.ops.aten.reshape.default(add_tensor_3, [8, 197, 768]);  add_tensor_3 = None
        
         # File: /home/sahanp/.conda/envs/pytorch-benchmarks/lib/python3.12/site-packages/timm/models/convit.py:234 in forward, code: x = x + self.drop_path(self.mlp(self.norm2(x)))
        add_214: "f32[8, 197, 768]" = torch.ops.aten.add.Tensor(add_210, view_529);  add_210 = view_529 = None
        
         # File: /home/sahanp/.conda/envs/pytorch-benchmarks/lib/python3.12/site-packages/timm/layers/norm.py:57 in forward, code: x = F.layer_norm(x, self.normalized_shape, self.weight, self.bias, self.eps)
        var_mean_47 = torch.ops.aten.var_mean.correction(add_214, [2], correction = 0, keepdim = True)
        getitem_103: "f32[8, 197, 1]" = var_mean_47[0]
        getitem_104: "f32[8, 197, 1]" = var_mean_47[1];  var_mean_47 = None
        sub_120: "f32[8, 197, 768]" = torch.ops.aten.sub.Tensor(add_214, getitem_104);  getitem_104 = None
        add_215: "f32[8, 197, 1]" = torch.ops.aten.add.Tensor(getitem_103, 1e-06);  getitem_103 = None
        rsqrt_47: "f32[8, 197, 1]" = torch.ops.aten.rsqrt.default(add_215);  add_215 = None
        mul_226: "f32[8, 197, 768]" = torch.ops.aten.mul.Tensor(sub_120, rsqrt_47);  sub_120 = rsqrt_47 = None
        mul_227: "f32[8, 197, 768]" = torch.ops.aten.mul.Tensor(mul_226, arg166_1);  mul_226 = arg166_1 = None
        add_216: "f32[8, 197, 768]" = torch.ops.aten.add.Tensor(mul_227, arg167_1);  mul_227 = arg167_1 = None
        
         # File: /home/sahanp/.conda/envs/pytorch-benchmarks/lib/python3.12/site-packages/timm/models/convit.py:173 in forward, code: qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)
        view_530: "f32[1576, 768]" = torch.ops.aten.reshape.default(add_216, [1576, 768]);  add_216 = None
        permute_244: "f32[768, 2304]" = torch.ops.aten.permute.default(arg168_1, [1, 0]);  arg168_1 = None
        mm_63: "f32[1576, 2304]" = torch.ops.aten.mm.default(view_530, permute_244);  view_530 = permute_244 = None
        view_531: "f32[8, 197, 2304]" = torch.ops.aten.reshape.default(mm_63, [8, 197, 2304]);  mm_63 = None
        view_532: "f32[8, 197, 3, 16, 48]" = torch.ops.aten.reshape.default(view_531, [8, 197, 3, 16, 48]);  view_531 = None
        permute_245: "f32[3, 8, 16, 197, 48]" = torch.ops.aten.permute.default(view_532, [2, 0, 3, 1, 4]);  view_532 = None
        
         # File: /home/sahanp/.conda/envs/pytorch-benchmarks/lib/python3.12/site-packages/timm/models/convit.py:174 in forward, code: q, k, v = qkv.unbind(0)
        unbind_3 = torch.ops.aten.unbind.int(permute_245);  permute_245 = None
        getitem_105: "f32[8, 16, 197, 48]" = unbind_3[0]
        getitem_106: "f32[8, 16, 197, 48]" = unbind_3[1]
        getitem_107: "f32[8, 16, 197, 48]" = unbind_3[2];  unbind_3 = None
        
         # File: /home/sahanp/.conda/envs/pytorch-benchmarks/lib/python3.12/site-packages/timm/models/convit.py:176 in forward, code: attn = (q @ k.transpose(-2, -1)) * self.scale
        expand_134: "f32[8, 16, 197, 48]" = torch.ops.aten.expand.default(getitem_105, [8, 16, 197, 48]);  getitem_105 = None
        clone_287: "f32[8, 16, 197, 48]" = torch.ops.aten.clone.default(expand_134, memory_format = torch.contiguous_format);  expand_134 = None
        view_533: "f32[128, 197, 48]" = torch.ops.aten.reshape.default(clone_287, [128, 197, 48]);  clone_287 = None
        permute_246: "f32[8, 16, 48, 197]" = torch.ops.aten.permute.default(getitem_106, [0, 1, 3, 2]);  getitem_106 = None
        expand_135: "f32[8, 16, 48, 197]" = torch.ops.aten.expand.default(permute_246, [8, 16, 48, 197]);  permute_246 = None
        clone_288: "f32[8, 16, 48, 197]" = torch.ops.aten.clone.default(expand_135, memory_format = torch.contiguous_format);  expand_135 = None
        view_534: "f32[128, 48, 197]" = torch.ops.aten.reshape.default(clone_288, [128, 48, 197]);  clone_288 = None
        bmm_46: "f32[128, 197, 197]" = torch.ops.aten.bmm.default(view_533, view_534);  view_533 = view_534 = None
        view_535: "f32[8, 16, 197, 197]" = torch.ops.aten.reshape.default(bmm_46, [8, 16, 197, 197]);  bmm_46 = None
        
        # No stacktrace found for following nodes
        mul_tensor: "f32[8, 16, 197, 197]" = torch.ops.aten.mul.Tensor(view_535, 1);  view_535 = None
        amax_default: "f32[8, 16, 197, 1]" = torch.ops.aten.amax.default(mul_tensor, [-1], True)
        sub_tensor: "f32[8, 16, 197, 197]" = torch.ops.aten.sub.Tensor(mul_tensor, amax_default);  mul_tensor = amax_default = None
        mul_tensor_1: "f32[8, 16, 197, 197]" = torch.ops.aten.mul.Tensor(sub_tensor, 0.14433756729740643);  sub_tensor = None
        
         # File: /home/sahanp/.conda/envs/pytorch-benchmarks/lib/python3.12/site-packages/timm/models/convit.py:177 in forward, code: attn = attn.softmax(dim=-1)
        exp_43: "f32[8, 16, 197, 197]" = torch.ops.aten.exp.default(mul_tensor_1);  mul_tensor_1 = None
        sum_64: "f32[8, 16, 197, 1]" = torch.ops.aten.sum.dim_IntList(exp_43, [-1], True)
        div_63: "f32[8, 16, 197, 197]" = torch.ops.aten.div.Tensor(exp_43, sum_64);  exp_43 = sum_64 = None
        
         # File: /home/sahanp/.conda/envs/pytorch-benchmarks/lib/python3.12/site-packages/timm/models/convit.py:180 in forward, code: x = (attn @ v).transpose(1, 2).reshape(B, N, C)
        expand_136: "f32[8, 16, 197, 197]" = torch.ops.aten.expand.default(div_63, [8, 16, 197, 197]);  div_63 = None
        view_536: "f32[128, 197, 197]" = torch.ops.aten.reshape.default(expand_136, [128, 197, 197]);  expand_136 = None
        expand_137: "f32[8, 16, 197, 48]" = torch.ops.aten.expand.default(getitem_107, [8, 16, 197, 48]);  getitem_107 = None
        clone_290: "f32[8, 16, 197, 48]" = torch.ops.aten.clone.default(expand_137, memory_format = torch.contiguous_format);  expand_137 = None
        view_537: "f32[128, 197, 48]" = torch.ops.aten.reshape.default(clone_290, [128, 197, 48]);  clone_290 = None
        bmm_47: "f32[128, 197, 48]" = torch.ops.aten.bmm.default(view_536, view_537);  view_536 = view_537 = None
        view_538: "f32[8, 16, 197, 48]" = torch.ops.aten.reshape.default(bmm_47, [8, 16, 197, 48]);  bmm_47 = None
        permute_247: "f32[8, 197, 16, 48]" = torch.ops.aten.permute.default(view_538, [0, 2, 1, 3]);  view_538 = None
        clone_291: "f32[8, 197, 16, 48]" = torch.ops.aten.clone.default(permute_247, memory_format = torch.contiguous_format);  permute_247 = None
        view_539: "f32[8, 197, 768]" = torch.ops.aten.reshape.default(clone_291, [8, 197, 768]);  clone_291 = None
        
         # File: /home/sahanp/.conda/envs/pytorch-benchmarks/lib/python3.12/site-packages/timm/models/convit.py:181 in forward, code: x = self.proj(x)
        view_540: "f32[1576, 768]" = torch.ops.aten.reshape.default(view_539, [1576, 768]);  view_539 = None
        permute_248: "f32[768, 768]" = torch.ops.aten.permute.default(arg169_1, [1, 0]);  arg169_1 = None
        
        # No stacktrace found for following nodes
        mm_default_2: "f32[1576, 768]" = torch.ops.aten.mm.default(view_540, permute_248);  view_540 = permute_248 = None
        add_tensor_2: "f32[1576, 768]" = torch.ops.aten.add.Tensor(mm_default_2, arg170_1);  mm_default_2 = arg170_1 = None
        
         # File: /home/sahanp/.conda/envs/pytorch-benchmarks/lib/python3.12/site-packages/timm/models/convit.py:181 in forward, code: x = self.proj(x)
        view_541: "f32[8, 197, 768]" = torch.ops.aten.reshape.default(add_tensor_2, [8, 197, 768]);  add_tensor_2 = None
        
         # File: /home/sahanp/.conda/envs/pytorch-benchmarks/lib/python3.12/site-packages/timm/models/convit.py:233 in forward, code: x = x + self.drop_path(self.attn(self.norm1(x)))
        add_217: "f32[8, 197, 768]" = torch.ops.aten.add.Tensor(add_214, view_541);  add_214 = view_541 = None
        
         # File: /home/sahanp/.conda/envs/pytorch-benchmarks/lib/python3.12/site-packages/timm/layers/norm.py:57 in forward, code: x = F.layer_norm(x, self.normalized_shape, self.weight, self.bias, self.eps)
        var_mean_48 = torch.ops.aten.var_mean.correction(add_217, [2], correction = 0, keepdim = True)
        getitem_108: "f32[8, 197, 1]" = var_mean_48[0]
        getitem_109: "f32[8, 197, 1]" = var_mean_48[1];  var_mean_48 = None
        sub_122: "f32[8, 197, 768]" = torch.ops.aten.sub.Tensor(add_217, getitem_109);  getitem_109 = None
        add_218: "f32[8, 197, 1]" = torch.ops.aten.add.Tensor(getitem_108, 1e-06);  getitem_108 = None
        rsqrt_48: "f32[8, 197, 1]" = torch.ops.aten.rsqrt.default(add_218);  add_218 = None
        mul_229: "f32[8, 197, 768]" = torch.ops.aten.mul.Tensor(sub_122, rsqrt_48);  sub_122 = rsqrt_48 = None
        mul_230: "f32[8, 197, 768]" = torch.ops.aten.mul.Tensor(mul_229, arg171_1);  mul_229 = arg171_1 = None
        add_219: "f32[8, 197, 768]" = torch.ops.aten.add.Tensor(mul_230, arg172_1);  mul_230 = arg172_1 = None
        
         # File: /home/sahanp/.conda/envs/pytorch-benchmarks/lib/python3.12/site-packages/timm/layers/mlp.py:42 in forward, code: x = self.fc1(x)
        view_542: "f32[1576, 768]" = torch.ops.aten.reshape.default(add_219, [1576, 768]);  add_219 = None
        permute_249: "f32[768, 3072]" = torch.ops.aten.permute.default(arg173_1, [1, 0]);  arg173_1 = None
        
        # No stacktrace found for following nodes
        mm_default_1: "f32[1576, 3072]" = torch.ops.aten.mm.default(view_542, permute_249);  view_542 = permute_249 = None
        add_tensor_1: "f32[1576, 3072]" = torch.ops.aten.add.Tensor(mm_default_1, arg174_1);  mm_default_1 = arg174_1 = None
        
         # File: /home/sahanp/.conda/envs/pytorch-benchmarks/lib/python3.12/site-packages/timm/layers/mlp.py:42 in forward, code: x = self.fc1(x)
        view_543: "f32[8, 197, 3072]" = torch.ops.aten.reshape.default(add_tensor_1, [8, 197, 3072]);  add_tensor_1 = None
        
         # File: /home/sahanp/.conda/envs/pytorch-benchmarks/lib/python3.12/site-packages/timm/layers/mlp.py:43 in forward, code: x = self.act(x)
        mul_231: "f32[8, 197, 3072]" = torch.ops.aten.mul.Tensor(view_543, 0.5)
        mul_232: "f32[8, 197, 3072]" = torch.ops.aten.mul.Tensor(view_543, 0.7071067811865476);  view_543 = None
        erf_23: "f32[8, 197, 3072]" = torch.ops.aten.erf.default(mul_232);  mul_232 = None
        add_220: "f32[8, 197, 3072]" = torch.ops.aten.add.Tensor(erf_23, 1);  erf_23 = None
        mul_233: "f32[8, 197, 3072]" = torch.ops.aten.mul.Tensor(mul_231, add_220);  mul_231 = add_220 = None
        
         # File: /home/sahanp/.conda/envs/pytorch-benchmarks/lib/python3.12/site-packages/timm/layers/mlp.py:46 in forward, code: x = self.fc2(x)
        view_544: "f32[1576, 3072]" = torch.ops.aten.reshape.default(mul_233, [1576, 3072]);  mul_233 = None
        permute_250: "f32[3072, 768]" = torch.ops.aten.permute.default(arg175_1, [1, 0]);  arg175_1 = None
        
        # No stacktrace found for following nodes
        mm_default: "f32[1576, 768]" = torch.ops.aten.mm.default(view_544, permute_250);  view_544 = permute_250 = None
        add_tensor: "f32[1576, 768]" = torch.ops.aten.add.Tensor(mm_default, arg176_1);  mm_default = arg176_1 = None
        
         # File: /home/sahanp/.conda/envs/pytorch-benchmarks/lib/python3.12/site-packages/timm/layers/mlp.py:46 in forward, code: x = self.fc2(x)
        view_545: "f32[8, 197, 768]" = torch.ops.aten.reshape.default(add_tensor, [8, 197, 768]);  add_tensor = None
        
         # File: /home/sahanp/.conda/envs/pytorch-benchmarks/lib/python3.12/site-packages/timm/models/convit.py:234 in forward, code: x = x + self.drop_path(self.mlp(self.norm2(x)))
        add_221: "f32[8, 197, 768]" = torch.ops.aten.add.Tensor(add_217, view_545);  add_217 = view_545 = None
        
         # File: /home/sahanp/.conda/envs/pytorch-benchmarks/lib/python3.12/site-packages/timm/layers/norm.py:57 in forward, code: x = F.layer_norm(x, self.normalized_shape, self.weight, self.bias, self.eps)
        var_mean_49 = torch.ops.aten.var_mean.correction(add_221, [2], correction = 0, keepdim = True)
        getitem_110: "f32[8, 197, 1]" = var_mean_49[0]
        getitem_111: "f32[8, 197, 1]" = var_mean_49[1];  var_mean_49 = None
        sub_123: "f32[8, 197, 768]" = torch.ops.aten.sub.Tensor(add_221, getitem_111);  add_221 = getitem_111 = None
        add_222: "f32[8, 197, 1]" = torch.ops.aten.add.Tensor(getitem_110, 1e-06);  getitem_110 = None
        rsqrt_49: "f32[8, 197, 1]" = torch.ops.aten.rsqrt.default(add_222);  add_222 = None
        mul_234: "f32[8, 197, 768]" = torch.ops.aten.mul.Tensor(sub_123, rsqrt_49);  sub_123 = rsqrt_49 = None
        mul_235: "f32[8, 197, 768]" = torch.ops.aten.mul.Tensor(mul_234, arg177_1);  mul_234 = arg177_1 = None
        add_223: "f32[8, 197, 768]" = torch.ops.aten.add.Tensor(mul_235, arg178_1);  mul_235 = arg178_1 = None
        
         # File: /home/sahanp/.conda/envs/pytorch-benchmarks/lib/python3.12/site-packages/timm/models/convit.py:372 in forward_head, code: x = x[:, 1:].mean(dim=1) if self.global_pool == 'avg' else x[:, 0]
        select_121: "f32[8, 768]" = torch.ops.aten.select.int(add_223, 1, 0);  add_223 = None
        
         # File: /home/sahanp/.conda/envs/pytorch-benchmarks/lib/python3.12/site-packages/timm/models/convit.py:373 in forward_head, code: x = self.head_drop(x)
        clone_295: "f32[8, 768]" = torch.ops.aten.clone.default(select_121);  select_121 = None
        
         # File: /home/sahanp/.conda/envs/pytorch-benchmarks/lib/python3.12/site-packages/timm/models/convit.py:374 in forward_head, code: return x if pre_logits else self.head(x)
        permute_251: "f32[768, 1000]" = torch.ops.aten.permute.default(arg179_1, [1, 0]);  arg179_1 = None
        addmm_73: "f32[8, 1000]" = torch.ops.aten.addmm.default(arg180_1, clone_295, permute_251);  arg180_1 = clone_295 = permute_251 = None
        return (addmm_73, device_put, device_put_1, device_put_2, device_put_3, device_put_4, device_put_5, device_put_6, device_put_7, device_put_8, device_put_9)
        