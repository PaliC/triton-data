buf0: ExternKernelSchedulerNode(ExternKernelAlloc)
buf0.writes = [StarDep(name='buf0', mode=None)]
buf0.unmet_dependencies = []
buf0.met_dependencies = [StarDep(name='arg200_1', mode=None), StarDep(name='arg2_1', mode=None)]
buf0.users = [NodeUser(node=SchedulerNode(name='buf1'), can_inplace=False, is_weak=False), NodeUser(node=SchedulerNode(name='buf2'), can_inplace=False, is_weak=False), NodeUser(node=SchedulerNode(name='buf3'), can_inplace=False, is_weak=False), NodeUser(node=SchedulerNode(name='buf7'), can_inplace=False, is_weak=False), NodeUser(node=SchedulerNode(name='buf18'), can_inplace=False, is_weak=False)]
buf0.node.kernel = extern_kernels.convolution


buf1: SchedulerNode(ComputedBuffer)
buf1.writes = [MemoryDep('buf1', c0, {c0: 1182}, None)]
buf1.unmet_dependencies = [   MemoryDep('buf0', 196*c1 + ModularIndexing(c0 - 1, 1, 196), {c0: 197, c1: 768}, None)]
buf1.met_dependencies = 
    [   MemoryDep('arg0_1', c1, {c0: 197, c1: 768}, None),
        MemoryDep('arg1_1', c0, {c0: 151296}, None),
        MemoryDep('arg3_1', c1, {c0: 197, c1: 768}, None)]
buf1.users = [NodeUser(node=SchedulerNode(name='buf4'), can_inplace=False, is_weak=False), NodeUser(node=SchedulerNode(name='buf5'), can_inplace=False, is_weak=False), NodeUser(node=SchedulerNode(name='buf6'), can_inplace=False, is_weak=False)]
buf1.group.device = cuda:0
buf1.group.iteration = (1182, 128)
buf1.sizes = ([197, 6], [128])
buf0_layout = FixedLayout('cuda', torch.float32, size=[1, 768, 14, 14], stride=[150528, 196, 14, 1])
arg1_1_layout = FixedLayout('cuda', torch.float32, size=[1, 197, 768], stride=[151296, 768, 1])
arg0_1_layout = FixedLayout('cuda', torch.float32, size=[1, 1, 768], stride=[768, 768, 1])
arg3_1_layout = FixedLayout('cuda', torch.float32, size=[768], stride=[1])
buf1_layout = FixedLayout('cuda', torch.float32, size=[1, 197, 1, 6], stride=[1184, 6, 1184, 1])
class buf1_loop_body:
    var_ranges = {z0: 197, z1: 6, z2: 128}
    index0 = z0
    index1 = 128*z1 + z2
    index2 = 25088*z1 + 196*z2 + ModularIndexing(z0 - 1, 1, 196)
    index3 = 768*z0 + 128*z1 + z2
    index4 = 6*z0 + z1
    def body(self, ops):
        get_index = self.get_index('index0')
        index_expr = ops.index_expr(get_index, torch.int64)
        constant = ops.constant(0, torch.int64)
        ge = ops.ge(index_expr, constant)
        get_index_1 = self.get_index('index0')
        index_expr_1 = ops.index_expr(get_index_1, torch.int64)
        constant_1 = ops.constant(1, torch.int64)
        lt = ops.lt(index_expr_1, constant_1)
        masked_subblock1 = self.masked_subblock1(lt, 0.0)
        get_index_2 = self.get_index('index0')
        index_expr_2 = ops.index_expr(get_index_2, torch.int64)
        constant_2 = ops.constant(1, torch.int64)
        ge_1 = ops.ge(index_expr_2, constant_2)
        get_index_3 = self.get_index('index0')
        index_expr_3 = ops.index_expr(get_index_3, torch.int64)
        constant_3 = ops.constant(197, torch.int64)
        lt_1 = ops.lt(index_expr_3, constant_3)
        masked_subblock2 = self.masked_subblock2(ge_1, 0.0)
        where = ops.where(lt, masked_subblock1, masked_subblock2)
        get_index_4 = self.get_index('index3')
        load = ops.load('arg1_1', get_index_4)
        add = ops.add(where, load)
        reduction = ops.reduction(torch.float32, torch.float32, 'welford_reduce', add)
        getitem = reduction[0]
        getitem_1 = reduction[1]
        getitem_2 = reduction[2]
        get_index_5 = self.get_index('index4')
        store_reduction = ops.store_reduction('buf1', get_index_5, getitem)
        return store_reduction
    def masked_subblock1(self, ops):
        get_index = self.get_index('index1')
        load = ops.load('arg0_1', get_index)
        return load
    def masked_subblock2(self, ops):
        get_index = self.get_index('index2')
        load = ops.load('buf0', get_index)
        get_index_1 = self.get_index('index1')
        load_1 = ops.load('arg3_1', get_index_1)
        add = ops.add(load, load_1)
        return add
buf1 Triton code:
    import triton
    import triton.language as tl
    from triton.compiler.compiler import AttrsDescriptor

    from torch._inductor.runtime import triton_helpers, triton_heuristics
    from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
    from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, instance_descriptor, DeviceProperties

    @triton_heuristics.reduction(
        size_hints=[2048, 128],
        reduction_hint=ReductionHint.OUTER,
        filename=__file__,
        triton_meta={'signature': {0: '*fp32', 1: '*fp32', 2: '*fp32', 3: '*fp32', 4: '*fp32', 5: 'i32', 6: 'i32'}, 'device': DeviceProperties(type='cuda', index=0, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, multi_processor_count=132), 'constants': {}, 'configs': [AttrsDescriptor(divisible_by_16=(0, 1, 2, 3, 4, 6), equal_to_1=())]},
        inductor_meta={'autotune_hints': set(), 'kernel_name': 'Placeholder.DESCRIPTIVE_NAME', 'mutated_arg_names': [], 'no_x_dim': False, 'num_load': 4, 'num_reduction': 1, 'backend_hash': '3944FAAD9D18E987535CF029C4441E186640DD453AE5AE6831EDF736C09E0A47', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': False, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False}
    )
    @triton.jit
    def triton_(in_ptr0, in_ptr1, in_ptr2, in_ptr3, out_ptr0, xnumel, rnumel, XBLOCK : tl.constexpr, RBLOCK : tl.constexpr):
        xnumel = 1182
        rnumel = 128
        xoffset = tl.program_id(0) * XBLOCK
        xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
        xmask = xindex < xnumel
        rbase = tl.arange(0, RBLOCK)[None, :]
        x1 = (xindex // 6)
        x0 = xindex % 6
        x3 = xindex
        tmp20_mean = tl.zeros([XBLOCK, RBLOCK], tl.float32)
        tmp20_m2 = tl.zeros([XBLOCK, RBLOCK], tl.float32)
        tmp20_weight = tl.zeros([XBLOCK, RBLOCK], tl.float32)
        for roffset in range(0, rnumel, RBLOCK):
            rindex = roffset + rbase
            rmask = rindex < rnumel
            r2 = rindex
            tmp17 = tl.load(in_ptr3 + (r2 + (128*x3)), rmask & xmask, eviction_policy='evict_last', other=0.0)
            tmp0 = x1
            tmp1 = tl.full([1, 1], 0, tl.int64)
            tmp2 = tmp0 >= tmp1
            tmp3 = tl.full([1, 1], 1, tl.int64)
            tmp4 = tmp0 < tmp3
            tmp5 = tl.load(in_ptr0 + (r2 + (128*x0)), rmask & tmp4 & xmask, eviction_policy='evict_last', other=0.0)
            tmp6 = tl.full(tmp5.shape, 0.0, tmp5.dtype)
            tmp7 = tl.where(tmp4, tmp5, tmp6)
            tmp8 = tmp0 >= tmp3
            tmp9 = tl.full([1, 1], 197, tl.int64)
            tmp10 = tmp0 < tmp9
            tmp11 = tl.load(in_ptr1 + ((196*r2) + (25088*x0) + (((-1) + x1) % 196)), rmask & tmp8 & xmask, eviction_policy='evict_last', other=0.0)
            tmp12 = tl.load(in_ptr2 + (r2 + (128*x0)), rmask & tmp8 & xmask, eviction_policy='evict_last', other=0.0)
            tmp13 = tmp11 + tmp12
            tmp14 = tl.full(tmp13.shape, 0.0, tmp13.dtype)
            tmp15 = tl.where(tmp8, tmp13, tmp14)
            tmp16 = tl.where(tmp4, tmp7, tmp15)
            tmp18 = tmp16 + tmp17
            tmp19 = tl.broadcast_to(tmp18, [XBLOCK, RBLOCK])
            tmp20_mean_next, tmp20_m2_next, tmp20_weight_next = triton_helpers.welford_reduce(
                tmp19, tmp20_mean, tmp20_m2, tmp20_weight, roffset == 0
            )
            tmp20_mean = tl.where(rmask & xmask, tmp20_mean_next, tmp20_mean)
            tmp20_m2 = tl.where(rmask & xmask, tmp20_m2_next, tmp20_m2)
            tmp20_weight = tl.where(rmask & xmask, tmp20_weight_next, tmp20_weight)
        tmp20_tmp, tmp21_tmp, tmp22_tmp = triton_helpers.welford(
            tmp20_mean, tmp20_m2, tmp20_weight, 1
        )
        tmp20 = tmp20_tmp[:, None]
        tmp21 = tmp21_tmp[:, None]
        tmp22 = tmp22_tmp[:, None]
        tl.store(out_ptr0 + (x3), tmp20, xmask)


buf2: SchedulerNode(ComputedBuffer)
buf2.writes = [MemoryDep('buf2', c0, {c0: 1182}, None)]
buf2.unmet_dependencies = [   MemoryDep('buf0', 196*c1 + ModularIndexing(c0 - 1, 1, 196), {c0: 197, c1: 768}, None)]
buf2.met_dependencies = 
    [   MemoryDep('arg0_1', c1, {c0: 197, c1: 768}, None),
        MemoryDep('arg1_1', c0, {c0: 151296}, None),
        MemoryDep('arg3_1', c1, {c0: 197, c1: 768}, None)]
buf2.users = [NodeUser(node=SchedulerNode(name='buf4'), can_inplace=False, is_weak=False), NodeUser(node=SchedulerNode(name='buf5'), can_inplace=False, is_weak=False), NodeUser(node=SchedulerNode(name='buf6'), can_inplace=False, is_weak=False)]
buf2.group.device = cuda:0
buf2.group.iteration = (1182, 128)
buf2.sizes = ([197, 6], [128])
buf0_layout = FixedLayout('cuda', torch.float32, size=[1, 768, 14, 14], stride=[150528, 196, 14, 1])
arg1_1_layout = FixedLayout('cuda', torch.float32, size=[1, 197, 768], stride=[151296, 768, 1])
arg0_1_layout = FixedLayout('cuda', torch.float32, size=[1, 1, 768], stride=[768, 768, 1])
arg3_1_layout = FixedLayout('cuda', torch.float32, size=[768], stride=[1])
buf2_layout = FixedLayout('cuda', torch.float32, size=[1, 197, 1, 6], stride=[1184, 6, 1184, 1])
class buf2_loop_body:
    var_ranges = {z0: 197, z1: 6, z2: 128}
    index0 = z0
    index1 = 128*z1 + z2
    index2 = 25088*z1 + 196*z2 + ModularIndexing(z0 - 1, 1, 196)
    index3 = 768*z0 + 128*z1 + z2
    index4 = 6*z0 + z1
    def body(self, ops):
        get_index = self.get_index('index0')
        index_expr = ops.index_expr(get_index, torch.int64)
        constant = ops.constant(0, torch.int64)
        ge = ops.ge(index_expr, constant)
        get_index_1 = self.get_index('index0')
        index_expr_1 = ops.index_expr(get_index_1, torch.int64)
        constant_1 = ops.constant(1, torch.int64)
        lt = ops.lt(index_expr_1, constant_1)
        masked_subblock1 = self.masked_subblock1(lt, 0.0)
        get_index_2 = self.get_index('index0')
        index_expr_2 = ops.index_expr(get_index_2, torch.int64)
        constant_2 = ops.constant(1, torch.int64)
        ge_1 = ops.ge(index_expr_2, constant_2)
        get_index_3 = self.get_index('index0')
        index_expr_3 = ops.index_expr(get_index_3, torch.int64)
        constant_3 = ops.constant(197, torch.int64)
        lt_1 = ops.lt(index_expr_3, constant_3)
        masked_subblock2 = self.masked_subblock2(ge_1, 0.0)
        where = ops.where(lt, masked_subblock1, masked_subblock2)
        get_index_4 = self.get_index('index3')
        load = ops.load('arg1_1', get_index_4)
        add = ops.add(where, load)
        reduction = ops.reduction(torch.float32, torch.float32, 'welford_reduce', add)
        getitem = reduction[0]
        getitem_1 = reduction[1]
        getitem_2 = reduction[2]
        get_index_5 = self.get_index('index4')
        store_reduction = ops.store_reduction('buf2', get_index_5, getitem_1)
        return store_reduction
    def masked_subblock1(self, ops):
        get_index = self.get_index('index1')
        load = ops.load('arg0_1', get_index)
        return load
    def masked_subblock2(self, ops):
        get_index = self.get_index('index2')
        load = ops.load('buf0', get_index)
        get_index_1 = self.get_index('index1')
        load_1 = ops.load('arg3_1', get_index_1)
        add = ops.add(load, load_1)
        return add
buf2 Triton code:
    import triton
    import triton.language as tl
    from triton.compiler.compiler import AttrsDescriptor

    from torch._inductor.runtime import triton_helpers, triton_heuristics
    from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
    from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, instance_descriptor, DeviceProperties

    @triton_heuristics.reduction(
        size_hints=[2048, 128],
        reduction_hint=ReductionHint.OUTER,
        filename=__file__,
        triton_meta={'signature': {0: '*fp32', 1: '*fp32', 2: '*fp32', 3: '*fp32', 4: '*fp32', 5: 'i32', 6: 'i32'}, 'device': DeviceProperties(type='cuda', index=0, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, multi_processor_count=132), 'constants': {}, 'configs': [AttrsDescriptor(divisible_by_16=(0, 1, 2, 3, 4, 6), equal_to_1=())]},
        inductor_meta={'autotune_hints': set(), 'kernel_name': 'Placeholder.DESCRIPTIVE_NAME', 'mutated_arg_names': [], 'no_x_dim': False, 'num_load': 4, 'num_reduction': 1, 'backend_hash': '3944FAAD9D18E987535CF029C4441E186640DD453AE5AE6831EDF736C09E0A47', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': False, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False}
    )
    @triton.jit
    def triton_(in_ptr0, in_ptr1, in_ptr2, in_ptr3, out_ptr0, xnumel, rnumel, XBLOCK : tl.constexpr, RBLOCK : tl.constexpr):
        xnumel = 1182
        rnumel = 128
        xoffset = tl.program_id(0) * XBLOCK
        xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
        xmask = xindex < xnumel
        rbase = tl.arange(0, RBLOCK)[None, :]
        x1 = (xindex // 6)
        x0 = xindex % 6
        x3 = xindex
        tmp20_mean = tl.zeros([XBLOCK, RBLOCK], tl.float32)
        tmp20_m2 = tl.zeros([XBLOCK, RBLOCK], tl.float32)
        tmp20_weight = tl.zeros([XBLOCK, RBLOCK], tl.float32)
        for roffset in range(0, rnumel, RBLOCK):
            rindex = roffset + rbase
            rmask = rindex < rnumel
            r2 = rindex
            tmp17 = tl.load(in_ptr3 + (r2 + (128*x3)), rmask & xmask, eviction_policy='evict_last', other=0.0)
            tmp0 = x1
            tmp1 = tl.full([1, 1], 0, tl.int64)
            tmp2 = tmp0 >= tmp1
            tmp3 = tl.full([1, 1], 1, tl.int64)
            tmp4 = tmp0 < tmp3
            tmp5 = tl.load(in_ptr0 + (r2 + (128*x0)), rmask & tmp4 & xmask, eviction_policy='evict_last', other=0.0)
            tmp6 = tl.full(tmp5.shape, 0.0, tmp5.dtype)
            tmp7 = tl.where(tmp4, tmp5, tmp6)
            tmp8 = tmp0 >= tmp3
            tmp9 = tl.full([1, 1], 197, tl.int64)
            tmp10 = tmp0 < tmp9
            tmp11 = tl.load(in_ptr1 + ((196*r2) + (25088*x0) + (((-1) + x1) % 196)), rmask & tmp8 & xmask, eviction_policy='evict_last', other=0.0)
            tmp12 = tl.load(in_ptr2 + (r2 + (128*x0)), rmask & tmp8 & xmask, eviction_policy='evict_last', other=0.0)
            tmp13 = tmp11 + tmp12
            tmp14 = tl.full(tmp13.shape, 0.0, tmp13.dtype)
            tmp15 = tl.where(tmp8, tmp13, tmp14)
            tmp16 = tl.where(tmp4, tmp7, tmp15)
            tmp18 = tmp16 + tmp17
            tmp19 = tl.broadcast_to(tmp18, [XBLOCK, RBLOCK])
            tmp20_mean_next, tmp20_m2_next, tmp20_weight_next = triton_helpers.welford_reduce(
                tmp19, tmp20_mean, tmp20_m2, tmp20_weight, roffset == 0
            )
            tmp20_mean = tl.where(rmask & xmask, tmp20_mean_next, tmp20_mean)
            tmp20_m2 = tl.where(rmask & xmask, tmp20_m2_next, tmp20_m2)
            tmp20_weight = tl.where(rmask & xmask, tmp20_weight_next, tmp20_weight)
        tmp20_tmp, tmp21_tmp, tmp22_tmp = triton_helpers.welford(
            tmp20_mean, tmp20_m2, tmp20_weight, 1
        )
        tmp20 = tmp20_tmp[:, None]
        tmp21 = tmp21_tmp[:, None]
        tmp22 = tmp22_tmp[:, None]
        tl.store(out_ptr0 + (x3), tmp21, xmask)


buf3: SchedulerNode(ComputedBuffer)
buf3.writes = [MemoryDep('buf3', c0, {c0: 1182}, None)]
buf3.unmet_dependencies = [   MemoryDep('buf0', 196*c1 + ModularIndexing(c0 - 1, 1, 196), {c0: 197, c1: 768}, None)]
buf3.met_dependencies = 
    [   MemoryDep('arg0_1', c1, {c0: 197, c1: 768}, None),
        MemoryDep('arg1_1', c0, {c0: 151296}, None),
        MemoryDep('arg3_1', c1, {c0: 197, c1: 768}, None)]
buf3.users = [NodeUser(node=SchedulerNode(name='buf4'), can_inplace=False, is_weak=False), NodeUser(node=SchedulerNode(name='buf5'), can_inplace=False, is_weak=False), NodeUser(node=SchedulerNode(name='buf6'), can_inplace=False, is_weak=False)]
buf3.group.device = cuda:0
buf3.group.iteration = (1182, 128)
buf3.sizes = ([197, 6], [128])
buf0_layout = FixedLayout('cuda', torch.float32, size=[1, 768, 14, 14], stride=[150528, 196, 14, 1])
arg1_1_layout = FixedLayout('cuda', torch.float32, size=[1, 197, 768], stride=[151296, 768, 1])
arg0_1_layout = FixedLayout('cuda', torch.float32, size=[1, 1, 768], stride=[768, 768, 1])
arg3_1_layout = FixedLayout('cuda', torch.float32, size=[768], stride=[1])
buf3_layout = FixedLayout('cuda', torch.float32, size=[1, 197, 1, 6], stride=[1184, 6, 1184, 1])
class buf3_loop_body:
    var_ranges = {z0: 197, z1: 6, z2: 128}
    index0 = z0
    index1 = 128*z1 + z2
    index2 = 25088*z1 + 196*z2 + ModularIndexing(z0 - 1, 1, 196)
    index3 = 768*z0 + 128*z1 + z2
    index4 = 6*z0 + z1
    def body(self, ops):
        get_index = self.get_index('index0')
        index_expr = ops.index_expr(get_index, torch.int64)
        constant = ops.constant(0, torch.int64)
        ge = ops.ge(index_expr, constant)
        get_index_1 = self.get_index('index0')
        index_expr_1 = ops.index_expr(get_index_1, torch.int64)
        constant_1 = ops.constant(1, torch.int64)
        lt = ops.lt(index_expr_1, constant_1)
        masked_subblock1 = self.masked_subblock1(lt, 0.0)
        get_index_2 = self.get_index('index0')
        index_expr_2 = ops.index_expr(get_index_2, torch.int64)
        constant_2 = ops.constant(1, torch.int64)
        ge_1 = ops.ge(index_expr_2, constant_2)
        get_index_3 = self.get_index('index0')
        index_expr_3 = ops.index_expr(get_index_3, torch.int64)
        constant_3 = ops.constant(197, torch.int64)
        lt_1 = ops.lt(index_expr_3, constant_3)
        masked_subblock2 = self.masked_subblock2(ge_1, 0.0)
        where = ops.where(lt, masked_subblock1, masked_subblock2)
        get_index_4 = self.get_index('index3')
        load = ops.load('arg1_1', get_index_4)
        add = ops.add(where, load)
        reduction = ops.reduction(torch.float32, torch.float32, 'welford_reduce', add)
        getitem = reduction[0]
        getitem_1 = reduction[1]
        getitem_2 = reduction[2]
        get_index_5 = self.get_index('index4')
        store_reduction = ops.store_reduction('buf3', get_index_5, getitem_2)
        return store_reduction
    def masked_subblock1(self, ops):
        get_index = self.get_index('index1')
        load = ops.load('arg0_1', get_index)
        return load
    def masked_subblock2(self, ops):
        get_index = self.get_index('index2')
        load = ops.load('buf0', get_index)
        get_index_1 = self.get_index('index1')
        load_1 = ops.load('arg3_1', get_index_1)
        add = ops.add(load, load_1)
        return add
buf3 Triton code:
    import triton
    import triton.language as tl
    from triton.compiler.compiler import AttrsDescriptor

    from torch._inductor.runtime import triton_helpers, triton_heuristics
    from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
    from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, instance_descriptor, DeviceProperties

    @triton_heuristics.reduction(
        size_hints=[2048, 128],
        reduction_hint=ReductionHint.OUTER,
        filename=__file__,
        triton_meta={'signature': {0: '*fp32', 1: '*fp32', 2: '*fp32', 3: '*fp32', 4: '*fp32', 5: 'i32', 6: 'i32'}, 'device': DeviceProperties(type='cuda', index=0, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, multi_processor_count=132), 'constants': {}, 'configs': [AttrsDescriptor(divisible_by_16=(0, 1, 2, 3, 4, 6), equal_to_1=())]},
        inductor_meta={'autotune_hints': set(), 'kernel_name': 'Placeholder.DESCRIPTIVE_NAME', 'mutated_arg_names': [], 'no_x_dim': False, 'num_load': 4, 'num_reduction': 1, 'backend_hash': '3944FAAD9D18E987535CF029C4441E186640DD453AE5AE6831EDF736C09E0A47', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': False, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False}
    )
    @triton.jit
    def triton_(in_ptr0, in_ptr1, in_ptr2, in_ptr3, out_ptr0, xnumel, rnumel, XBLOCK : tl.constexpr, RBLOCK : tl.constexpr):
        xnumel = 1182
        rnumel = 128
        xoffset = tl.program_id(0) * XBLOCK
        xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
        xmask = xindex < xnumel
        rbase = tl.arange(0, RBLOCK)[None, :]
        x1 = (xindex // 6)
        x0 = xindex % 6
        x3 = xindex
        tmp20_mean = tl.zeros([XBLOCK, RBLOCK], tl.float32)
        tmp20_m2 = tl.zeros([XBLOCK, RBLOCK], tl.float32)
        tmp20_weight = tl.zeros([XBLOCK, RBLOCK], tl.float32)
        for roffset in range(0, rnumel, RBLOCK):
            rindex = roffset + rbase
            rmask = rindex < rnumel
            r2 = rindex
            tmp17 = tl.load(in_ptr3 + (r2 + (128*x3)), rmask & xmask, eviction_policy='evict_last', other=0.0)
            tmp0 = x1
            tmp1 = tl.full([1, 1], 0, tl.int64)
            tmp2 = tmp0 >= tmp1
            tmp3 = tl.full([1, 1], 1, tl.int64)
            tmp4 = tmp0 < tmp3
            tmp5 = tl.load(in_ptr0 + (r2 + (128*x0)), rmask & tmp4 & xmask, eviction_policy='evict_last', other=0.0)
            tmp6 = tl.full(tmp5.shape, 0.0, tmp5.dtype)
            tmp7 = tl.where(tmp4, tmp5, tmp6)
            tmp8 = tmp0 >= tmp3
            tmp9 = tl.full([1, 1], 197, tl.int64)
            tmp10 = tmp0 < tmp9
            tmp11 = tl.load(in_ptr1 + ((196*r2) + (25088*x0) + (((-1) + x1) % 196)), rmask & tmp8 & xmask, eviction_policy='evict_last', other=0.0)
            tmp12 = tl.load(in_ptr2 + (r2 + (128*x0)), rmask & tmp8 & xmask, eviction_policy='evict_last', other=0.0)
            tmp13 = tmp11 + tmp12
            tmp14 = tl.full(tmp13.shape, 0.0, tmp13.dtype)
            tmp15 = tl.where(tmp8, tmp13, tmp14)
            tmp16 = tl.where(tmp4, tmp7, tmp15)
            tmp18 = tmp16 + tmp17
            tmp19 = tl.broadcast_to(tmp18, [XBLOCK, RBLOCK])
            tmp20_mean_next, tmp20_m2_next, tmp20_weight_next = triton_helpers.welford_reduce(
                tmp19, tmp20_mean, tmp20_m2, tmp20_weight, roffset == 0
            )
            tmp20_mean = tl.where(rmask & xmask, tmp20_mean_next, tmp20_mean)
            tmp20_m2 = tl.where(rmask & xmask, tmp20_m2_next, tmp20_m2)
            tmp20_weight = tl.where(rmask & xmask, tmp20_weight_next, tmp20_weight)
        tmp20_tmp, tmp21_tmp, tmp22_tmp = triton_helpers.welford(
            tmp20_mean, tmp20_m2, tmp20_weight, 1
        )
        tmp20 = tmp20_tmp[:, None]
        tmp21 = tmp21_tmp[:, None]
        tmp22 = tmp22_tmp[:, None]
        tl.store(out_ptr0 + (x3), tmp22, xmask)


buf4: SchedulerNode(ComputedBuffer)
buf4.writes = [MemoryDep('buf4', c0, {c0: 197}, None)]
buf4.unmet_dependencies = 
    [   MemoryDep('buf1', c0, {c0: 1182}, None),
        MemoryDep('buf2', c0, {c0: 1182}, None),
        MemoryDep('buf3', c0, {c0: 1182}, None)]
buf4.met_dependencies = []
buf4.users = [NodeUser(node=SchedulerNode(name='buf7'), can_inplace=False, is_weak=False)]
buf4.group.device = cuda:0
buf4.group.iteration = (197, 6)
buf4.sizes = ([197], [6])
buf3_layout = FixedLayout('cuda', torch.float32, size=[1, 197, 1, 6], stride=[1184, 6, 1184, 1])
buf2_layout = FixedLayout('cuda', torch.float32, size=[1, 197, 1, 6], stride=[1184, 6, 1184, 1])
buf1_layout = FixedLayout('cuda', torch.float32, size=[1, 197, 1, 6], stride=[1184, 6, 1184, 1])
buf4_layout = FixedLayout('cuda', torch.float32, size=[1, 197, 1], stride=[197, 1, 197])
class buf4_loop_body:
    var_ranges = {z0: 197, z1: 6}
    index0 = 6*z0 + z1
    index1 = z0
    def body(self, ops):
        get_index = self.get_index('index0')
        load = ops.load('buf1', get_index)
        get_index_1 = self.get_index('index0')
        load_1 = ops.load('buf2', get_index_1)
        get_index_2 = self.get_index('index0')
        load_2 = ops.load('buf3', get_index_2)
        reduction = ops.reduction(torch.float32, torch.float32, 'welford_combine', (load, load_1, load_2))
        getitem = reduction[0]
        getitem_1 = reduction[1]
        getitem_2 = reduction[2]
        get_index_3 = self.get_index('index1')
        store_reduction = ops.store_reduction('buf4', get_index_3, getitem)
        return store_reduction
buf4 Triton code:
    import triton
    import triton.language as tl
    from triton.compiler.compiler import AttrsDescriptor

    from torch._inductor.runtime import triton_helpers, triton_heuristics
    from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
    from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, instance_descriptor, DeviceProperties

    @triton_heuristics.persistent_reduction(
        size_hints=[256, 8],
        reduction_hint=ReductionHint.INNER,
        filename=__file__,
        triton_meta={'signature': {0: '*fp32', 1: '*fp32', 2: '*fp32', 3: '*fp32', 4: 'i32', 5: 'i32'}, 'device': DeviceProperties(type='cuda', index=0, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, multi_processor_count=132), 'constants': {}, 'configs': [AttrsDescriptor(divisible_by_16=(0, 1, 2, 3), equal_to_1=())]},
        inductor_meta={'autotune_hints': set(), 'kernel_name': 'Placeholder.DESCRIPTIVE_NAME', 'mutated_arg_names': [], 'no_x_dim': False, 'num_load': 3, 'num_reduction': 1, 'backend_hash': '3944FAAD9D18E987535CF029C4441E186640DD453AE5AE6831EDF736C09E0A47', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': False, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False}
    )
    @triton.jit
    def triton_(in_ptr0, in_ptr1, in_ptr2, out_ptr0, xnumel, rnumel, XBLOCK : tl.constexpr):
        xnumel = 197
        rnumel = 6
        RBLOCK: tl.constexpr = 8
        xoffset = tl.program_id(0) * XBLOCK
        xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
        xmask = xindex < xnumel
        rindex = tl.arange(0, RBLOCK)[None, :]
        roffset = 0
        rmask = rindex < rnumel
        r1 = rindex
        x0 = xindex
        tmp0 = tl.load(in_ptr0 + (r1 + (6*x0)), rmask & xmask, other=0.0)
        tmp1 = tl.load(in_ptr1 + (r1 + (6*x0)), rmask & xmask, other=0.0)
        tmp2 = tl.load(in_ptr2 + (r1 + (6*x0)), rmask & xmask, other=0.0)
        tmp3 = tl.broadcast_to(tmp0, [XBLOCK, RBLOCK])
        tmp4 = tl.broadcast_to(tmp1, [XBLOCK, RBLOCK])
        tmp5 = tl.broadcast_to(tmp2, [XBLOCK, RBLOCK])
        tmp7 = tl.where(rmask & xmask, tmp3, 0)
        tmp8 = tl.where(rmask & xmask, tmp4, 0)
        tmp9 = tl.where(rmask & xmask, tmp5, 0)
        tmp10, tmp11, tmp12 = triton_helpers.welford(tmp7, tmp8, tmp9, 1)
        tmp13 = tmp10[:, None]
        tmp14 = tmp11[:, None]
        tmp15 = tmp12[:, None]
        tl.store(out_ptr0 + (x0), tmp13, xmask)


buf5: SchedulerNode(ComputedBuffer)
buf5.writes = [MemoryDep('buf5', c0, {c0: 197}, None)]
buf5.unmet_dependencies = 
    [   MemoryDep('buf1', c0, {c0: 1182}, None),
        MemoryDep('buf2', c0, {c0: 1182}, None),
        MemoryDep('buf3', c0, {c0: 1182}, None)]
buf5.met_dependencies = []
buf5.users = [NodeUser(node=SchedulerNode(name='buf7'), can_inplace=False, is_weak=False)]
buf5.group.device = cuda:0
buf5.group.iteration = (197, 6)
buf5.sizes = ([197], [6])
buf3_layout = FixedLayout('cuda', torch.float32, size=[1, 197, 1, 6], stride=[1184, 6, 1184, 1])
buf2_layout = FixedLayout('cuda', torch.float32, size=[1, 197, 1, 6], stride=[1184, 6, 1184, 1])
buf1_layout = FixedLayout('cuda', torch.float32, size=[1, 197, 1, 6], stride=[1184, 6, 1184, 1])
buf5_layout = FixedLayout('cuda', torch.float32, size=[1, 197, 1], stride=[197, 1, 197])
class buf5_loop_body:
    var_ranges = {z0: 197, z1: 6}
    index0 = 6*z0 + z1
    index1 = z0
    def body(self, ops):
        get_index = self.get_index('index0')
        load = ops.load('buf1', get_index)
        get_index_1 = self.get_index('index0')
        load_1 = ops.load('buf2', get_index_1)
        get_index_2 = self.get_index('index0')
        load_2 = ops.load('buf3', get_index_2)
        reduction = ops.reduction(torch.float32, torch.float32, 'welford_combine', (load, load_1, load_2))
        getitem = reduction[0]
        getitem_1 = reduction[1]
        getitem_2 = reduction[2]
        get_index_3 = self.get_index('index1')
        store_reduction = ops.store_reduction('buf5', get_index_3, getitem_1)
        return store_reduction
buf5 Triton code:
    import triton
    import triton.language as tl
    from triton.compiler.compiler import AttrsDescriptor

    from torch._inductor.runtime import triton_helpers, triton_heuristics
    from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
    from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, instance_descriptor, DeviceProperties

    @triton_heuristics.persistent_reduction(
        size_hints=[256, 8],
        reduction_hint=ReductionHint.INNER,
        filename=__file__,
        triton_meta={'signature': {0: '*fp32', 1: '*fp32', 2: '*fp32', 3: '*fp32', 4: 'i32', 5: 'i32'}, 'device': DeviceProperties(type='cuda', index=0, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, multi_processor_count=132), 'constants': {}, 'configs': [AttrsDescriptor(divisible_by_16=(0, 1, 2, 3), equal_to_1=())]},
        inductor_meta={'autotune_hints': set(), 'kernel_name': 'Placeholder.DESCRIPTIVE_NAME', 'mutated_arg_names': [], 'no_x_dim': False, 'num_load': 3, 'num_reduction': 1, 'backend_hash': '3944FAAD9D18E987535CF029C4441E186640DD453AE5AE6831EDF736C09E0A47', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': False, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False}
    )
    @triton.jit
    def triton_(in_ptr0, in_ptr1, in_ptr2, out_ptr0, xnumel, rnumel, XBLOCK : tl.constexpr):
        xnumel = 197
        rnumel = 6
        RBLOCK: tl.constexpr = 8
        xoffset = tl.program_id(0) * XBLOCK
        xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
        xmask = xindex < xnumel
        rindex = tl.arange(0, RBLOCK)[None, :]
        roffset = 0
        rmask = rindex < rnumel
        r1 = rindex
        x0 = xindex
        tmp0 = tl.load(in_ptr0 + (r1 + (6*x0)), rmask & xmask, other=0.0)
        tmp1 = tl.load(in_ptr1 + (r1 + (6*x0)), rmask & xmask, other=0.0)
        tmp2 = tl.load(in_ptr2 + (r1 + (6*x0)), rmask & xmask, other=0.0)
        tmp3 = tl.broadcast_to(tmp0, [XBLOCK, RBLOCK])
        tmp4 = tl.broadcast_to(tmp1, [XBLOCK, RBLOCK])
        tmp5 = tl.broadcast_to(tmp2, [XBLOCK, RBLOCK])
        tmp7 = tl.where(rmask & xmask, tmp3, 0)
        tmp8 = tl.where(rmask & xmask, tmp4, 0)
        tmp9 = tl.where(rmask & xmask, tmp5, 0)
        tmp10, tmp11, tmp12 = triton_helpers.welford(tmp7, tmp8, tmp9, 1)
        tmp13 = tmp10[:, None]
        tmp14 = tmp11[:, None]
        tmp15 = tmp12[:, None]
        tl.store(out_ptr0 + (x0), tmp14, xmask)


buf7: SchedulerNode(ComputedBuffer)
buf7.writes = [MemoryDep('buf7', c0, {c0: 151296}, None)]
buf7.unmet_dependencies = 
    [   MemoryDep('buf0', 196*c1 + ModularIndexing(c0 - 1, 1, 196), {c0: 197, c1: 768}, None),
        MemoryDep('buf4', c0, {c0: 197}, None),
        MemoryDep('buf5', c0, {c0: 197}, None)]
buf7.met_dependencies = 
    [   MemoryDep('arg0_1', c1, {c0: 197, c1: 768}, None),
        MemoryDep('arg1_1', c0, {c0: 151296}, None),
        MemoryDep('arg3_1', c1, {c0: 197, c1: 768}, None),
        MemoryDep('arg4_1', c1, {c0: 197, c1: 768}, None)]
buf7.users = [NodeUser(node=SchedulerNode(name='buf8'), can_inplace=True, is_weak=False)]
buf7.group.device = cuda:0
buf7.group.iteration = (151296, 1)
buf7.sizes = ([197, 768], [])
buf5_layout = FixedLayout('cuda', torch.float32, size=[1, 197, 1], stride=[197, 1, 197])
buf0_layout = FixedLayout('cuda', torch.float32, size=[1, 768, 14, 14], stride=[150528, 196, 14, 1])
arg0_1_layout = FixedLayout('cuda', torch.float32, size=[1, 1, 768], stride=[768, 768, 1])
buf4_layout = FixedLayout('cuda', torch.float32, size=[1, 197, 1], stride=[197, 1, 197])
arg1_1_layout = FixedLayout('cuda', torch.float32, size=[1, 197, 768], stride=[151296, 768, 1])
arg3_1_layout = FixedLayout('cuda', torch.float32, size=[768], stride=[1])
arg4_1_layout = FixedLayout('cuda', torch.float32, size=[768], stride=[1])
buf7_layout = FixedLayout('cuda', torch.float32, size=[1, 197, 768], stride=[151296, 768, 1])
class buf7_loop_body:
    var_ranges = {z0: 197, z1: 768}
    index0 = z0
    index1 = z1
    index2 = 196*z1 + ModularIndexing(z0 - 1, 1, 196)
    index3 = 768*z0 + z1
    def body(self, ops):
        get_index = self.get_index('index0')
        index_expr = ops.index_expr(get_index, torch.int64)
        constant = ops.constant(0, torch.int64)
        ge = ops.ge(index_expr, constant)
        get_index_1 = self.get_index('index0')
        index_expr_1 = ops.index_expr(get_index_1, torch.int64)
        constant_1 = ops.constant(1, torch.int64)
        lt = ops.lt(index_expr_1, constant_1)
        masked_subblock1 = self.masked_subblock1(lt, 0.0)
        get_index_2 = self.get_index('index0')
        index_expr_2 = ops.index_expr(get_index_2, torch.int64)
        constant_2 = ops.constant(1, torch.int64)
        ge_1 = ops.ge(index_expr_2, constant_2)
        get_index_3 = self.get_index('index0')
        index_expr_3 = ops.index_expr(get_index_3, torch.int64)
        constant_3 = ops.constant(197, torch.int64)
        lt_1 = ops.lt(index_expr_3, constant_3)
        masked_subblock2 = self.masked_subblock2(ge_1, 0.0)
        where = ops.where(lt, masked_subblock1, masked_subblock2)
        get_index_4 = self.get_index('index3')
        load = ops.load('arg1_1', get_index_4)
        add = ops.add(where, load)
        get_index_5 = self.get_index('index0')
        load_1 = ops.load('buf4', get_index_5)
        sub = ops.sub(add, load_1)
        get_index_6 = self.get_index('index0')
        load_2 = ops.load('buf5', get_index_6)
        constant_4 = ops.constant(768.0, torch.float32)
        truediv = ops.truediv(load_2, constant_4)
        constant_5 = ops.constant(1e-12, torch.float32)
        add_1 = ops.add(truediv, constant_5)
        rsqrt = ops.rsqrt(add_1)
        mul = ops.mul(sub, rsqrt)
        get_index_7 = self.get_index('index1')
        load_3 = ops.load('arg4_1', get_index_7)
        mul_1 = ops.mul(mul, load_3)
        get_index_8 = self.get_index('index3')
        store = ops.store('buf7', get_index_8, mul_1, None)
        return store
    def masked_subblock1(self, ops):
        get_index = self.get_index('index1')
        load = ops.load('arg0_1', get_index)
        return load
    def masked_subblock2(self, ops):
        get_index = self.get_index('index2')
        load = ops.load('buf0', get_index)
        get_index_1 = self.get_index('index1')
        load_1 = ops.load('arg3_1', get_index_1)
        add = ops.add(load, load_1)
        return add
buf7 Triton code:
    import triton
    import triton.language as tl
    from triton.compiler.compiler import AttrsDescriptor

    from torch._inductor.runtime import triton_helpers, triton_heuristics
    from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
    from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, instance_descriptor, DeviceProperties

    @triton_heuristics.pointwise(
        size_hints=[262144], 
        filename=__file__,
        triton_meta={'signature': {0: '*fp32', 1: '*fp32', 2: '*fp32', 3: '*fp32', 4: '*fp32', 5: '*fp32', 6: '*fp32', 7: '*fp32', 8: 'i32'}, 'device': DeviceProperties(type='cuda', index=0, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, multi_processor_count=132), 'constants': {}, 'configs': [AttrsDescriptor(divisible_by_16=(0, 1, 2, 3, 4, 5, 6, 7, 8), equal_to_1=())]},
        inductor_meta={'autotune_hints': set(), 'kernel_name': 'Placeholder.DESCRIPTIVE_NAME', 'mutated_arg_names': [], 'no_x_dim': False, 'num_load': 7, 'num_reduction': 0, 'backend_hash': '3944FAAD9D18E987535CF029C4441E186640DD453AE5AE6831EDF736C09E0A47', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': False, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False},
        min_elem_per_thread=0
    )
    @triton.jit
    def triton_(in_ptr0, in_ptr1, in_ptr2, in_ptr3, in_ptr4, in_ptr5, in_ptr6, out_ptr0, xnumel, XBLOCK : tl.constexpr):
        xnumel = 151296
        xoffset = tl.program_id(0) * XBLOCK
        xindex = xoffset + tl.arange(0, XBLOCK)[:]
        xmask = xindex < xnumel
        x1 = (xindex // 768)
        x0 = xindex % 768
        x2 = xindex
        tmp17 = tl.load(in_ptr3 + (x2), xmask)
        tmp19 = tl.load(in_ptr4 + (x1), xmask, eviction_policy='evict_last')
        tmp21 = tl.load(in_ptr5 + (x1), xmask, eviction_policy='evict_last')
        tmp28 = tl.load(in_ptr6 + (x0), xmask, eviction_policy='evict_last')
        tmp0 = x1
        tmp1 = tl.full([1], 0, tl.int64)
        tmp2 = tmp0 >= tmp1
        tmp3 = tl.full([1], 1, tl.int64)
        tmp4 = tmp0 < tmp3
        tmp5 = tl.load(in_ptr0 + (x0), tmp4 & xmask, eviction_policy='evict_last', other=0.0)
        tmp6 = tl.full(tmp5.shape, 0.0, tmp5.dtype)
        tmp7 = tl.where(tmp4, tmp5, tmp6)
        tmp8 = tmp0 >= tmp3
        tmp9 = tl.full([1], 197, tl.int64)
        tmp10 = tmp0 < tmp9
        tmp11 = tl.load(in_ptr1 + ((196*x0) + (((-1) + x1) % 196)), tmp8 & xmask, eviction_policy='evict_last', other=0.0)
        tmp12 = tl.load(in_ptr2 + (x0), tmp8 & xmask, eviction_policy='evict_last', other=0.0)
        tmp13 = tmp11 + tmp12
        tmp14 = tl.full(tmp13.shape, 0.0, tmp13.dtype)
        tmp15 = tl.where(tmp8, tmp13, tmp14)
        tmp16 = tl.where(tmp4, tmp7, tmp15)
        tmp18 = tmp16 + tmp17
        tmp20 = tmp18 - tmp19
        tmp22 = 768.0
        tmp23 = tmp21 / tmp22
        tmp24 = 1e-12
        tmp25 = tmp23 + tmp24
        tmp26 = libdevice.rsqrt(tmp25)
        tmp27 = tmp20 * tmp26
        tmp29 = tmp27 * tmp28
        tl.store(out_ptr0 + (x2), tmp29, xmask)


buf8: SchedulerNode(ComputedBuffer)
buf8.writes = [MemoryDep('buf8', c0, {c0: 151296}, None)]
buf8.unmet_dependencies = [MemoryDep('buf7', c0, {c0: 151296}, None)]
buf8.met_dependencies = [MemoryDep('arg5_1', c1, {c0: 197, c1: 768}, None)]
buf8.users = [NodeUser(node=ExternKernelSchedulerNode(name='buf9'), can_inplace=False, is_weak=False), NodeUser(node=ExternKernelSchedulerNode(name='buf10'), can_inplace=False, is_weak=False), NodeUser(node=ExternKernelSchedulerNode(name='buf11'), can_inplace=False, is_weak=False)]
buf8.group.device = cuda:0
buf8.group.iteration = (151296, 1)
buf8.sizes = ([197, 768], [])
buf7_layout = FixedLayout('cuda', torch.float32, size=[1, 197, 768], stride=[151296, 768, 1])
arg5_1_layout = FixedLayout('cuda', torch.float32, size=[768], stride=[1])
buf8_layout = FixedLayout('cuda', torch.float32, size=[1, 197, 768], stride=[151296, 768, 1])
class buf8_loop_body:
    var_ranges = {z0: 197, z1: 768}
    index0 = 768*z0 + z1
    index1 = z1
    def body(self, ops):
        get_index = self.get_index('index0')
        load = ops.load('buf7', get_index)
        get_index_1 = self.get_index('index1')
        load_1 = ops.load('arg5_1', get_index_1)
        add = ops.add(load, load_1)
        get_index_2 = self.get_index('index0')
        store = ops.store('buf8', get_index_2, add, None)
        return store
buf8 Triton code:
    import triton
    import triton.language as tl
    from triton.compiler.compiler import AttrsDescriptor

    from torch._inductor.runtime import triton_helpers, triton_heuristics
    from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
    from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, instance_descriptor, DeviceProperties

    @triton_heuristics.pointwise(
        size_hints=[262144], 
        filename=__file__,
        triton_meta={'signature': {0: '*fp32', 1: '*fp32', 2: 'i32'}, 'device': DeviceProperties(type='cuda', index=0, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, multi_processor_count=132), 'constants': {}, 'configs': [AttrsDescriptor(divisible_by_16=(0, 1, 2), equal_to_1=())]},
        inductor_meta={'autotune_hints': set(), 'kernel_name': 'Placeholder.DESCRIPTIVE_NAME', 'mutated_arg_names': ['in_out_ptr0'], 'no_x_dim': False, 'num_load': 2, 'num_reduction': 0, 'backend_hash': '3944FAAD9D18E987535CF029C4441E186640DD453AE5AE6831EDF736C09E0A47', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': False, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False},
        min_elem_per_thread=0
    )
    @triton.jit
    def triton_(in_out_ptr0, in_ptr0, xnumel, XBLOCK : tl.constexpr):
        xnumel = 151296
        xoffset = tl.program_id(0) * XBLOCK
        xindex = xoffset + tl.arange(0, XBLOCK)[:]
        xmask = xindex < xnumel
        x2 = xindex
        x0 = xindex % 768
        tmp0 = tl.load(in_out_ptr0 + (x2), xmask)
        tmp1 = tl.load(in_ptr0 + (x0), xmask, eviction_policy='evict_last')
        tmp2 = tmp0 + tmp1
        tl.store(in_out_ptr0 + (x2), tmp2, xmask)


buf9: ExternKernelSchedulerNode(ExternKernelOut)
buf9.writes = [StarDep(name='buf9', mode=None)]
buf9.unmet_dependencies = [StarDep(name='buf8', mode=None)]
buf9.met_dependencies = [StarDep(name='arg6_1', mode=None), StarDep(name='arg7_1', mode=None)]
buf9.users = [NodeUser(node=ExternKernelSchedulerNode(name='buf12'), can_inplace=False, is_weak=False)]
buf9.node.kernel = extern_kernels.addmm


buf10: ExternKernelSchedulerNode(ExternKernelOut)
buf10.writes = [StarDep(name='buf10', mode=None)]
buf10.unmet_dependencies = [StarDep(name='buf8', mode=None)]
buf10.met_dependencies = [StarDep(name='arg8_1', mode=None), StarDep(name='arg9_1', mode=None)]
buf10.users = [NodeUser(node=ExternKernelSchedulerNode(name='buf12'), can_inplace=False, is_weak=False)]
buf10.node.kernel = extern_kernels.addmm


buf11: ExternKernelSchedulerNode(ExternKernelOut)
buf11.writes = [StarDep(name='buf11', mode=None)]
buf11.unmet_dependencies = [StarDep(name='buf8', mode=None)]
buf11.met_dependencies = [StarDep(name='arg10_1', mode=None), StarDep(name='arg11_1', mode=None)]
buf11.users = [NodeUser(node=ExternKernelSchedulerNode(name='buf12'), can_inplace=False, is_weak=False)]
buf11.node.kernel = extern_kernels.addmm


buf12: ExternKernelSchedulerNode(FallbackKernel)
buf12.writes = [StarDep(name='buf12', mode=None)]
buf12.unmet_dependencies = 
    [   StarDep(name='buf10', mode=None),
        StarDep(name='buf11', mode=None),
        StarDep(name='buf9', mode=None)]
buf12.met_dependencies = []
buf12.users = [NodeUser(node=ExternKernelSchedulerNode(name='buf13'), can_inplace=False, is_weak=False), NodeUser(node=ExternKernelSchedulerNode(name='buf14'), can_inplace=False, is_weak=False), NodeUser(node=ExternKernelSchedulerNode(name='buf15'), can_inplace=False, is_weak=False), NodeUser(node=ExternKernelSchedulerNode(name='buf16'), can_inplace=False, is_weak=False)]
buf12.node.kernel = None


buf13: ExternKernelSchedulerNode(MultiOutput)
buf13.writes = [StarDep(name='buf13', mode=None)]
buf13.unmet_dependencies = [StarDep(name='buf12', mode=None)]
buf13.met_dependencies = []
buf13.users = [NodeUser(node=ExternKernelSchedulerNode(name='buf17'), can_inplace=False, is_weak=False)]
buf13.node.kernel = None


buf17: ExternKernelSchedulerNode(ExternKernelOut)
buf17.writes = [StarDep(name='buf17', mode=None)]
buf17.unmet_dependencies = [StarDep(name='buf13', mode=None)]
buf17.met_dependencies = [StarDep(name='arg12_1', mode=None)]
buf17.users = [NodeUser(node=SchedulerNode(name='buf18'), can_inplace=True, is_weak=False)]
buf17.node.kernel = extern_kernels.mm


buf18: SchedulerNode(ComputedBuffer)
buf18.writes = [MemoryDep('buf18', c0, {c0: 151296}, None)]
buf18.unmet_dependencies = 
    [   MemoryDep('buf0', 196*c1 + ModularIndexing(c0 - 1, 1, 196), {c0: 197, c1: 768}, None),
        MemoryDep('buf17', c0, {c0: 151296}, None)]
buf18.met_dependencies = 
    [   MemoryDep('arg0_1', c1, {c0: 197, c1: 768}, None),
        MemoryDep('arg13_1', c1, {c0: 197, c1: 768}, None),
        MemoryDep('arg1_1', c0, {c0: 151296}, None),
        MemoryDep('arg3_1', c1, {c0: 197, c1: 768}, None)]
buf18.users = [NodeUser(node=SchedulerNode(name='buf19'), can_inplace=False, is_weak=False), NodeUser(node=SchedulerNode(name='buf20'), can_inplace=False, is_weak=False), NodeUser(node=SchedulerNode(name='buf21'), can_inplace=False, is_weak=False), NodeUser(node=SchedulerNode(name='buf22'), can_inplace=True, is_weak=False), NodeUser(node=SchedulerNode(name='buf26'), can_inplace=False, is_weak=False), NodeUser(node=SchedulerNode(name='buf27'), can_inplace=False, is_weak=False), NodeUser(node=SchedulerNode(name='buf28'), can_inplace=False, is_weak=False), NodeUser(node=SchedulerNode(name='buf29'), can_inplace=True, is_weak=False), NodeUser(node=SchedulerNode(name='buf39'), can_inplace=True, is_weak=False)]
buf18.group.device = cuda:0
buf18.group.iteration = (151296, 1)
buf18.sizes = ([197, 768], [])
buf0_layout = FixedLayout('cuda', torch.float32, size=[1, 768, 14, 14], stride=[150528, 196, 14, 1])
buf17_layout = FixedLayout('cuda', torch.float32, size=[197, 768], stride=[768, 1])
arg13_1_layout = FixedLayout('cuda', torch.float32, size=[768], stride=[1])
arg0_1_layout = FixedLayout('cuda', torch.float32, size=[1, 1, 768], stride=[768, 768, 1])
arg3_1_layout = FixedLayout('cuda', torch.float32, size=[768], stride=[1])
arg1_1_layout = FixedLayout('cuda', torch.float32, size=[1, 197, 768], stride=[151296, 768, 1])
buf18_layout = FixedLayout('cuda', torch.float32, size=[1, 197, 768], stride=[151296, 768, 1])
class buf18_loop_body:
    var_ranges = {z0: 197, z1: 768}
    index0 = 768*z0 + z1
    index1 = z1
    index2 = z0
    index3 = 196*z1 + ModularIndexing(z0 - 1, 1, 196)
    def body(self, ops):
        get_index = self.get_index('index0')
        load = ops.load('buf17', get_index)
        get_index_1 = self.get_index('index1')
        load_1 = ops.load('arg13_1', get_index_1)
        add = ops.add(load, load_1)
        get_index_2 = self.get_index('index2')
        index_expr = ops.index_expr(get_index_2, torch.int64)
        constant = ops.constant(0, torch.int64)
        ge = ops.ge(index_expr, constant)
        get_index_3 = self.get_index('index2')
        index_expr_1 = ops.index_expr(get_index_3, torch.int64)
        constant_1 = ops.constant(1, torch.int64)
        lt = ops.lt(index_expr_1, constant_1)
        masked_subblock1 = self.masked_subblock1(lt, 0.0)
        get_index_4 = self.get_index('index2')
        index_expr_2 = ops.index_expr(get_index_4, torch.int64)
        constant_2 = ops.constant(1, torch.int64)
        ge_1 = ops.ge(index_expr_2, constant_2)
        get_index_5 = self.get_index('index2')
        index_expr_3 = ops.index_expr(get_index_5, torch.int64)
        constant_3 = ops.constant(197, torch.int64)
        lt_1 = ops.lt(index_expr_3, constant_3)
        masked_subblock2 = self.masked_subblock2(ge_1, 0.0)
        where = ops.where(lt, masked_subblock1, masked_subblock2)
        get_index_6 = self.get_index('index0')
        load_2 = ops.load('arg1_1', get_index_6)
        add_1 = ops.add(where, load_2)
        add_2 = ops.add(add, add_1)
        get_index_7 = self.get_index('index0')
        store = ops.store('buf18', get_index_7, add_2, None)
        return store
    def masked_subblock1(self, ops):
        get_index = self.get_index('index1')
        load = ops.load('arg0_1', get_index)
        return load
    def masked_subblock2(self, ops):
        get_index = self.get_index('index3')
        load = ops.load('buf0', get_index)
        get_index_1 = self.get_index('index1')
        load_1 = ops.load('arg3_1', get_index_1)
        add = ops.add(load, load_1)
        return add
buf18 Triton code:
    import triton
    import triton.language as tl
    from triton.compiler.compiler import AttrsDescriptor

    from torch._inductor.runtime import triton_helpers, triton_heuristics
    from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
    from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, instance_descriptor, DeviceProperties

    @triton_heuristics.pointwise(
        size_hints=[262144], 
        filename=__file__,
        triton_meta={'signature': {0: '*fp32', 1: '*fp32', 2: '*fp32', 3: '*fp32', 4: '*fp32', 5: '*fp32', 6: 'i32'}, 'device': DeviceProperties(type='cuda', index=0, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, multi_processor_count=132), 'constants': {}, 'configs': [AttrsDescriptor(divisible_by_16=(0, 1, 2, 3, 4, 5, 6), equal_to_1=())]},
        inductor_meta={'autotune_hints': set(), 'kernel_name': 'Placeholder.DESCRIPTIVE_NAME', 'mutated_arg_names': ['in_out_ptr0'], 'no_x_dim': False, 'num_load': 6, 'num_reduction': 0, 'backend_hash': '3944FAAD9D18E987535CF029C4441E186640DD453AE5AE6831EDF736C09E0A47', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': False, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False},
        min_elem_per_thread=0
    )
    @triton.jit
    def triton_(in_out_ptr0, in_ptr0, in_ptr1, in_ptr2, in_ptr3, in_ptr4, xnumel, XBLOCK : tl.constexpr):
        xnumel = 151296
        xoffset = tl.program_id(0) * XBLOCK
        xindex = xoffset + tl.arange(0, XBLOCK)[:]
        xmask = xindex < xnumel
        x2 = xindex
        x0 = xindex % 768
        x1 = (xindex // 768)
        tmp0 = tl.load(in_out_ptr0 + (x2), xmask)
        tmp1 = tl.load(in_ptr0 + (x0), xmask, eviction_policy='evict_last')
        tmp20 = tl.load(in_ptr4 + (x2), xmask)
        tmp2 = tmp0 + tmp1
        tmp3 = x1
        tmp4 = tl.full([1], 0, tl.int64)
        tmp5 = tmp3 >= tmp4
        tmp6 = tl.full([1], 1, tl.int64)
        tmp7 = tmp3 < tmp6
        tmp8 = tl.load(in_ptr1 + (x0), tmp7 & xmask, eviction_policy='evict_last', other=0.0)
        tmp9 = tl.full(tmp8.shape, 0.0, tmp8.dtype)
        tmp10 = tl.where(tmp7, tmp8, tmp9)
        tmp11 = tmp3 >= tmp6
        tmp12 = tl.full([1], 197, tl.int64)
        tmp13 = tmp3 < tmp12
        tmp14 = tl.load(in_ptr2 + ((196*x0) + (((-1) + x1) % 196)), tmp11 & xmask, eviction_policy='evict_last', other=0.0)
        tmp15 = tl.load(in_ptr3 + (x0), tmp11 & xmask, eviction_policy='evict_last', other=0.0)
        tmp16 = tmp14 + tmp15
        tmp17 = tl.full(tmp16.shape, 0.0, tmp16.dtype)
        tmp18 = tl.where(tmp11, tmp16, tmp17)
        tmp19 = tl.where(tmp7, tmp10, tmp18)
        tmp21 = tmp19 + tmp20
        tmp22 = tmp2 + tmp21
        tl.store(in_out_ptr0 + (x2), tmp22, xmask)


buf19: SchedulerNode(ComputedBuffer)
buf19.writes = [MemoryDep('buf19', c0, {c0: 197}, None)]
buf19.unmet_dependencies = [MemoryDep('buf18', c0, {c0: 151296}, None)]
buf19.met_dependencies = []
buf19.users = [NodeUser(node=SchedulerNode(name='buf22'), can_inplace=False, is_weak=False)]
buf19.group.device = cuda:0
buf19.group.iteration = (197, 768)
buf19.sizes = ([197], [768])
buf18_layout = FixedLayout('cuda', torch.float32, size=[1, 197, 768], stride=[151296, 768, 1])
buf19_layout = FixedLayout('cuda', torch.float32, size=[1, 197, 1], stride=[197, 1, 197])
class buf19_loop_body:
    var_ranges = {z0: 197, z1: 768}
    index0 = 768*z0 + z1
    index1 = z0
    def body(self, ops):
        get_index = self.get_index('index0')
        load = ops.load('buf18', get_index)
        reduction = ops.reduction(torch.float32, torch.float32, 'welford_reduce', load)
        getitem = reduction[0]
        getitem_1 = reduction[1]
        getitem_2 = reduction[2]
        get_index_1 = self.get_index('index1')
        store_reduction = ops.store_reduction('buf19', get_index_1, getitem)
        return store_reduction
buf19 Triton code:
    import triton
    import triton.language as tl
    from triton.compiler.compiler import AttrsDescriptor

    from torch._inductor.runtime import triton_helpers, triton_heuristics
    from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
    from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, instance_descriptor, DeviceProperties

    @triton_heuristics.persistent_reduction(
        size_hints=[256, 1024],
        reduction_hint=ReductionHint.INNER,
        filename=__file__,
        triton_meta={'signature': {0: '*fp32', 1: '*fp32', 2: 'i32', 3: 'i32'}, 'device': DeviceProperties(type='cuda', index=0, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, multi_processor_count=132), 'constants': {}, 'configs': [AttrsDescriptor(divisible_by_16=(0, 1, 3), equal_to_1=())]},
        inductor_meta={'autotune_hints': set(), 'kernel_name': 'Placeholder.DESCRIPTIVE_NAME', 'mutated_arg_names': [], 'no_x_dim': True, 'num_load': 1, 'num_reduction': 3, 'backend_hash': '3944FAAD9D18E987535CF029C4441E186640DD453AE5AE6831EDF736C09E0A47', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': False, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False}
    )
    @triton.jit
    def triton_(in_ptr0, out_ptr0, xnumel, rnumel):
        xnumel = 197
        XBLOCK: tl.constexpr = 1
        rnumel = 768
        RBLOCK: tl.constexpr = 1024
        xoffset = tl.program_id(0) * XBLOCK
        xindex = tl.full([1], xoffset, tl.int32)
        xmask = xindex < xnumel
        rindex = tl.arange(0, RBLOCK)[:]
        roffset = 0
        rmask = rindex < rnumel
        r1 = rindex
        x0 = xindex
        tmp0 = tl.load(in_ptr0 + (r1 + (768*x0)), rmask & xmask, other=0.0)
        tmp1 = tl.broadcast_to(tmp0, [RBLOCK])
        tmp3 = tl.where(rmask & xmask, tmp1, 0)
        tmp4 = tl.broadcast_to(tmp1, [RBLOCK])
        tmp6 = tl.where(rmask & xmask, tmp4, 0)
        tmp7 = triton_helpers.promote_to_tensor(tl.sum(tmp6, 0))
        tmp8 = tl.full([1], 768, tl.int32)
        tmp9 = tmp8.to(tl.float32)
        tmp10 = tmp7 / tmp9
        tmp11 = tmp1 - tmp10
        tmp12 = tmp11 * tmp11
        tmp13 = tl.broadcast_to(tmp12, [RBLOCK])
        tmp15 = tl.where(rmask & xmask, tmp13, 0)
        tmp16 = triton_helpers.promote_to_tensor(tl.sum(tmp15, 0))
        tl.store(out_ptr0 + (x0), tmp10, xmask)


buf20: SchedulerNode(ComputedBuffer)
buf20.writes = [MemoryDep('buf20', c0, {c0: 197}, None)]
buf20.unmet_dependencies = [MemoryDep('buf18', c0, {c0: 151296}, None)]
buf20.met_dependencies = []
buf20.users = [NodeUser(node=SchedulerNode(name='buf22'), can_inplace=False, is_weak=False)]
buf20.group.device = cuda:0
buf20.group.iteration = (197, 768)
buf20.sizes = ([197], [768])
buf18_layout = FixedLayout('cuda', torch.float32, size=[1, 197, 768], stride=[151296, 768, 1])
buf20_layout = FixedLayout('cuda', torch.float32, size=[1, 197, 1], stride=[197, 1, 197])
class buf20_loop_body:
    var_ranges = {z0: 197, z1: 768}
    index0 = 768*z0 + z1
    index1 = z0
    def body(self, ops):
        get_index = self.get_index('index0')
        load = ops.load('buf18', get_index)
        reduction = ops.reduction(torch.float32, torch.float32, 'welford_reduce', load)
        getitem = reduction[0]
        getitem_1 = reduction[1]
        getitem_2 = reduction[2]
        get_index_1 = self.get_index('index1')
        store_reduction = ops.store_reduction('buf20', get_index_1, getitem_1)
        return store_reduction
buf20 Triton code:
    import triton
    import triton.language as tl
    from triton.compiler.compiler import AttrsDescriptor

    from torch._inductor.runtime import triton_helpers, triton_heuristics
    from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
    from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, instance_descriptor, DeviceProperties

    @triton_heuristics.persistent_reduction(
        size_hints=[256, 1024],
        reduction_hint=ReductionHint.INNER,
        filename=__file__,
        triton_meta={'signature': {0: '*fp32', 1: '*fp32', 2: 'i32', 3: 'i32'}, 'device': DeviceProperties(type='cuda', index=0, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, multi_processor_count=132), 'constants': {}, 'configs': [AttrsDescriptor(divisible_by_16=(0, 1, 3), equal_to_1=())]},
        inductor_meta={'autotune_hints': set(), 'kernel_name': 'Placeholder.DESCRIPTIVE_NAME', 'mutated_arg_names': [], 'no_x_dim': True, 'num_load': 1, 'num_reduction': 3, 'backend_hash': '3944FAAD9D18E987535CF029C4441E186640DD453AE5AE6831EDF736C09E0A47', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': False, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False}
    )
    @triton.jit
    def triton_(in_ptr0, out_ptr0, xnumel, rnumel):
        xnumel = 197
        XBLOCK: tl.constexpr = 1
        rnumel = 768
        RBLOCK: tl.constexpr = 1024
        xoffset = tl.program_id(0) * XBLOCK
        xindex = tl.full([1], xoffset, tl.int32)
        xmask = xindex < xnumel
        rindex = tl.arange(0, RBLOCK)[:]
        roffset = 0
        rmask = rindex < rnumel
        r1 = rindex
        x0 = xindex
        tmp0 = tl.load(in_ptr0 + (r1 + (768*x0)), rmask & xmask, other=0.0)
        tmp1 = tl.broadcast_to(tmp0, [RBLOCK])
        tmp3 = tl.where(rmask & xmask, tmp1, 0)
        tmp4 = tl.broadcast_to(tmp1, [RBLOCK])
        tmp6 = tl.where(rmask & xmask, tmp4, 0)
        tmp7 = triton_helpers.promote_to_tensor(tl.sum(tmp6, 0))
        tmp8 = tl.full([1], 768, tl.int32)
        tmp9 = tmp8.to(tl.float32)
        tmp10 = tmp7 / tmp9
        tmp11 = tmp1 - tmp10
        tmp12 = tmp11 * tmp11
        tmp13 = tl.broadcast_to(tmp12, [RBLOCK])
        tmp15 = tl.where(rmask & xmask, tmp13, 0)
        tmp16 = triton_helpers.promote_to_tensor(tl.sum(tmp15, 0))
        tl.store(out_ptr0 + (x0), tmp16, xmask)


buf22: SchedulerNode(ComputedBuffer)
buf22.writes = [MemoryDep('buf22', c0, {c0: 151296}, None)]
buf22.unmet_dependencies = 
    [   MemoryDep('buf18', c0, {c0: 151296}, None),
        MemoryDep('buf19', c0, {c0: 197}, None),
        MemoryDep('buf20', c0, {c0: 197}, None)]
buf22.met_dependencies = 
    [   MemoryDep('arg14_1', c1, {c0: 197, c1: 768}, None),
        MemoryDep('arg15_1', c1, {c0: 197, c1: 768}, None)]
buf22.users = [NodeUser(node=ExternKernelSchedulerNode(name='buf23'), can_inplace=False, is_weak=False)]
buf22.group.device = cuda:0
buf22.group.iteration = (151296, 1)
buf22.sizes = ([197, 768], [])
arg14_1_layout = FixedLayout('cuda', torch.float32, size=[768], stride=[1])
buf19_layout = FixedLayout('cuda', torch.float32, size=[1, 197, 1], stride=[197, 1, 197])
arg15_1_layout = FixedLayout('cuda', torch.float32, size=[768], stride=[1])
buf18_layout = FixedLayout('cuda', torch.float32, size=[1, 197, 768], stride=[151296, 768, 1])
buf20_layout = FixedLayout('cuda', torch.float32, size=[1, 197, 1], stride=[197, 1, 197])
buf22_layout = FixedLayout('cuda', torch.float32, size=[1, 197, 768], stride=[151296, 768, 1])
class buf22_loop_body:
    var_ranges = {z0: 197, z1: 768}
    index0 = 768*z0 + z1
    index1 = z0
    index2 = z1
    def body(self, ops):
        get_index = self.get_index('index0')
        load = ops.load('buf18', get_index)
        get_index_1 = self.get_index('index1')
        load_1 = ops.load('buf19', get_index_1)
        sub = ops.sub(load, load_1)
        get_index_2 = self.get_index('index1')
        load_2 = ops.load('buf20', get_index_2)
        constant = ops.constant(768.0, torch.float32)
        truediv = ops.truediv(load_2, constant)
        constant_1 = ops.constant(1e-12, torch.float32)
        add = ops.add(truediv, constant_1)
        rsqrt = ops.rsqrt(add)
        mul = ops.mul(sub, rsqrt)
        get_index_3 = self.get_index('index2')
        load_3 = ops.load('arg14_1', get_index_3)
        mul_1 = ops.mul(mul, load_3)
        get_index_4 = self.get_index('index2')
        load_4 = ops.load('arg15_1', get_index_4)
        add_1 = ops.add(mul_1, load_4)
        get_index_5 = self.get_index('index0')
        store = ops.store('buf22', get_index_5, add_1, None)
        return store
buf22 Triton code:
    import triton
    import triton.language as tl
    from triton.compiler.compiler import AttrsDescriptor

    from torch._inductor.runtime import triton_helpers, triton_heuristics
    from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
    from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, instance_descriptor, DeviceProperties

    @triton_heuristics.pointwise(
        size_hints=[262144], 
        filename=__file__,
        triton_meta={'signature': {0: '*fp32', 1: '*fp32', 2: '*fp32', 3: '*fp32', 4: '*fp32', 5: '*fp32', 6: 'i32'}, 'device': DeviceProperties(type='cuda', index=0, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, multi_processor_count=132), 'constants': {}, 'configs': [AttrsDescriptor(divisible_by_16=(0, 1, 2, 3, 4, 5, 6), equal_to_1=())]},
        inductor_meta={'autotune_hints': set(), 'kernel_name': 'Placeholder.DESCRIPTIVE_NAME', 'mutated_arg_names': [], 'no_x_dim': False, 'num_load': 5, 'num_reduction': 0, 'backend_hash': '3944FAAD9D18E987535CF029C4441E186640DD453AE5AE6831EDF736C09E0A47', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': False, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False},
        min_elem_per_thread=0
    )
    @triton.jit
    def triton_(in_ptr0, in_ptr1, in_ptr2, in_ptr3, in_ptr4, out_ptr0, xnumel, XBLOCK : tl.constexpr):
        xnumel = 151296
        xoffset = tl.program_id(0) * XBLOCK
        xindex = xoffset + tl.arange(0, XBLOCK)[:]
        xmask = xindex < xnumel
        x2 = xindex
        x1 = (xindex // 768)
        x0 = xindex % 768
        tmp0 = tl.load(in_ptr0 + (x2), xmask)
        tmp1 = tl.load(in_ptr1 + (x1), xmask, eviction_policy='evict_last')
        tmp3 = tl.load(in_ptr2 + (x1), xmask, eviction_policy='evict_last')
        tmp10 = tl.load(in_ptr3 + (x0), xmask, eviction_policy='evict_last')
        tmp12 = tl.load(in_ptr4 + (x0), xmask, eviction_policy='evict_last')
        tmp2 = tmp0 - tmp1
        tmp4 = 768.0
        tmp5 = tmp3 / tmp4
        tmp6 = 1e-12
        tmp7 = tmp5 + tmp6
        tmp8 = libdevice.rsqrt(tmp7)
        tmp9 = tmp2 * tmp8
        tmp11 = tmp9 * tmp10
        tmp13 = tmp11 + tmp12
        tl.store(out_ptr0 + (x2), tmp13, xmask)


buf23: ExternKernelSchedulerNode(ExternKernelOut)
buf23.writes = [StarDep(name='buf23', mode=None)]
buf23.unmet_dependencies = [StarDep(name='buf22', mode=None)]
buf23.met_dependencies = [StarDep(name='arg16_1', mode=None)]
buf23.users = [NodeUser(node=SchedulerNode(name='buf24'), can_inplace=True, is_weak=False)]
buf23.node.kernel = extern_kernels.mm


buf24: SchedulerNode(ComputedBuffer)
buf24.writes = [MemoryDep('buf24', c0, {c0: 605184}, None)]
buf24.unmet_dependencies = [MemoryDep('buf23', c0, {c0: 605184}, None)]
buf24.met_dependencies = [MemoryDep('arg17_1', c1, {c0: 197, c1: 3072}, None)]
buf24.users = [NodeUser(node=ExternKernelSchedulerNode(name='buf25'), can_inplace=False, is_weak=False)]
buf24.group.device = cuda:0
buf24.group.iteration = (605184, 1)
buf24.sizes = ([197, 3072], [])
buf23_layout = FixedLayout('cuda', torch.float32, size=[197, 3072], stride=[3072, 1])
arg17_1_layout = FixedLayout('cuda', torch.float32, size=[3072], stride=[1])
buf24_layout = FixedLayout('cuda', torch.float32, size=[1, 197, 3072], stride=[605184, 3072, 1])
class buf24_loop_body:
    var_ranges = {z0: 197, z1: 3072}
    index0 = 3072*z0 + z1
    index1 = z1
    def body(self, ops):
        get_index = self.get_index('index0')
        load = ops.load('buf23', get_index)
        get_index_1 = self.get_index('index1')
        load_1 = ops.load('arg17_1', get_index_1)
        add = ops.add(load, load_1)
        constant = ops.constant(0.5, torch.float32)
        mul = ops.mul(add, constant)
        get_index_2 = self.get_index('index0')
        load_2 = ops.load('buf23', get_index_2)
        get_index_3 = self.get_index('index1')
        load_3 = ops.load('arg17_1', get_index_3)
        add_1 = ops.add(load_2, load_3)
        constant_1 = ops.constant(0.7071067811865476, torch.float32)
        mul_1 = ops.mul(add_1, constant_1)
        erf = ops.erf(mul_1)
        constant_2 = ops.constant(1.0, torch.float32)
        add_2 = ops.add(erf, constant_2)
        mul_2 = ops.mul(mul, add_2)
        get_index_4 = self.get_index('index0')
        store = ops.store('buf24', get_index_4, mul_2, None)
        return store
buf24 Triton code:
    import triton
    import triton.language as tl
    from triton.compiler.compiler import AttrsDescriptor

    from torch._inductor.runtime import triton_helpers, triton_heuristics
    from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
    from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, instance_descriptor, DeviceProperties

    @triton_heuristics.pointwise(
        size_hints=[1048576], 
        filename=__file__,
        triton_meta={'signature': {0: '*fp32', 1: '*fp32', 2: 'i32'}, 'device': DeviceProperties(type='cuda', index=0, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, multi_processor_count=132), 'constants': {}, 'configs': [AttrsDescriptor(divisible_by_16=(0, 1, 2), equal_to_1=())]},
        inductor_meta={'autotune_hints': set(), 'kernel_name': 'Placeholder.DESCRIPTIVE_NAME', 'mutated_arg_names': ['in_out_ptr0'], 'no_x_dim': False, 'num_load': 2, 'num_reduction': 0, 'backend_hash': '3944FAAD9D18E987535CF029C4441E186640DD453AE5AE6831EDF736C09E0A47', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': False, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False},
        min_elem_per_thread=0
    )
    @triton.jit
    def triton_(in_out_ptr0, in_ptr0, xnumel, XBLOCK : tl.constexpr):
        xnumel = 605184
        xoffset = tl.program_id(0) * XBLOCK
        xindex = xoffset + tl.arange(0, XBLOCK)[:]
        xmask = xindex < xnumel
        x2 = xindex
        x0 = xindex % 3072
        tmp0 = tl.load(in_out_ptr0 + (x2), xmask)
        tmp1 = tl.load(in_ptr0 + (x0), xmask, eviction_policy='evict_last')
        tmp2 = tmp0 + tmp1
        tmp3 = 0.5
        tmp4 = tmp2 * tmp3
        tmp5 = 0.7071067811865476
        tmp6 = tmp2 * tmp5
        tmp7 = libdevice.erf(tmp6)
        tmp8 = 1.0
        tmp9 = tmp7 + tmp8
        tmp10 = tmp4 * tmp9
        tl.store(in_out_ptr0 + (x2), tmp10, xmask)


buf25: ExternKernelSchedulerNode(ExternKernelOut)
buf25.writes = [StarDep(name='buf25', mode=None)]
buf25.unmet_dependencies = [StarDep(name='buf24', mode=None)]
buf25.met_dependencies = [StarDep(name='arg18_1', mode=None)]
buf25.users = [NodeUser(node=SchedulerNode(name='buf26'), can_inplace=False, is_weak=False), NodeUser(node=SchedulerNode(name='buf27'), can_inplace=False, is_weak=False), NodeUser(node=SchedulerNode(name='buf28'), can_inplace=False, is_weak=False), NodeUser(node=SchedulerNode(name='buf29'), can_inplace=True, is_weak=False), NodeUser(node=SchedulerNode(name='buf39'), can_inplace=True, is_weak=False)]
buf25.node.kernel = extern_kernels.mm


buf26: SchedulerNode(ComputedBuffer)
buf26.writes = [MemoryDep('buf26', c0, {c0: 197}, None)]
buf26.unmet_dependencies = 
    [   MemoryDep('buf18', c0, {c0: 151296}, None),
        MemoryDep('buf25', c0, {c0: 151296}, None)]
buf26.met_dependencies = [MemoryDep('arg19_1', c1, {c0: 197, c1: 768}, None)]
buf26.users = [NodeUser(node=SchedulerNode(name='buf29'), can_inplace=False, is_weak=False)]
buf26.group.device = cuda:0
buf26.group.iteration = (197, 768)
buf26.sizes = ([197], [768])
arg19_1_layout = FixedLayout('cuda', torch.float32, size=[768], stride=[1])
buf25_layout = FixedLayout('cuda', torch.float32, size=[197, 768], stride=[768, 1])
buf18_layout = FixedLayout('cuda', torch.float32, size=[1, 197, 768], stride=[151296, 768, 1])
buf26_layout = FixedLayout('cuda', torch.float32, size=[1, 197, 1], stride=[197, 1, 197])
class buf26_loop_body:
    var_ranges = {z0: 197, z1: 768}
    index0 = 768*z0 + z1
    index1 = z1
    index2 = z0
    def body(self, ops):
        get_index = self.get_index('index0')
        load = ops.load('buf25', get_index)
        get_index_1 = self.get_index('index1')
        load_1 = ops.load('arg19_1', get_index_1)
        add = ops.add(load, load_1)
        get_index_2 = self.get_index('index0')
        load_2 = ops.load('buf18', get_index_2)
        add_1 = ops.add(add, load_2)
        reduction = ops.reduction(torch.float32, torch.float32, 'welford_reduce', add_1)
        getitem = reduction[0]
        getitem_1 = reduction[1]
        getitem_2 = reduction[2]
        get_index_3 = self.get_index('index2')
        store_reduction = ops.store_reduction('buf26', get_index_3, getitem)
        return store_reduction
buf26 Triton code:
    import triton
    import triton.language as tl
    from triton.compiler.compiler import AttrsDescriptor

    from torch._inductor.runtime import triton_helpers, triton_heuristics
    from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
    from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, instance_descriptor, DeviceProperties

    @triton_heuristics.persistent_reduction(
        size_hints=[256, 1024],
        reduction_hint=ReductionHint.INNER,
        filename=__file__,
        triton_meta={'signature': {0: '*fp32', 1: '*fp32', 2: '*fp32', 3: '*fp32', 4: 'i32', 5: 'i32'}, 'device': DeviceProperties(type='cuda', index=0, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, multi_processor_count=132), 'constants': {}, 'configs': [AttrsDescriptor(divisible_by_16=(0, 1, 2, 3, 5), equal_to_1=())]},
        inductor_meta={'autotune_hints': set(), 'kernel_name': 'Placeholder.DESCRIPTIVE_NAME', 'mutated_arg_names': [], 'no_x_dim': True, 'num_load': 3, 'num_reduction': 3, 'backend_hash': '3944FAAD9D18E987535CF029C4441E186640DD453AE5AE6831EDF736C09E0A47', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': False, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False}
    )
    @triton.jit
    def triton_(in_ptr0, in_ptr1, in_ptr2, out_ptr0, xnumel, rnumel):
        xnumel = 197
        XBLOCK: tl.constexpr = 1
        rnumel = 768
        RBLOCK: tl.constexpr = 1024
        xoffset = tl.program_id(0) * XBLOCK
        xindex = tl.full([1], xoffset, tl.int32)
        xmask = xindex < xnumel
        rindex = tl.arange(0, RBLOCK)[:]
        roffset = 0
        rmask = rindex < rnumel
        r1 = rindex
        x0 = xindex
        tmp0 = tl.load(in_ptr0 + (r1 + (768*x0)), rmask & xmask, other=0.0)
        tmp1 = tl.load(in_ptr1 + (r1), rmask, eviction_policy='evict_last', other=0.0)
        tmp3 = tl.load(in_ptr2 + (r1 + (768*x0)), rmask & xmask, other=0.0)
        tmp2 = tmp0 + tmp1
        tmp4 = tmp2 + tmp3
        tmp5 = tl.broadcast_to(tmp4, [RBLOCK])
        tmp7 = tl.where(rmask & xmask, tmp5, 0)
        tmp8 = tl.broadcast_to(tmp5, [RBLOCK])
        tmp10 = tl.where(rmask & xmask, tmp8, 0)
        tmp11 = triton_helpers.promote_to_tensor(tl.sum(tmp10, 0))
        tmp12 = tl.full([1], 768, tl.int32)
        tmp13 = tmp12.to(tl.float32)
        tmp14 = tmp11 / tmp13
        tmp15 = tmp5 - tmp14
        tmp16 = tmp15 * tmp15
        tmp17 = tl.broadcast_to(tmp16, [RBLOCK])
        tmp19 = tl.where(rmask & xmask, tmp17, 0)
        tmp20 = triton_helpers.promote_to_tensor(tl.sum(tmp19, 0))
        tl.store(out_ptr0 + (x0), tmp14, xmask)


buf27: SchedulerNode(ComputedBuffer)
buf27.writes = [MemoryDep('buf27', c0, {c0: 197}, None)]
buf27.unmet_dependencies = 
    [   MemoryDep('buf18', c0, {c0: 151296}, None),
        MemoryDep('buf25', c0, {c0: 151296}, None)]
buf27.met_dependencies = [MemoryDep('arg19_1', c1, {c0: 197, c1: 768}, None)]
buf27.users = [NodeUser(node=SchedulerNode(name='buf29'), can_inplace=False, is_weak=False)]
buf27.group.device = cuda:0
buf27.group.iteration = (197, 768)
buf27.sizes = ([197], [768])
arg19_1_layout = FixedLayout('cuda', torch.float32, size=[768], stride=[1])
buf25_layout = FixedLayout('cuda', torch.float32, size=[197, 768], stride=[768, 1])
buf18_layout = FixedLayout('cuda', torch.float32, size=[1, 197, 768], stride=[151296, 768, 1])
buf27_layout = FixedLayout('cuda', torch.float32, size=[1, 197, 1], stride=[197, 1, 197])
class buf27_loop_body:
    var_ranges = {z0: 197, z1: 768}
    index0 = 768*z0 + z1
    index1 = z1
    index2 = z0
    def body(self, ops):
        get_index = self.get_index('index0')
        load = ops.load('buf25', get_index)
        get_index_1 = self.get_index('index1')
        load_1 = ops.load('arg19_1', get_index_1)
        add = ops.add(load, load_1)
        get_index_2 = self.get_index('index0')
        load_2 = ops.load('buf18', get_index_2)
        add_1 = ops.add(add, load_2)
        reduction = ops.reduction(torch.float32, torch.float32, 'welford_reduce', add_1)
        getitem = reduction[0]
        getitem_1 = reduction[1]
        getitem_2 = reduction[2]
        get_index_3 = self.get_index('index2')
        store_reduction = ops.store_reduction('buf27', get_index_3, getitem_1)
        return store_reduction
buf27 Triton code:
    import triton
    import triton.language as tl
    from triton.compiler.compiler import AttrsDescriptor

    from torch._inductor.runtime import triton_helpers, triton_heuristics
    from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
    from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, instance_descriptor, DeviceProperties

    @triton_heuristics.persistent_reduction(
        size_hints=[256, 1024],
        reduction_hint=ReductionHint.INNER,
        filename=__file__,
        triton_meta={'signature': {0: '*fp32', 1: '*fp32', 2: '*fp32', 3: '*fp32', 4: 'i32', 5: 'i32'}, 'device': DeviceProperties(type='cuda', index=0, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, multi_processor_count=132), 'constants': {}, 'configs': [AttrsDescriptor(divisible_by_16=(0, 1, 2, 3, 5), equal_to_1=())]},
        inductor_meta={'autotune_hints': set(), 'kernel_name': 'Placeholder.DESCRIPTIVE_NAME', 'mutated_arg_names': [], 'no_x_dim': True, 'num_load': 3, 'num_reduction': 3, 'backend_hash': '3944FAAD9D18E987535CF029C4441E186640DD453AE5AE6831EDF736C09E0A47', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': False, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False}
    )
    @triton.jit
    def triton_(in_ptr0, in_ptr1, in_ptr2, out_ptr0, xnumel, rnumel):
        xnumel = 197
        XBLOCK: tl.constexpr = 1
        rnumel = 768
        RBLOCK: tl.constexpr = 1024
        xoffset = tl.program_id(0) * XBLOCK
        xindex = tl.full([1], xoffset, tl.int32)
        xmask = xindex < xnumel
        rindex = tl.arange(0, RBLOCK)[:]
        roffset = 0
        rmask = rindex < rnumel
        r1 = rindex
        x0 = xindex
        tmp0 = tl.load(in_ptr0 + (r1 + (768*x0)), rmask & xmask, other=0.0)
        tmp1 = tl.load(in_ptr1 + (r1), rmask, eviction_policy='evict_last', other=0.0)
        tmp3 = tl.load(in_ptr2 + (r1 + (768*x0)), rmask & xmask, other=0.0)
        tmp2 = tmp0 + tmp1
        tmp4 = tmp2 + tmp3
        tmp5 = tl.broadcast_to(tmp4, [RBLOCK])
        tmp7 = tl.where(rmask & xmask, tmp5, 0)
        tmp8 = tl.broadcast_to(tmp5, [RBLOCK])
        tmp10 = tl.where(rmask & xmask, tmp8, 0)
        tmp11 = triton_helpers.promote_to_tensor(tl.sum(tmp10, 0))
        tmp12 = tl.full([1], 768, tl.int32)
        tmp13 = tmp12.to(tl.float32)
        tmp14 = tmp11 / tmp13
        tmp15 = tmp5 - tmp14
        tmp16 = tmp15 * tmp15
        tmp17 = tl.broadcast_to(tmp16, [RBLOCK])
        tmp19 = tl.where(rmask & xmask, tmp17, 0)
        tmp20 = triton_helpers.promote_to_tensor(tl.sum(tmp19, 0))
        tl.store(out_ptr0 + (x0), tmp20, xmask)


buf29: SchedulerNode(ComputedBuffer)
buf29.writes = [MemoryDep('buf29', c0, {c0: 151296}, None)]
buf29.unmet_dependencies = 
    [   MemoryDep('buf18', c0, {c0: 151296}, None),
        MemoryDep('buf25', c0, {c0: 151296}, None),
        MemoryDep('buf26', c0, {c0: 197}, None),
        MemoryDep('buf27', c0, {c0: 197}, None)]
buf29.met_dependencies = 
    [   MemoryDep('arg19_1', c1, {c0: 197, c1: 768}, None),
        MemoryDep('arg20_1', c1, {c0: 197, c1: 768}, None),
        MemoryDep('arg21_1', c1, {c0: 197, c1: 768}, None)]
buf29.users = [NodeUser(node=ExternKernelSchedulerNode(name='buf30'), can_inplace=False, is_weak=False), NodeUser(node=ExternKernelSchedulerNode(name='buf31'), can_inplace=False, is_weak=False), NodeUser(node=ExternKernelSchedulerNode(name='buf32'), can_inplace=False, is_weak=False)]
buf29.group.device = cuda:0
buf29.group.iteration = (151296, 1)
buf29.sizes = ([197, 768], [])
buf25_layout = FixedLayout('cuda', torch.float32, size=[197, 768], stride=[768, 1])
buf27_layout = FixedLayout('cuda', torch.float32, size=[1, 197, 1], stride=[197, 1, 197])
buf26_layout = FixedLayout('cuda', torch.float32, size=[1, 197, 1], stride=[197, 1, 197])
arg21_1_layout = FixedLayout('cuda', torch.float32, size=[768], stride=[1])
arg19_1_layout = FixedLayout('cuda', torch.float32, size=[768], stride=[1])
buf18_layout = FixedLayout('cuda', torch.float32, size=[1, 197, 768], stride=[151296, 768, 1])
arg20_1_layout = FixedLayout('cuda', torch.float32, size=[768], stride=[1])
buf29_layout = FixedLayout('cuda', torch.float32, size=[1, 197, 768], stride=[151296, 768, 1])
class buf29_loop_body:
    var_ranges = {z0: 197, z1: 768}
    index0 = 768*z0 + z1
    index1 = z1
    index2 = z0
    def body(self, ops):
        get_index = self.get_index('index0')
        load = ops.load('buf25', get_index)
        get_index_1 = self.get_index('index1')
        load_1 = ops.load('arg19_1', get_index_1)
        add = ops.add(load, load_1)
        get_index_2 = self.get_index('index0')
        load_2 = ops.load('buf18', get_index_2)
        add_1 = ops.add(add, load_2)
        get_index_3 = self.get_index('index2')
        load_3 = ops.load('buf26', get_index_3)
        sub = ops.sub(add_1, load_3)
        get_index_4 = self.get_index('index2')
        load_4 = ops.load('buf27', get_index_4)
        constant = ops.constant(768.0, torch.float32)
        truediv = ops.truediv(load_4, constant)
        constant_1 = ops.constant(1e-12, torch.float32)
        add_2 = ops.add(truediv, constant_1)
        rsqrt = ops.rsqrt(add_2)
        mul = ops.mul(sub, rsqrt)
        get_index_5 = self.get_index('index1')
        load_5 = ops.load('arg20_1', get_index_5)
        mul_1 = ops.mul(mul, load_5)
        get_index_6 = self.get_index('index1')
        load_6 = ops.load('arg21_1', get_index_6)
        add_3 = ops.add(mul_1, load_6)
        get_index_7 = self.get_index('index0')
        store = ops.store('buf29', get_index_7, add_3, None)
        return store
buf29 Triton code:
    import triton
    import triton.language as tl
    from triton.compiler.compiler import AttrsDescriptor

    from torch._inductor.runtime import triton_helpers, triton_heuristics
    from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
    from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, instance_descriptor, DeviceProperties

    @triton_heuristics.pointwise(
        size_hints=[262144], 
        filename=__file__,
        triton_meta={'signature': {0: '*fp32', 1: '*fp32', 2: '*fp32', 3: '*fp32', 4: '*fp32', 5: '*fp32', 6: '*fp32', 7: '*fp32', 8: 'i32'}, 'device': DeviceProperties(type='cuda', index=0, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, multi_processor_count=132), 'constants': {}, 'configs': [AttrsDescriptor(divisible_by_16=(0, 1, 2, 3, 4, 5, 6, 7, 8), equal_to_1=())]},
        inductor_meta={'autotune_hints': set(), 'kernel_name': 'Placeholder.DESCRIPTIVE_NAME', 'mutated_arg_names': [], 'no_x_dim': False, 'num_load': 7, 'num_reduction': 0, 'backend_hash': '3944FAAD9D18E987535CF029C4441E186640DD453AE5AE6831EDF736C09E0A47', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': False, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False},
        min_elem_per_thread=0
    )
    @triton.jit
    def triton_(in_ptr0, in_ptr1, in_ptr2, in_ptr3, in_ptr4, in_ptr5, in_ptr6, out_ptr0, xnumel, XBLOCK : tl.constexpr):
        xnumel = 151296
        xoffset = tl.program_id(0) * XBLOCK
        xindex = xoffset + tl.arange(0, XBLOCK)[:]
        xmask = xindex < xnumel
        x2 = xindex
        x0 = xindex % 768
        x1 = (xindex // 768)
        tmp0 = tl.load(in_ptr0 + (x2), xmask)
        tmp1 = tl.load(in_ptr1 + (x0), xmask, eviction_policy='evict_last')
        tmp3 = tl.load(in_ptr2 + (x2), xmask)
        tmp5 = tl.load(in_ptr3 + (x1), xmask, eviction_policy='evict_last')
        tmp7 = tl.load(in_ptr4 + (x1), xmask, eviction_policy='evict_last')
        tmp14 = tl.load(in_ptr5 + (x0), xmask, eviction_policy='evict_last')
        tmp16 = tl.load(in_ptr6 + (x0), xmask, eviction_policy='evict_last')
        tmp2 = tmp0 + tmp1
        tmp4 = tmp2 + tmp3
        tmp6 = tmp4 - tmp5
        tmp8 = 768.0
        tmp9 = tmp7 / tmp8
        tmp10 = 1e-12
        tmp11 = tmp9 + tmp10
        tmp12 = libdevice.rsqrt(tmp11)
        tmp13 = tmp6 * tmp12
        tmp15 = tmp13 * tmp14
        tmp17 = tmp15 + tmp16
        tl.store(out_ptr0 + (x2), tmp17, xmask)


buf30: ExternKernelSchedulerNode(ExternKernelOut)
buf30.writes = [StarDep(name='buf30', mode=None)]
buf30.unmet_dependencies = [StarDep(name='buf29', mode=None)]
buf30.met_dependencies = [StarDep(name='arg22_1', mode=None), StarDep(name='arg23_1', mode=None)]
buf30.users = [NodeUser(node=ExternKernelSchedulerNode(name='buf33'), can_inplace=False, is_weak=False)]
buf30.node.kernel = extern_kernels.addmm


buf31: ExternKernelSchedulerNode(ExternKernelOut)
buf31.writes = [StarDep(name='buf31', mode=None)]
buf31.unmet_dependencies = [StarDep(name='buf29', mode=None)]
buf31.met_dependencies = [StarDep(name='arg24_1', mode=None), StarDep(name='arg25_1', mode=None)]
buf31.users = [NodeUser(node=ExternKernelSchedulerNode(name='buf33'), can_inplace=False, is_weak=False)]
buf31.node.kernel = extern_kernels.addmm


buf32: ExternKernelSchedulerNode(ExternKernelOut)
buf32.writes = [StarDep(name='buf32', mode=None)]
buf32.unmet_dependencies = [StarDep(name='buf29', mode=None)]
buf32.met_dependencies = [StarDep(name='arg26_1', mode=None), StarDep(name='arg27_1', mode=None)]
buf32.users = [NodeUser(node=ExternKernelSchedulerNode(name='buf33'), can_inplace=False, is_weak=False)]
buf32.node.kernel = extern_kernels.addmm


buf33: ExternKernelSchedulerNode(FallbackKernel)
buf33.writes = [StarDep(name='buf33', mode=None)]
buf33.unmet_dependencies = 
    [   StarDep(name='buf30', mode=None),
        StarDep(name='buf31', mode=None),
        StarDep(name='buf32', mode=None)]
buf33.met_dependencies = []
buf33.users = [NodeUser(node=ExternKernelSchedulerNode(name='buf34'), can_inplace=False, is_weak=False), NodeUser(node=ExternKernelSchedulerNode(name='buf35'), can_inplace=False, is_weak=False), NodeUser(node=ExternKernelSchedulerNode(name='buf36'), can_inplace=False, is_weak=False), NodeUser(node=ExternKernelSchedulerNode(name='buf37'), can_inplace=False, is_weak=False)]
buf33.node.kernel = None


buf34: ExternKernelSchedulerNode(MultiOutput)
buf34.writes = [StarDep(name='buf34', mode=None)]
buf34.unmet_dependencies = [StarDep(name='buf33', mode=None)]
buf34.met_dependencies = []
buf34.users = [NodeUser(node=ExternKernelSchedulerNode(name='buf38'), can_inplace=False, is_weak=False)]
buf34.node.kernel = None


buf38: ExternKernelSchedulerNode(ExternKernelOut)
buf38.writes = [StarDep(name='buf38', mode=None)]
buf38.unmet_dependencies = [StarDep(name='buf34', mode=None)]
buf38.met_dependencies = [StarDep(name='arg28_1', mode=None)]
buf38.users = [NodeUser(node=SchedulerNode(name='buf39'), can_inplace=True, is_weak=False)]
buf38.node.kernel = extern_kernels.mm


buf39: SchedulerNode(ComputedBuffer)
buf39.writes = [MemoryDep('buf39', c0, {c0: 151296}, None)]
buf39.unmet_dependencies = 
    [   MemoryDep('buf18', c0, {c0: 151296}, None),
        MemoryDep('buf25', c0, {c0: 151296}, None),
        MemoryDep('buf38', c0, {c0: 151296}, None)]
buf39.met_dependencies = 
    [   MemoryDep('arg19_1', c1, {c0: 197, c1: 768}, None),
        MemoryDep('arg29_1', c1, {c0: 197, c1: 768}, None)]
buf39.users = [NodeUser(node=SchedulerNode(name='buf40'), can_inplace=False, is_weak=False), NodeUser(node=SchedulerNode(name='buf41'), can_inplace=False, is_weak=False), NodeUser(node=SchedulerNode(name='buf42'), can_inplace=False, is_weak=False), NodeUser(node=SchedulerNode(name='buf43'), can_inplace=True, is_weak=False), NodeUser(node=SchedulerNode(name='buf47'), can_inplace=False, is_weak=False), NodeUser(node=SchedulerNode(name='buf48'), can_inplace=False, is_weak=False), NodeUser(node=SchedulerNode(name='buf49'), can_inplace=False, is_weak=False), NodeUser(node=SchedulerNode(name='buf50'), can_inplace=True, is_weak=False), NodeUser(node=SchedulerNode(name='buf60'), can_inplace=True, is_weak=False)]
buf39.group.device = cuda:0
buf39.group.iteration = (151296, 1)
buf39.sizes = ([197, 768], [])
arg29_1_layout = FixedLayout('cuda', torch.float32, size=[768], stride=[1])
buf38_layout = FixedLayout('cuda', torch.float32, size=[197, 768], stride=[768, 1])
buf25_layout = FixedLayout('cuda', torch.float32, size=[197, 768], stride=[768, 1])
arg19_1_layout = FixedLayout('cuda', torch.float32, size=[768], stride=[1])
buf18_layout = FixedLayout('cuda', torch.float32, size=[1, 197, 768], stride=[151296, 768, 1])
buf39_layout = FixedLayout('cuda', torch.float32, size=[1, 197, 768], stride=[151296, 768, 1])
class buf39_loop_body:
    var_ranges = {z0: 197, z1: 768}
    index0 = 768*z0 + z1
    index1 = z1
    def body(self, ops):
        get_index = self.get_index('index0')
        load = ops.load('buf38', get_index)
        get_index_1 = self.get_index('index1')
        load_1 = ops.load('arg29_1', get_index_1)
        add = ops.add(load, load_1)
        get_index_2 = self.get_index('index0')
        load_2 = ops.load('buf25', get_index_2)
        get_index_3 = self.get_index('index1')
        load_3 = ops.load('arg19_1', get_index_3)
        add_1 = ops.add(load_2, load_3)
        get_index_4 = self.get_index('index0')
        load_4 = ops.load('buf18', get_index_4)
        add_2 = ops.add(add_1, load_4)
        add_3 = ops.add(add, add_2)
        get_index_5 = self.get_index('index0')
        store = ops.store('buf39', get_index_5, add_3, None)
        return store
buf39 Triton code:
    import triton
    import triton.language as tl
    from triton.compiler.compiler import AttrsDescriptor

    from torch._inductor.runtime import triton_helpers, triton_heuristics
    from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
    from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, instance_descriptor, DeviceProperties

    @triton_heuristics.pointwise(
        size_hints=[262144], 
        filename=__file__,
        triton_meta={'signature': {0: '*fp32', 1: '*fp32', 2: '*fp32', 3: '*fp32', 4: '*fp32', 5: 'i32'}, 'device': DeviceProperties(type='cuda', index=0, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, multi_processor_count=132), 'constants': {}, 'configs': [AttrsDescriptor(divisible_by_16=(0, 1, 2, 3, 4, 5), equal_to_1=())]},
        inductor_meta={'autotune_hints': set(), 'kernel_name': 'Placeholder.DESCRIPTIVE_NAME', 'mutated_arg_names': ['in_out_ptr0'], 'no_x_dim': False, 'num_load': 5, 'num_reduction': 0, 'backend_hash': '3944FAAD9D18E987535CF029C4441E186640DD453AE5AE6831EDF736C09E0A47', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': False, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False},
        min_elem_per_thread=0
    )
    @triton.jit
    def triton_(in_out_ptr0, in_ptr0, in_ptr1, in_ptr2, in_ptr3, xnumel, XBLOCK : tl.constexpr):
        xnumel = 151296
        xoffset = tl.program_id(0) * XBLOCK
        xindex = xoffset + tl.arange(0, XBLOCK)[:]
        xmask = xindex < xnumel
        x2 = xindex
        x0 = xindex % 768
        tmp0 = tl.load(in_out_ptr0 + (x2), xmask)
        tmp1 = tl.load(in_ptr0 + (x0), xmask, eviction_policy='evict_last')
        tmp3 = tl.load(in_ptr1 + (x2), xmask)
        tmp4 = tl.load(in_ptr2 + (x0), xmask, eviction_policy='evict_last')
        tmp6 = tl.load(in_ptr3 + (x2), xmask)
        tmp2 = tmp0 + tmp1
        tmp5 = tmp3 + tmp4
        tmp7 = tmp5 + tmp6
        tmp8 = tmp2 + tmp7
        tl.store(in_out_ptr0 + (x2), tmp8, xmask)


buf40: SchedulerNode(ComputedBuffer)
buf40.writes = [MemoryDep('buf40', c0, {c0: 197}, None)]
buf40.unmet_dependencies = [MemoryDep('buf39', c0, {c0: 151296}, None)]
buf40.met_dependencies = []
buf40.users = [NodeUser(node=SchedulerNode(name='buf43'), can_inplace=False, is_weak=False)]
buf40.group.device = cuda:0
buf40.group.iteration = (197, 768)
buf40.sizes = ([197], [768])
buf39_layout = FixedLayout('cuda', torch.float32, size=[1, 197, 768], stride=[151296, 768, 1])
buf40_layout = FixedLayout('cuda', torch.float32, size=[1, 197, 1], stride=[197, 1, 197])
class buf40_loop_body:
    var_ranges = {z0: 197, z1: 768}
    index0 = 768*z0 + z1
    index1 = z0
    def body(self, ops):
        get_index = self.get_index('index0')
        load = ops.load('buf39', get_index)
        reduction = ops.reduction(torch.float32, torch.float32, 'welford_reduce', load)
        getitem = reduction[0]
        getitem_1 = reduction[1]
        getitem_2 = reduction[2]
        get_index_1 = self.get_index('index1')
        store_reduction = ops.store_reduction('buf40', get_index_1, getitem)
        return store_reduction
buf40 Triton code:
    import triton
    import triton.language as tl
    from triton.compiler.compiler import AttrsDescriptor

    from torch._inductor.runtime import triton_helpers, triton_heuristics
    from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
    from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, instance_descriptor, DeviceProperties

    @triton_heuristics.persistent_reduction(
        size_hints=[256, 1024],
        reduction_hint=ReductionHint.INNER,
        filename=__file__,
        triton_meta={'signature': {0: '*fp32', 1: '*fp32', 2: 'i32', 3: 'i32'}, 'device': DeviceProperties(type='cuda', index=0, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, multi_processor_count=132), 'constants': {}, 'configs': [AttrsDescriptor(divisible_by_16=(0, 1, 3), equal_to_1=())]},
        inductor_meta={'autotune_hints': set(), 'kernel_name': 'Placeholder.DESCRIPTIVE_NAME', 'mutated_arg_names': [], 'no_x_dim': True, 'num_load': 1, 'num_reduction': 3, 'backend_hash': '3944FAAD9D18E987535CF029C4441E186640DD453AE5AE6831EDF736C09E0A47', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': False, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False}
    )
    @triton.jit
    def triton_(in_ptr0, out_ptr0, xnumel, rnumel):
        xnumel = 197
        XBLOCK: tl.constexpr = 1
        rnumel = 768
        RBLOCK: tl.constexpr = 1024
        xoffset = tl.program_id(0) * XBLOCK
        xindex = tl.full([1], xoffset, tl.int32)
        xmask = xindex < xnumel
        rindex = tl.arange(0, RBLOCK)[:]
        roffset = 0
        rmask = rindex < rnumel
        r1 = rindex
        x0 = xindex
        tmp0 = tl.load(in_ptr0 + (r1 + (768*x0)), rmask & xmask, other=0.0)
        tmp1 = tl.broadcast_to(tmp0, [RBLOCK])
        tmp3 = tl.where(rmask & xmask, tmp1, 0)
        tmp4 = tl.broadcast_to(tmp1, [RBLOCK])
        tmp6 = tl.where(rmask & xmask, tmp4, 0)
        tmp7 = triton_helpers.promote_to_tensor(tl.sum(tmp6, 0))
        tmp8 = tl.full([1], 768, tl.int32)
        tmp9 = tmp8.to(tl.float32)
        tmp10 = tmp7 / tmp9
        tmp11 = tmp1 - tmp10
        tmp12 = tmp11 * tmp11
        tmp13 = tl.broadcast_to(tmp12, [RBLOCK])
        tmp15 = tl.where(rmask & xmask, tmp13, 0)
        tmp16 = triton_helpers.promote_to_tensor(tl.sum(tmp15, 0))
        tl.store(out_ptr0 + (x0), tmp10, xmask)


buf41: SchedulerNode(ComputedBuffer)
buf41.writes = [MemoryDep('buf41', c0, {c0: 197}, None)]
buf41.unmet_dependencies = [MemoryDep('buf39', c0, {c0: 151296}, None)]
buf41.met_dependencies = []
buf41.users = [NodeUser(node=SchedulerNode(name='buf43'), can_inplace=False, is_weak=False)]
buf41.group.device = cuda:0
buf41.group.iteration = (197, 768)
buf41.sizes = ([197], [768])
buf39_layout = FixedLayout('cuda', torch.float32, size=[1, 197, 768], stride=[151296, 768, 1])
buf41_layout = FixedLayout('cuda', torch.float32, size=[1, 197, 1], stride=[197, 1, 197])
class buf41_loop_body:
    var_ranges = {z0: 197, z1: 768}
    index0 = 768*z0 + z1
    index1 = z0
    def body(self, ops):
        get_index = self.get_index('index0')
        load = ops.load('buf39', get_index)
        reduction = ops.reduction(torch.float32, torch.float32, 'welford_reduce', load)
        getitem = reduction[0]
        getitem_1 = reduction[1]
        getitem_2 = reduction[2]
        get_index_1 = self.get_index('index1')
        store_reduction = ops.store_reduction('buf41', get_index_1, getitem_1)
        return store_reduction
buf41 Triton code:
    import triton
    import triton.language as tl
    from triton.compiler.compiler import AttrsDescriptor

    from torch._inductor.runtime import triton_helpers, triton_heuristics
    from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
    from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, instance_descriptor, DeviceProperties

    @triton_heuristics.persistent_reduction(
        size_hints=[256, 1024],
        reduction_hint=ReductionHint.INNER,
        filename=__file__,
        triton_meta={'signature': {0: '*fp32', 1: '*fp32', 2: 'i32', 3: 'i32'}, 'device': DeviceProperties(type='cuda', index=0, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, multi_processor_count=132), 'constants': {}, 'configs': [AttrsDescriptor(divisible_by_16=(0, 1, 3), equal_to_1=())]},
        inductor_meta={'autotune_hints': set(), 'kernel_name': 'Placeholder.DESCRIPTIVE_NAME', 'mutated_arg_names': [], 'no_x_dim': True, 'num_load': 1, 'num_reduction': 3, 'backend_hash': '3944FAAD9D18E987535CF029C4441E186640DD453AE5AE6831EDF736C09E0A47', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': False, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False}
    )
    @triton.jit
    def triton_(in_ptr0, out_ptr0, xnumel, rnumel):
        xnumel = 197
        XBLOCK: tl.constexpr = 1
        rnumel = 768
        RBLOCK: tl.constexpr = 1024
        xoffset = tl.program_id(0) * XBLOCK
        xindex = tl.full([1], xoffset, tl.int32)
        xmask = xindex < xnumel
        rindex = tl.arange(0, RBLOCK)[:]
        roffset = 0
        rmask = rindex < rnumel
        r1 = rindex
        x0 = xindex
        tmp0 = tl.load(in_ptr0 + (r1 + (768*x0)), rmask & xmask, other=0.0)
        tmp1 = tl.broadcast_to(tmp0, [RBLOCK])
        tmp3 = tl.where(rmask & xmask, tmp1, 0)
        tmp4 = tl.broadcast_to(tmp1, [RBLOCK])
        tmp6 = tl.where(rmask & xmask, tmp4, 0)
        tmp7 = triton_helpers.promote_to_tensor(tl.sum(tmp6, 0))
        tmp8 = tl.full([1], 768, tl.int32)
        tmp9 = tmp8.to(tl.float32)
        tmp10 = tmp7 / tmp9
        tmp11 = tmp1 - tmp10
        tmp12 = tmp11 * tmp11
        tmp13 = tl.broadcast_to(tmp12, [RBLOCK])
        tmp15 = tl.where(rmask & xmask, tmp13, 0)
        tmp16 = triton_helpers.promote_to_tensor(tl.sum(tmp15, 0))
        tl.store(out_ptr0 + (x0), tmp16, xmask)


buf43: SchedulerNode(ComputedBuffer)
buf43.writes = [MemoryDep('buf43', c0, {c0: 151296}, None)]
buf43.unmet_dependencies = 
    [   MemoryDep('buf39', c0, {c0: 151296}, None),
        MemoryDep('buf40', c0, {c0: 197}, None),
        MemoryDep('buf41', c0, {c0: 197}, None)]
buf43.met_dependencies = 
    [   MemoryDep('arg30_1', c1, {c0: 197, c1: 768}, None),
        MemoryDep('arg31_1', c1, {c0: 197, c1: 768}, None)]
buf43.users = [NodeUser(node=ExternKernelSchedulerNode(name='buf44'), can_inplace=False, is_weak=False)]
buf43.group.device = cuda:0
buf43.group.iteration = (151296, 1)
buf43.sizes = ([197, 768], [])
buf40_layout = FixedLayout('cuda', torch.float32, size=[1, 197, 1], stride=[197, 1, 197])
arg30_1_layout = FixedLayout('cuda', torch.float32, size=[768], stride=[1])
buf39_layout = FixedLayout('cuda', torch.float32, size=[1, 197, 768], stride=[151296, 768, 1])
arg31_1_layout = FixedLayout('cuda', torch.float32, size=[768], stride=[1])
buf41_layout = FixedLayout('cuda', torch.float32, size=[1, 197, 1], stride=[197, 1, 197])
buf43_layout = FixedLayout('cuda', torch.float32, size=[1, 197, 768], stride=[151296, 768, 1])
class buf43_loop_body:
    var_ranges = {z0: 197, z1: 768}
    index0 = 768*z0 + z1
    index1 = z0
    index2 = z1
    def body(self, ops):
        get_index = self.get_index('index0')
        load = ops.load('buf39', get_index)
        get_index_1 = self.get_index('index1')
        load_1 = ops.load('buf40', get_index_1)
        sub = ops.sub(load, load_1)
        get_index_2 = self.get_index('index1')
        load_2 = ops.load('buf41', get_index_2)
        constant = ops.constant(768.0, torch.float32)
        truediv = ops.truediv(load_2, constant)
        constant_1 = ops.constant(1e-12, torch.float32)
        add = ops.add(truediv, constant_1)
        rsqrt = ops.rsqrt(add)
        mul = ops.mul(sub, rsqrt)
        get_index_3 = self.get_index('index2')
        load_3 = ops.load('arg30_1', get_index_3)
        mul_1 = ops.mul(mul, load_3)
        get_index_4 = self.get_index('index2')
        load_4 = ops.load('arg31_1', get_index_4)
        add_1 = ops.add(mul_1, load_4)
        get_index_5 = self.get_index('index0')
        store = ops.store('buf43', get_index_5, add_1, None)
        return store
buf43 Triton code:
    import triton
    import triton.language as tl
    from triton.compiler.compiler import AttrsDescriptor

    from torch._inductor.runtime import triton_helpers, triton_heuristics
    from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
    from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, instance_descriptor, DeviceProperties

    @triton_heuristics.pointwise(
        size_hints=[262144], 
        filename=__file__,
        triton_meta={'signature': {0: '*fp32', 1: '*fp32', 2: '*fp32', 3: '*fp32', 4: '*fp32', 5: '*fp32', 6: 'i32'}, 'device': DeviceProperties(type='cuda', index=0, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, multi_processor_count=132), 'constants': {}, 'configs': [AttrsDescriptor(divisible_by_16=(0, 1, 2, 3, 4, 5, 6), equal_to_1=())]},
        inductor_meta={'autotune_hints': set(), 'kernel_name': 'Placeholder.DESCRIPTIVE_NAME', 'mutated_arg_names': [], 'no_x_dim': False, 'num_load': 5, 'num_reduction': 0, 'backend_hash': '3944FAAD9D18E987535CF029C4441E186640DD453AE5AE6831EDF736C09E0A47', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': False, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False},
        min_elem_per_thread=0
    )
    @triton.jit
    def triton_(in_ptr0, in_ptr1, in_ptr2, in_ptr3, in_ptr4, out_ptr0, xnumel, XBLOCK : tl.constexpr):
        xnumel = 151296
        xoffset = tl.program_id(0) * XBLOCK
        xindex = xoffset + tl.arange(0, XBLOCK)[:]
        xmask = xindex < xnumel
        x2 = xindex
        x1 = (xindex // 768)
        x0 = xindex % 768
        tmp0 = tl.load(in_ptr0 + (x2), xmask)
        tmp1 = tl.load(in_ptr1 + (x1), xmask, eviction_policy='evict_last')
        tmp3 = tl.load(in_ptr2 + (x1), xmask, eviction_policy='evict_last')
        tmp10 = tl.load(in_ptr3 + (x0), xmask, eviction_policy='evict_last')
        tmp12 = tl.load(in_ptr4 + (x0), xmask, eviction_policy='evict_last')
        tmp2 = tmp0 - tmp1
        tmp4 = 768.0
        tmp5 = tmp3 / tmp4
        tmp6 = 1e-12
        tmp7 = tmp5 + tmp6
        tmp8 = libdevice.rsqrt(tmp7)
        tmp9 = tmp2 * tmp8
        tmp11 = tmp9 * tmp10
        tmp13 = tmp11 + tmp12
        tl.store(out_ptr0 + (x2), tmp13, xmask)


buf44: ExternKernelSchedulerNode(ExternKernelOut)
buf44.writes = [StarDep(name='buf44', mode=None)]
buf44.unmet_dependencies = [StarDep(name='buf43', mode=None)]
buf44.met_dependencies = [StarDep(name='arg32_1', mode=None)]
buf44.users = [NodeUser(node=SchedulerNode(name='buf45'), can_inplace=True, is_weak=False)]
buf44.node.kernel = extern_kernels.mm


buf45: SchedulerNode(ComputedBuffer)
buf45.writes = [MemoryDep('buf45', c0, {c0: 605184}, None)]
buf45.unmet_dependencies = [MemoryDep('buf44', c0, {c0: 605184}, None)]
buf45.met_dependencies = [MemoryDep('arg33_1', c1, {c0: 197, c1: 3072}, None)]
buf45.users = [NodeUser(node=ExternKernelSchedulerNode(name='buf46'), can_inplace=False, is_weak=False)]
buf45.group.device = cuda:0
buf45.group.iteration = (605184, 1)
buf45.sizes = ([197, 3072], [])
arg33_1_layout = FixedLayout('cuda', torch.float32, size=[3072], stride=[1])
buf44_layout = FixedLayout('cuda', torch.float32, size=[197, 3072], stride=[3072, 1])
buf45_layout = FixedLayout('cuda', torch.float32, size=[1, 197, 3072], stride=[605184, 3072, 1])
class buf45_loop_body:
    var_ranges = {z0: 197, z1: 3072}
    index0 = 3072*z0 + z1
    index1 = z1
    def body(self, ops):
        get_index = self.get_index('index0')
        load = ops.load('buf44', get_index)
        get_index_1 = self.get_index('index1')
        load_1 = ops.load('arg33_1', get_index_1)
        add = ops.add(load, load_1)
        constant = ops.constant(0.5, torch.float32)
        mul = ops.mul(add, constant)
        get_index_2 = self.get_index('index0')
        load_2 = ops.load('buf44', get_index_2)
        get_index_3 = self.get_index('index1')
        load_3 = ops.load('arg33_1', get_index_3)
        add_1 = ops.add(load_2, load_3)
        constant_1 = ops.constant(0.7071067811865476, torch.float32)
        mul_1 = ops.mul(add_1, constant_1)
        erf = ops.erf(mul_1)
        constant_2 = ops.constant(1.0, torch.float32)
        add_2 = ops.add(erf, constant_2)
        mul_2 = ops.mul(mul, add_2)
        get_index_4 = self.get_index('index0')
        store = ops.store('buf45', get_index_4, mul_2, None)
        return store
buf45 Triton code:
    import triton
    import triton.language as tl
    from triton.compiler.compiler import AttrsDescriptor

    from torch._inductor.runtime import triton_helpers, triton_heuristics
    from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
    from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, instance_descriptor, DeviceProperties

    @triton_heuristics.pointwise(
        size_hints=[1048576], 
        filename=__file__,
        triton_meta={'signature': {0: '*fp32', 1: '*fp32', 2: 'i32'}, 'device': DeviceProperties(type='cuda', index=0, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, multi_processor_count=132), 'constants': {}, 'configs': [AttrsDescriptor(divisible_by_16=(0, 1, 2), equal_to_1=())]},
        inductor_meta={'autotune_hints': set(), 'kernel_name': 'Placeholder.DESCRIPTIVE_NAME', 'mutated_arg_names': ['in_out_ptr0'], 'no_x_dim': False, 'num_load': 2, 'num_reduction': 0, 'backend_hash': '3944FAAD9D18E987535CF029C4441E186640DD453AE5AE6831EDF736C09E0A47', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': False, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False},
        min_elem_per_thread=0
    )
    @triton.jit
    def triton_(in_out_ptr0, in_ptr0, xnumel, XBLOCK : tl.constexpr):
        xnumel = 605184
        xoffset = tl.program_id(0) * XBLOCK
        xindex = xoffset + tl.arange(0, XBLOCK)[:]
        xmask = xindex < xnumel
        x2 = xindex
        x0 = xindex % 3072
        tmp0 = tl.load(in_out_ptr0 + (x2), xmask)
        tmp1 = tl.load(in_ptr0 + (x0), xmask, eviction_policy='evict_last')
        tmp2 = tmp0 + tmp1
        tmp3 = 0.5
        tmp4 = tmp2 * tmp3
        tmp5 = 0.7071067811865476
        tmp6 = tmp2 * tmp5
        tmp7 = libdevice.erf(tmp6)
        tmp8 = 1.0
        tmp9 = tmp7 + tmp8
        tmp10 = tmp4 * tmp9
        tl.store(in_out_ptr0 + (x2), tmp10, xmask)


buf46: ExternKernelSchedulerNode(ExternKernelOut)
buf46.writes = [StarDep(name='buf46', mode=None)]
buf46.unmet_dependencies = [StarDep(name='buf45', mode=None)]
buf46.met_dependencies = [StarDep(name='arg34_1', mode=None)]
buf46.users = [NodeUser(node=SchedulerNode(name='buf47'), can_inplace=False, is_weak=False), NodeUser(node=SchedulerNode(name='buf48'), can_inplace=False, is_weak=False), NodeUser(node=SchedulerNode(name='buf49'), can_inplace=False, is_weak=False), NodeUser(node=SchedulerNode(name='buf50'), can_inplace=True, is_weak=False), NodeUser(node=SchedulerNode(name='buf60'), can_inplace=True, is_weak=False)]
buf46.node.kernel = extern_kernels.mm


buf47: SchedulerNode(ComputedBuffer)
buf47.writes = [MemoryDep('buf47', c0, {c0: 197}, None)]
buf47.unmet_dependencies = 
    [   MemoryDep('buf39', c0, {c0: 151296}, None),
        MemoryDep('buf46', c0, {c0: 151296}, None)]
buf47.met_dependencies = [MemoryDep('arg35_1', c1, {c0: 197, c1: 768}, None)]
buf47.users = [NodeUser(node=SchedulerNode(name='buf50'), can_inplace=False, is_weak=False)]
buf47.group.device = cuda:0
buf47.group.iteration = (197, 768)
buf47.sizes = ([197], [768])
buf46_layout = FixedLayout('cuda', torch.float32, size=[197, 768], stride=[768, 1])
arg35_1_layout = FixedLayout('cuda', torch.float32, size=[768], stride=[1])
buf39_layout = FixedLayout('cuda', torch.float32, size=[1, 197, 768], stride=[151296, 768, 1])
buf47_layout = FixedLayout('cuda', torch.float32, size=[1, 197, 1], stride=[197, 1, 197])
class buf47_loop_body:
    var_ranges = {z0: 197, z1: 768}
    index0 = 768*z0 + z1
    index1 = z1
    index2 = z0
    def body(self, ops):
        get_index = self.get_index('index0')
        load = ops.load('buf46', get_index)
        get_index_1 = self.get_index('index1')
        load_1 = ops.load('arg35_1', get_index_1)
        add = ops.add(load, load_1)
        get_index_2 = self.get_index('index0')
        load_2 = ops.load('buf39', get_index_2)
        add_1 = ops.add(add, load_2)
        reduction = ops.reduction(torch.float32, torch.float32, 'welford_reduce', add_1)
        getitem = reduction[0]
        getitem_1 = reduction[1]
        getitem_2 = reduction[2]
        get_index_3 = self.get_index('index2')
        store_reduction = ops.store_reduction('buf47', get_index_3, getitem)
        return store_reduction
buf47 Triton code:
    import triton
    import triton.language as tl
    from triton.compiler.compiler import AttrsDescriptor

    from torch._inductor.runtime import triton_helpers, triton_heuristics
    from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
    from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, instance_descriptor, DeviceProperties

    @triton_heuristics.persistent_reduction(
        size_hints=[256, 1024],
        reduction_hint=ReductionHint.INNER,
        filename=__file__,
        triton_meta={'signature': {0: '*fp32', 1: '*fp32', 2: '*fp32', 3: '*fp32', 4: 'i32', 5: 'i32'}, 'device': DeviceProperties(type='cuda', index=0, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, multi_processor_count=132), 'constants': {}, 'configs': [AttrsDescriptor(divisible_by_16=(0, 1, 2, 3, 5), equal_to_1=())]},
        inductor_meta={'autotune_hints': set(), 'kernel_name': 'Placeholder.DESCRIPTIVE_NAME', 'mutated_arg_names': [], 'no_x_dim': True, 'num_load': 3, 'num_reduction': 3, 'backend_hash': '3944FAAD9D18E987535CF029C4441E186640DD453AE5AE6831EDF736C09E0A47', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': False, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False}
    )
    @triton.jit
    def triton_(in_ptr0, in_ptr1, in_ptr2, out_ptr0, xnumel, rnumel):
        xnumel = 197
        XBLOCK: tl.constexpr = 1
        rnumel = 768
        RBLOCK: tl.constexpr = 1024
        xoffset = tl.program_id(0) * XBLOCK
        xindex = tl.full([1], xoffset, tl.int32)
        xmask = xindex < xnumel
        rindex = tl.arange(0, RBLOCK)[:]
        roffset = 0
        rmask = rindex < rnumel
        r1 = rindex
        x0 = xindex
        tmp0 = tl.load(in_ptr0 + (r1 + (768*x0)), rmask & xmask, other=0.0)
        tmp1 = tl.load(in_ptr1 + (r1), rmask, eviction_policy='evict_last', other=0.0)
        tmp3 = tl.load(in_ptr2 + (r1 + (768*x0)), rmask & xmask, other=0.0)
        tmp2 = tmp0 + tmp1
        tmp4 = tmp2 + tmp3
        tmp5 = tl.broadcast_to(tmp4, [RBLOCK])
        tmp7 = tl.where(rmask & xmask, tmp5, 0)
        tmp8 = tl.broadcast_to(tmp5, [RBLOCK])
        tmp10 = tl.where(rmask & xmask, tmp8, 0)
        tmp11 = triton_helpers.promote_to_tensor(tl.sum(tmp10, 0))
        tmp12 = tl.full([1], 768, tl.int32)
        tmp13 = tmp12.to(tl.float32)
        tmp14 = tmp11 / tmp13
        tmp15 = tmp5 - tmp14
        tmp16 = tmp15 * tmp15
        tmp17 = tl.broadcast_to(tmp16, [RBLOCK])
        tmp19 = tl.where(rmask & xmask, tmp17, 0)
        tmp20 = triton_helpers.promote_to_tensor(tl.sum(tmp19, 0))
        tl.store(out_ptr0 + (x0), tmp14, xmask)


buf48: SchedulerNode(ComputedBuffer)
buf48.writes = [MemoryDep('buf48', c0, {c0: 197}, None)]
buf48.unmet_dependencies = 
    [   MemoryDep('buf39', c0, {c0: 151296}, None),
        MemoryDep('buf46', c0, {c0: 151296}, None)]
buf48.met_dependencies = [MemoryDep('arg35_1', c1, {c0: 197, c1: 768}, None)]
buf48.users = [NodeUser(node=SchedulerNode(name='buf50'), can_inplace=False, is_weak=False)]
buf48.group.device = cuda:0
buf48.group.iteration = (197, 768)
buf48.sizes = ([197], [768])
buf46_layout = FixedLayout('cuda', torch.float32, size=[197, 768], stride=[768, 1])
arg35_1_layout = FixedLayout('cuda', torch.float32, size=[768], stride=[1])
buf39_layout = FixedLayout('cuda', torch.float32, size=[1, 197, 768], stride=[151296, 768, 1])
buf48_layout = FixedLayout('cuda', torch.float32, size=[1, 197, 1], stride=[197, 1, 197])
class buf48_loop_body:
    var_ranges = {z0: 197, z1: 768}
    index0 = 768*z0 + z1
    index1 = z1
    index2 = z0
    def body(self, ops):
        get_index = self.get_index('index0')
        load = ops.load('buf46', get_index)
        get_index_1 = self.get_index('index1')
        load_1 = ops.load('arg35_1', get_index_1)
        add = ops.add(load, load_1)
        get_index_2 = self.get_index('index0')
        load_2 = ops.load('buf39', get_index_2)
        add_1 = ops.add(add, load_2)
        reduction = ops.reduction(torch.float32, torch.float32, 'welford_reduce', add_1)
        getitem = reduction[0]
        getitem_1 = reduction[1]
        getitem_2 = reduction[2]
        get_index_3 = self.get_index('index2')
        store_reduction = ops.store_reduction('buf48', get_index_3, getitem_1)
        return store_reduction
buf48 Triton code:
    import triton
    import triton.language as tl
    from triton.compiler.compiler import AttrsDescriptor

    from torch._inductor.runtime import triton_helpers, triton_heuristics
    from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
    from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, instance_descriptor, DeviceProperties

    @triton_heuristics.persistent_reduction(
        size_hints=[256, 1024],
        reduction_hint=ReductionHint.INNER,
        filename=__file__,
        triton_meta={'signature': {0: '*fp32', 1: '*fp32', 2: '*fp32', 3: '*fp32', 4: 'i32', 5: 'i32'}, 'device': DeviceProperties(type='cuda', index=0, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, multi_processor_count=132), 'constants': {}, 'configs': [AttrsDescriptor(divisible_by_16=(0, 1, 2, 3, 5), equal_to_1=())]},
        inductor_meta={'autotune_hints': set(), 'kernel_name': 'Placeholder.DESCRIPTIVE_NAME', 'mutated_arg_names': [], 'no_x_dim': True, 'num_load': 3, 'num_reduction': 3, 'backend_hash': '3944FAAD9D18E987535CF029C4441E186640DD453AE5AE6831EDF736C09E0A47', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': False, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False}
    )
    @triton.jit
    def triton_(in_ptr0, in_ptr1, in_ptr2, out_ptr0, xnumel, rnumel):
        xnumel = 197
        XBLOCK: tl.constexpr = 1
        rnumel = 768
        RBLOCK: tl.constexpr = 1024
        xoffset = tl.program_id(0) * XBLOCK
        xindex = tl.full([1], xoffset, tl.int32)
        xmask = xindex < xnumel
        rindex = tl.arange(0, RBLOCK)[:]
        roffset = 0
        rmask = rindex < rnumel
        r1 = rindex
        x0 = xindex
        tmp0 = tl.load(in_ptr0 + (r1 + (768*x0)), rmask & xmask, other=0.0)
        tmp1 = tl.load(in_ptr1 + (r1), rmask, eviction_policy='evict_last', other=0.0)
        tmp3 = tl.load(in_ptr2 + (r1 + (768*x0)), rmask & xmask, other=0.0)
        tmp2 = tmp0 + tmp1
        tmp4 = tmp2 + tmp3
        tmp5 = tl.broadcast_to(tmp4, [RBLOCK])
        tmp7 = tl.where(rmask & xmask, tmp5, 0)
        tmp8 = tl.broadcast_to(tmp5, [RBLOCK])
        tmp10 = tl.where(rmask & xmask, tmp8, 0)
        tmp11 = triton_helpers.promote_to_tensor(tl.sum(tmp10, 0))
        tmp12 = tl.full([1], 768, tl.int32)
        tmp13 = tmp12.to(tl.float32)
        tmp14 = tmp11 / tmp13
        tmp15 = tmp5 - tmp14
        tmp16 = tmp15 * tmp15
        tmp17 = tl.broadcast_to(tmp16, [RBLOCK])
        tmp19 = tl.where(rmask & xmask, tmp17, 0)
        tmp20 = triton_helpers.promote_to_tensor(tl.sum(tmp19, 0))
        tl.store(out_ptr0 + (x0), tmp20, xmask)


buf50: SchedulerNode(ComputedBuffer)
buf50.writes = [MemoryDep('buf50', c0, {c0: 151296}, None)]
buf50.unmet_dependencies = 
    [   MemoryDep('buf39', c0, {c0: 151296}, None),
        MemoryDep('buf46', c0, {c0: 151296}, None),
        MemoryDep('buf47', c0, {c0: 197}, None),
        MemoryDep('buf48', c0, {c0: 197}, None)]
buf50.met_dependencies = 
    [   MemoryDep('arg35_1', c1, {c0: 197, c1: 768}, None),
        MemoryDep('arg36_1', c1, {c0: 197, c1: 768}, None),
        MemoryDep('arg37_1', c1, {c0: 197, c1: 768}, None)]
buf50.users = [NodeUser(node=ExternKernelSchedulerNode(name='buf51'), can_inplace=False, is_weak=False), NodeUser(node=ExternKernelSchedulerNode(name='buf52'), can_inplace=False, is_weak=False), NodeUser(node=ExternKernelSchedulerNode(name='buf53'), can_inplace=False, is_weak=False)]
buf50.group.device = cuda:0
buf50.group.iteration = (151296, 1)
buf50.sizes = ([197, 768], [])
buf47_layout = FixedLayout('cuda', torch.float32, size=[1, 197, 1], stride=[197, 1, 197])
arg36_1_layout = FixedLayout('cuda', torch.float32, size=[768], stride=[1])
buf48_layout = FixedLayout('cuda', torch.float32, size=[1, 197, 1], stride=[197, 1, 197])
buf39_layout = FixedLayout('cuda', torch.float32, size=[1, 197, 768], stride=[151296, 768, 1])
buf46_layout = FixedLayout('cuda', torch.float32, size=[197, 768], stride=[768, 1])
arg37_1_layout = FixedLayout('cuda', torch.float32, size=[768], stride=[1])
arg35_1_layout = FixedLayout('cuda', torch.float32, size=[768], stride=[1])
buf50_layout = FixedLayout('cuda', torch.float32, size=[1, 197, 768], stride=[151296, 768, 1])
class buf50_loop_body:
    var_ranges = {z0: 197, z1: 768}
    index0 = 768*z0 + z1
    index1 = z1
    index2 = z0
    def body(self, ops):
        get_index = self.get_index('index0')
        load = ops.load('buf46', get_index)
        get_index_1 = self.get_index('index1')
        load_1 = ops.load('arg35_1', get_index_1)
        add = ops.add(load, load_1)
        get_index_2 = self.get_index('index0')
        load_2 = ops.load('buf39', get_index_2)
        add_1 = ops.add(add, load_2)
        get_index_3 = self.get_index('index2')
        load_3 = ops.load('buf47', get_index_3)
        sub = ops.sub(add_1, load_3)
        get_index_4 = self.get_index('index2')
        load_4 = ops.load('buf48', get_index_4)
        constant = ops.constant(768.0, torch.float32)
        truediv = ops.truediv(load_4, constant)
        constant_1 = ops.constant(1e-12, torch.float32)
        add_2 = ops.add(truediv, constant_1)
        rsqrt = ops.rsqrt(add_2)
        mul = ops.mul(sub, rsqrt)
        get_index_5 = self.get_index('index1')
        load_5 = ops.load('arg36_1', get_index_5)
        mul_1 = ops.mul(mul, load_5)
        get_index_6 = self.get_index('index1')
        load_6 = ops.load('arg37_1', get_index_6)
        add_3 = ops.add(mul_1, load_6)
        get_index_7 = self.get_index('index0')
        store = ops.store('buf50', get_index_7, add_3, None)
        return store
buf50 Triton code:
    import triton
    import triton.language as tl
    from triton.compiler.compiler import AttrsDescriptor

    from torch._inductor.runtime import triton_helpers, triton_heuristics
    from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
    from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, instance_descriptor, DeviceProperties

    @triton_heuristics.pointwise(
        size_hints=[262144], 
        filename=__file__,
        triton_meta={'signature': {0: '*fp32', 1: '*fp32', 2: '*fp32', 3: '*fp32', 4: '*fp32', 5: '*fp32', 6: '*fp32', 7: '*fp32', 8: 'i32'}, 'device': DeviceProperties(type='cuda', index=0, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, multi_processor_count=132), 'constants': {}, 'configs': [AttrsDescriptor(divisible_by_16=(0, 1, 2, 3, 4, 5, 6, 7, 8), equal_to_1=())]},
        inductor_meta={'autotune_hints': set(), 'kernel_name': 'Placeholder.DESCRIPTIVE_NAME', 'mutated_arg_names': [], 'no_x_dim': False, 'num_load': 7, 'num_reduction': 0, 'backend_hash': '3944FAAD9D18E987535CF029C4441E186640DD453AE5AE6831EDF736C09E0A47', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': False, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False},
        min_elem_per_thread=0
    )
    @triton.jit
    def triton_(in_ptr0, in_ptr1, in_ptr2, in_ptr3, in_ptr4, in_ptr5, in_ptr6, out_ptr0, xnumel, XBLOCK : tl.constexpr):
        xnumel = 151296
        xoffset = tl.program_id(0) * XBLOCK
        xindex = xoffset + tl.arange(0, XBLOCK)[:]
        xmask = xindex < xnumel
        x2 = xindex
        x0 = xindex % 768
        x1 = (xindex // 768)
        tmp0 = tl.load(in_ptr0 + (x2), xmask)
        tmp1 = tl.load(in_ptr1 + (x0), xmask, eviction_policy='evict_last')
        tmp3 = tl.load(in_ptr2 + (x2), xmask)
        tmp5 = tl.load(in_ptr3 + (x1), xmask, eviction_policy='evict_last')
        tmp7 = tl.load(in_ptr4 + (x1), xmask, eviction_policy='evict_last')
        tmp14 = tl.load(in_ptr5 + (x0), xmask, eviction_policy='evict_last')
        tmp16 = tl.load(in_ptr6 + (x0), xmask, eviction_policy='evict_last')
        tmp2 = tmp0 + tmp1
        tmp4 = tmp2 + tmp3
        tmp6 = tmp4 - tmp5
        tmp8 = 768.0
        tmp9 = tmp7 / tmp8
        tmp10 = 1e-12
        tmp11 = tmp9 + tmp10
        tmp12 = libdevice.rsqrt(tmp11)
        tmp13 = tmp6 * tmp12
        tmp15 = tmp13 * tmp14
        tmp17 = tmp15 + tmp16
        tl.store(out_ptr0 + (x2), tmp17, xmask)


buf51: ExternKernelSchedulerNode(ExternKernelOut)
buf51.writes = [StarDep(name='buf51', mode=None)]
buf51.unmet_dependencies = [StarDep(name='buf50', mode=None)]
buf51.met_dependencies = [StarDep(name='arg38_1', mode=None), StarDep(name='arg39_1', mode=None)]
buf51.users = [NodeUser(node=ExternKernelSchedulerNode(name='buf54'), can_inplace=False, is_weak=False)]
buf51.node.kernel = extern_kernels.addmm


buf52: ExternKernelSchedulerNode(ExternKernelOut)
buf52.writes = [StarDep(name='buf52', mode=None)]
buf52.unmet_dependencies = [StarDep(name='buf50', mode=None)]
buf52.met_dependencies = [StarDep(name='arg40_1', mode=None), StarDep(name='arg41_1', mode=None)]
buf52.users = [NodeUser(node=ExternKernelSchedulerNode(name='buf54'), can_inplace=False, is_weak=False)]
buf52.node.kernel = extern_kernels.addmm


buf53: ExternKernelSchedulerNode(ExternKernelOut)
buf53.writes = [StarDep(name='buf53', mode=None)]
buf53.unmet_dependencies = [StarDep(name='buf50', mode=None)]
buf53.met_dependencies = [StarDep(name='arg42_1', mode=None), StarDep(name='arg43_1', mode=None)]
buf53.users = [NodeUser(node=ExternKernelSchedulerNode(name='buf54'), can_inplace=False, is_weak=False)]
buf53.node.kernel = extern_kernels.addmm


buf54: ExternKernelSchedulerNode(FallbackKernel)
buf54.writes = [StarDep(name='buf54', mode=None)]
buf54.unmet_dependencies = 
    [   StarDep(name='buf51', mode=None),
        StarDep(name='buf52', mode=None),
        StarDep(name='buf53', mode=None)]
buf54.met_dependencies = []
buf54.users = [NodeUser(node=ExternKernelSchedulerNode(name='buf55'), can_inplace=False, is_weak=False), NodeUser(node=ExternKernelSchedulerNode(name='buf56'), can_inplace=False, is_weak=False), NodeUser(node=ExternKernelSchedulerNode(name='buf57'), can_inplace=False, is_weak=False), NodeUser(node=ExternKernelSchedulerNode(name='buf58'), can_inplace=False, is_weak=False)]
buf54.node.kernel = None


buf55: ExternKernelSchedulerNode(MultiOutput)
buf55.writes = [StarDep(name='buf55', mode=None)]
buf55.unmet_dependencies = [StarDep(name='buf54', mode=None)]
buf55.met_dependencies = []
buf55.users = [NodeUser(node=ExternKernelSchedulerNode(name='buf59'), can_inplace=False, is_weak=False)]
buf55.node.kernel = None


buf59: ExternKernelSchedulerNode(ExternKernelOut)
buf59.writes = [StarDep(name='buf59', mode=None)]
buf59.unmet_dependencies = [StarDep(name='buf55', mode=None)]
buf59.met_dependencies = [StarDep(name='arg44_1', mode=None)]
buf59.users = [NodeUser(node=SchedulerNode(name='buf60'), can_inplace=True, is_weak=False)]
buf59.node.kernel = extern_kernels.mm


buf60: SchedulerNode(ComputedBuffer)
buf60.writes = [MemoryDep('buf60', c0, {c0: 151296}, None)]
buf60.unmet_dependencies = 
    [   MemoryDep('buf39', c0, {c0: 151296}, None),
        MemoryDep('buf46', c0, {c0: 151296}, None),
        MemoryDep('buf59', c0, {c0: 151296}, None)]
buf60.met_dependencies = 
    [   MemoryDep('arg35_1', c1, {c0: 197, c1: 768}, None),
        MemoryDep('arg45_1', c1, {c0: 197, c1: 768}, None)]
buf60.users = [NodeUser(node=SchedulerNode(name='buf61'), can_inplace=False, is_weak=False), NodeUser(node=SchedulerNode(name='buf62'), can_inplace=False, is_weak=False), NodeUser(node=SchedulerNode(name='buf63'), can_inplace=False, is_weak=False), NodeUser(node=SchedulerNode(name='buf64'), can_inplace=True, is_weak=False), NodeUser(node=SchedulerNode(name='buf68'), can_inplace=False, is_weak=False), NodeUser(node=SchedulerNode(name='buf69'), can_inplace=False, is_weak=False), NodeUser(node=SchedulerNode(name='buf70'), can_inplace=False, is_weak=False), NodeUser(node=SchedulerNode(name='buf71'), can_inplace=True, is_weak=False), NodeUser(node=SchedulerNode(name='buf81'), can_inplace=True, is_weak=False)]
buf60.group.device = cuda:0
buf60.group.iteration = (151296, 1)
buf60.sizes = ([197, 768], [])
arg45_1_layout = FixedLayout('cuda', torch.float32, size=[768], stride=[1])
buf39_layout = FixedLayout('cuda', torch.float32, size=[1, 197, 768], stride=[151296, 768, 1])
buf59_layout = FixedLayout('cuda', torch.float32, size=[197, 768], stride=[768, 1])
buf46_layout = FixedLayout('cuda', torch.float32, size=[197, 768], stride=[768, 1])
arg35_1_layout = FixedLayout('cuda', torch.float32, size=[768], stride=[1])
buf60_layout = FixedLayout('cuda', torch.float32, size=[1, 197, 768], stride=[151296, 768, 1])
class buf60_loop_body:
    var_ranges = {z0: 197, z1: 768}
    index0 = 768*z0 + z1
    index1 = z1
    def body(self, ops):
        get_index = self.get_index('index0')
        load = ops.load('buf59', get_index)
        get_index_1 = self.get_index('index1')
        load_1 = ops.load('arg45_1', get_index_1)
        add = ops.add(load, load_1)
        get_index_2 = self.get_index('index0')
        load_2 = ops.load('buf46', get_index_2)
        get_index_3 = self.get_index('index1')
        load_3 = ops.load('arg35_1', get_index_3)
        add_1 = ops.add(load_2, load_3)
        get_index_4 = self.get_index('index0')
        load_4 = ops.load('buf39', get_index_4)
        add_2 = ops.add(add_1, load_4)
        add_3 = ops.add(add, add_2)
        get_index_5 = self.get_index('index0')
        store = ops.store('buf60', get_index_5, add_3, None)
        return store
buf60 Triton code:
    import triton
    import triton.language as tl
    from triton.compiler.compiler import AttrsDescriptor

    from torch._inductor.runtime import triton_helpers, triton_heuristics
    from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
    from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, instance_descriptor, DeviceProperties

    @triton_heuristics.pointwise(
        size_hints=[262144], 
        filename=__file__,
        triton_meta={'signature': {0: '*fp32', 1: '*fp32', 2: '*fp32', 3: '*fp32', 4: '*fp32', 5: 'i32'}, 'device': DeviceProperties(type='cuda', index=0, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, multi_processor_count=132), 'constants': {}, 'configs': [AttrsDescriptor(divisible_by_16=(0, 1, 2, 3, 4, 5), equal_to_1=())]},
        inductor_meta={'autotune_hints': set(), 'kernel_name': 'Placeholder.DESCRIPTIVE_NAME', 'mutated_arg_names': ['in_out_ptr0'], 'no_x_dim': False, 'num_load': 5, 'num_reduction': 0, 'backend_hash': '3944FAAD9D18E987535CF029C4441E186640DD453AE5AE6831EDF736C09E0A47', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': False, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False},
        min_elem_per_thread=0
    )
    @triton.jit
    def triton_(in_out_ptr0, in_ptr0, in_ptr1, in_ptr2, in_ptr3, xnumel, XBLOCK : tl.constexpr):
        xnumel = 151296
        xoffset = tl.program_id(0) * XBLOCK
        xindex = xoffset + tl.arange(0, XBLOCK)[:]
        xmask = xindex < xnumel
        x2 = xindex
        x0 = xindex % 768
        tmp0 = tl.load(in_out_ptr0 + (x2), xmask)
        tmp1 = tl.load(in_ptr0 + (x0), xmask, eviction_policy='evict_last')
        tmp3 = tl.load(in_ptr1 + (x2), xmask)
        tmp4 = tl.load(in_ptr2 + (x0), xmask, eviction_policy='evict_last')
        tmp6 = tl.load(in_ptr3 + (x2), xmask)
        tmp2 = tmp0 + tmp1
        tmp5 = tmp3 + tmp4
        tmp7 = tmp5 + tmp6
        tmp8 = tmp2 + tmp7
        tl.store(in_out_ptr0 + (x2), tmp8, xmask)


buf61: SchedulerNode(ComputedBuffer)
buf61.writes = [MemoryDep('buf61', c0, {c0: 197}, None)]
buf61.unmet_dependencies = [MemoryDep('buf60', c0, {c0: 151296}, None)]
buf61.met_dependencies = []
buf61.users = [NodeUser(node=SchedulerNode(name='buf64'), can_inplace=False, is_weak=False)]
buf61.group.device = cuda:0
buf61.group.iteration = (197, 768)
buf61.sizes = ([197], [768])
buf60_layout = FixedLayout('cuda', torch.float32, size=[1, 197, 768], stride=[151296, 768, 1])
buf61_layout = FixedLayout('cuda', torch.float32, size=[1, 197, 1], stride=[197, 1, 197])
class buf61_loop_body:
    var_ranges = {z0: 197, z1: 768}
    index0 = 768*z0 + z1
    index1 = z0
    def body(self, ops):
        get_index = self.get_index('index0')
        load = ops.load('buf60', get_index)
        reduction = ops.reduction(torch.float32, torch.float32, 'welford_reduce', load)
        getitem = reduction[0]
        getitem_1 = reduction[1]
        getitem_2 = reduction[2]
        get_index_1 = self.get_index('index1')
        store_reduction = ops.store_reduction('buf61', get_index_1, getitem)
        return store_reduction
buf61 Triton code:
    import triton
    import triton.language as tl
    from triton.compiler.compiler import AttrsDescriptor

    from torch._inductor.runtime import triton_helpers, triton_heuristics
    from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
    from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, instance_descriptor, DeviceProperties

    @triton_heuristics.persistent_reduction(
        size_hints=[256, 1024],
        reduction_hint=ReductionHint.INNER,
        filename=__file__,
        triton_meta={'signature': {0: '*fp32', 1: '*fp32', 2: 'i32', 3: 'i32'}, 'device': DeviceProperties(type='cuda', index=0, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, multi_processor_count=132), 'constants': {}, 'configs': [AttrsDescriptor(divisible_by_16=(0, 1, 3), equal_to_1=())]},
        inductor_meta={'autotune_hints': set(), 'kernel_name': 'Placeholder.DESCRIPTIVE_NAME', 'mutated_arg_names': [], 'no_x_dim': True, 'num_load': 1, 'num_reduction': 3, 'backend_hash': '3944FAAD9D18E987535CF029C4441E186640DD453AE5AE6831EDF736C09E0A47', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': False, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False}
    )
    @triton.jit
    def triton_(in_ptr0, out_ptr0, xnumel, rnumel):
        xnumel = 197
        XBLOCK: tl.constexpr = 1
        rnumel = 768
        RBLOCK: tl.constexpr = 1024
        xoffset = tl.program_id(0) * XBLOCK
        xindex = tl.full([1], xoffset, tl.int32)
        xmask = xindex < xnumel
        rindex = tl.arange(0, RBLOCK)[:]
        roffset = 0
        rmask = rindex < rnumel
        r1 = rindex
        x0 = xindex
        tmp0 = tl.load(in_ptr0 + (r1 + (768*x0)), rmask & xmask, other=0.0)
        tmp1 = tl.broadcast_to(tmp0, [RBLOCK])
        tmp3 = tl.where(rmask & xmask, tmp1, 0)
        tmp4 = tl.broadcast_to(tmp1, [RBLOCK])
        tmp6 = tl.where(rmask & xmask, tmp4, 0)
        tmp7 = triton_helpers.promote_to_tensor(tl.sum(tmp6, 0))
        tmp8 = tl.full([1], 768, tl.int32)
        tmp9 = tmp8.to(tl.float32)
        tmp10 = tmp7 / tmp9
        tmp11 = tmp1 - tmp10
        tmp12 = tmp11 * tmp11
        tmp13 = tl.broadcast_to(tmp12, [RBLOCK])
        tmp15 = tl.where(rmask & xmask, tmp13, 0)
        tmp16 = triton_helpers.promote_to_tensor(tl.sum(tmp15, 0))
        tl.store(out_ptr0 + (x0), tmp10, xmask)


buf62: SchedulerNode(ComputedBuffer)
buf62.writes = [MemoryDep('buf62', c0, {c0: 197}, None)]
buf62.unmet_dependencies = [MemoryDep('buf60', c0, {c0: 151296}, None)]
buf62.met_dependencies = []
buf62.users = [NodeUser(node=SchedulerNode(name='buf64'), can_inplace=False, is_weak=False)]
buf62.group.device = cuda:0
buf62.group.iteration = (197, 768)
buf62.sizes = ([197], [768])
buf60_layout = FixedLayout('cuda', torch.float32, size=[1, 197, 768], stride=[151296, 768, 1])
buf62_layout = FixedLayout('cuda', torch.float32, size=[1, 197, 1], stride=[197, 1, 197])
class buf62_loop_body:
    var_ranges = {z0: 197, z1: 768}
    index0 = 768*z0 + z1
    index1 = z0
    def body(self, ops):
        get_index = self.get_index('index0')
        load = ops.load('buf60', get_index)
        reduction = ops.reduction(torch.float32, torch.float32, 'welford_reduce', load)
        getitem = reduction[0]
        getitem_1 = reduction[1]
        getitem_2 = reduction[2]
        get_index_1 = self.get_index('index1')
        store_reduction = ops.store_reduction('buf62', get_index_1, getitem_1)
        return store_reduction
buf62 Triton code:
    import triton
    import triton.language as tl
    from triton.compiler.compiler import AttrsDescriptor

    from torch._inductor.runtime import triton_helpers, triton_heuristics
    from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
    from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, instance_descriptor, DeviceProperties

    @triton_heuristics.persistent_reduction(
        size_hints=[256, 1024],
        reduction_hint=ReductionHint.INNER,
        filename=__file__,
        triton_meta={'signature': {0: '*fp32', 1: '*fp32', 2: 'i32', 3: 'i32'}, 'device': DeviceProperties(type='cuda', index=0, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, multi_processor_count=132), 'constants': {}, 'configs': [AttrsDescriptor(divisible_by_16=(0, 1, 3), equal_to_1=())]},
        inductor_meta={'autotune_hints': set(), 'kernel_name': 'Placeholder.DESCRIPTIVE_NAME', 'mutated_arg_names': [], 'no_x_dim': True, 'num_load': 1, 'num_reduction': 3, 'backend_hash': '3944FAAD9D18E987535CF029C4441E186640DD453AE5AE6831EDF736C09E0A47', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': False, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False}
    )
    @triton.jit
    def triton_(in_ptr0, out_ptr0, xnumel, rnumel):
        xnumel = 197
        XBLOCK: tl.constexpr = 1
        rnumel = 768
        RBLOCK: tl.constexpr = 1024
        xoffset = tl.program_id(0) * XBLOCK
        xindex = tl.full([1], xoffset, tl.int32)
        xmask = xindex < xnumel
        rindex = tl.arange(0, RBLOCK)[:]
        roffset = 0
        rmask = rindex < rnumel
        r1 = rindex
        x0 = xindex
        tmp0 = tl.load(in_ptr0 + (r1 + (768*x0)), rmask & xmask, other=0.0)
        tmp1 = tl.broadcast_to(tmp0, [RBLOCK])
        tmp3 = tl.where(rmask & xmask, tmp1, 0)
        tmp4 = tl.broadcast_to(tmp1, [RBLOCK])
        tmp6 = tl.where(rmask & xmask, tmp4, 0)
        tmp7 = triton_helpers.promote_to_tensor(tl.sum(tmp6, 0))
        tmp8 = tl.full([1], 768, tl.int32)
        tmp9 = tmp8.to(tl.float32)
        tmp10 = tmp7 / tmp9
        tmp11 = tmp1 - tmp10
        tmp12 = tmp11 * tmp11
        tmp13 = tl.broadcast_to(tmp12, [RBLOCK])
        tmp15 = tl.where(rmask & xmask, tmp13, 0)
        tmp16 = triton_helpers.promote_to_tensor(tl.sum(tmp15, 0))
        tl.store(out_ptr0 + (x0), tmp16, xmask)


buf64: SchedulerNode(ComputedBuffer)
buf64.writes = [MemoryDep('buf64', c0, {c0: 151296}, None)]
buf64.unmet_dependencies = 
    [   MemoryDep('buf60', c0, {c0: 151296}, None),
        MemoryDep('buf61', c0, {c0: 197}, None),
        MemoryDep('buf62', c0, {c0: 197}, None)]
buf64.met_dependencies = 
    [   MemoryDep('arg46_1', c1, {c0: 197, c1: 768}, None),
        MemoryDep('arg47_1', c1, {c0: 197, c1: 768}, None)]
buf64.users = [NodeUser(node=ExternKernelSchedulerNode(name='buf65'), can_inplace=False, is_weak=False)]
buf64.group.device = cuda:0
buf64.group.iteration = (151296, 1)
buf64.sizes = ([197, 768], [])
arg46_1_layout = FixedLayout('cuda', torch.float32, size=[768], stride=[1])
buf61_layout = FixedLayout('cuda', torch.float32, size=[1, 197, 1], stride=[197, 1, 197])
buf60_layout = FixedLayout('cuda', torch.float32, size=[1, 197, 768], stride=[151296, 768, 1])
buf62_layout = FixedLayout('cuda', torch.float32, size=[1, 197, 1], stride=[197, 1, 197])
arg47_1_layout = FixedLayout('cuda', torch.float32, size=[768], stride=[1])
buf64_layout = FixedLayout('cuda', torch.float32, size=[1, 197, 768], stride=[151296, 768, 1])
class buf64_loop_body:
    var_ranges = {z0: 197, z1: 768}
    index0 = 768*z0 + z1
    index1 = z0
    index2 = z1
    def body(self, ops):
        get_index = self.get_index('index0')
        load = ops.load('buf60', get_index)
        get_index_1 = self.get_index('index1')
        load_1 = ops.load('buf61', get_index_1)
        sub = ops.sub(load, load_1)
        get_index_2 = self.get_index('index1')
        load_2 = ops.load('buf62', get_index_2)
        constant = ops.constant(768.0, torch.float32)
        truediv = ops.truediv(load_2, constant)
        constant_1 = ops.constant(1e-12, torch.float32)
        add = ops.add(truediv, constant_1)
        rsqrt = ops.rsqrt(add)
        mul = ops.mul(sub, rsqrt)
        get_index_3 = self.get_index('index2')
        load_3 = ops.load('arg46_1', get_index_3)
        mul_1 = ops.mul(mul, load_3)
        get_index_4 = self.get_index('index2')
        load_4 = ops.load('arg47_1', get_index_4)
        add_1 = ops.add(mul_1, load_4)
        get_index_5 = self.get_index('index0')
        store = ops.store('buf64', get_index_5, add_1, None)
        return store
buf64 Triton code:
    import triton
    import triton.language as tl
    from triton.compiler.compiler import AttrsDescriptor

    from torch._inductor.runtime import triton_helpers, triton_heuristics
    from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
    from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, instance_descriptor, DeviceProperties

    @triton_heuristics.pointwise(
        size_hints=[262144], 
        filename=__file__,
        triton_meta={'signature': {0: '*fp32', 1: '*fp32', 2: '*fp32', 3: '*fp32', 4: '*fp32', 5: '*fp32', 6: 'i32'}, 'device': DeviceProperties(type='cuda', index=0, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, multi_processor_count=132), 'constants': {}, 'configs': [AttrsDescriptor(divisible_by_16=(0, 1, 2, 3, 4, 5, 6), equal_to_1=())]},
        inductor_meta={'autotune_hints': set(), 'kernel_name': 'Placeholder.DESCRIPTIVE_NAME', 'mutated_arg_names': [], 'no_x_dim': False, 'num_load': 5, 'num_reduction': 0, 'backend_hash': '3944FAAD9D18E987535CF029C4441E186640DD453AE5AE6831EDF736C09E0A47', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': False, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False},
        min_elem_per_thread=0
    )
    @triton.jit
    def triton_(in_ptr0, in_ptr1, in_ptr2, in_ptr3, in_ptr4, out_ptr0, xnumel, XBLOCK : tl.constexpr):
        xnumel = 151296
        xoffset = tl.program_id(0) * XBLOCK
        xindex = xoffset + tl.arange(0, XBLOCK)[:]
        xmask = xindex < xnumel
        x2 = xindex
        x1 = (xindex // 768)
        x0 = xindex % 768
        tmp0 = tl.load(in_ptr0 + (x2), xmask)
        tmp1 = tl.load(in_ptr1 + (x1), xmask, eviction_policy='evict_last')
        tmp3 = tl.load(in_ptr2 + (x1), xmask, eviction_policy='evict_last')
        tmp10 = tl.load(in_ptr3 + (x0), xmask, eviction_policy='evict_last')
        tmp12 = tl.load(in_ptr4 + (x0), xmask, eviction_policy='evict_last')
        tmp2 = tmp0 - tmp1
        tmp4 = 768.0
        tmp5 = tmp3 / tmp4
        tmp6 = 1e-12
        tmp7 = tmp5 + tmp6
        tmp8 = libdevice.rsqrt(tmp7)
        tmp9 = tmp2 * tmp8
        tmp11 = tmp9 * tmp10
        tmp13 = tmp11 + tmp12
        tl.store(out_ptr0 + (x2), tmp13, xmask)


buf65: ExternKernelSchedulerNode(ExternKernelOut)
buf65.writes = [StarDep(name='buf65', mode=None)]
buf65.unmet_dependencies = [StarDep(name='buf64', mode=None)]
buf65.met_dependencies = [StarDep(name='arg48_1', mode=None)]
buf65.users = [NodeUser(node=SchedulerNode(name='buf66'), can_inplace=True, is_weak=False)]
buf65.node.kernel = extern_kernels.mm


buf66: SchedulerNode(ComputedBuffer)
buf66.writes = [MemoryDep('buf66', c0, {c0: 605184}, None)]
buf66.unmet_dependencies = [MemoryDep('buf65', c0, {c0: 605184}, None)]
buf66.met_dependencies = [MemoryDep('arg49_1', c1, {c0: 197, c1: 3072}, None)]
buf66.users = [NodeUser(node=ExternKernelSchedulerNode(name='buf67'), can_inplace=False, is_weak=False)]
buf66.group.device = cuda:0
buf66.group.iteration = (605184, 1)
buf66.sizes = ([197, 3072], [])
arg49_1_layout = FixedLayout('cuda', torch.float32, size=[3072], stride=[1])
buf65_layout = FixedLayout('cuda', torch.float32, size=[197, 3072], stride=[3072, 1])
buf66_layout = FixedLayout('cuda', torch.float32, size=[1, 197, 3072], stride=[605184, 3072, 1])
class buf66_loop_body:
    var_ranges = {z0: 197, z1: 3072}
    index0 = 3072*z0 + z1
    index1 = z1
    def body(self, ops):
        get_index = self.get_index('index0')
        load = ops.load('buf65', get_index)
        get_index_1 = self.get_index('index1')
        load_1 = ops.load('arg49_1', get_index_1)
        add = ops.add(load, load_1)
        constant = ops.constant(0.5, torch.float32)
        mul = ops.mul(add, constant)
        get_index_2 = self.get_index('index0')
        load_2 = ops.load('buf65', get_index_2)
        get_index_3 = self.get_index('index1')
        load_3 = ops.load('arg49_1', get_index_3)
        add_1 = ops.add(load_2, load_3)
        constant_1 = ops.constant(0.7071067811865476, torch.float32)
        mul_1 = ops.mul(add_1, constant_1)
        erf = ops.erf(mul_1)
        constant_2 = ops.constant(1.0, torch.float32)
        add_2 = ops.add(erf, constant_2)
        mul_2 = ops.mul(mul, add_2)
        get_index_4 = self.get_index('index0')
        store = ops.store('buf66', get_index_4, mul_2, None)
        return store
buf66 Triton code:
    import triton
    import triton.language as tl
    from triton.compiler.compiler import AttrsDescriptor

    from torch._inductor.runtime import triton_helpers, triton_heuristics
    from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
    from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, instance_descriptor, DeviceProperties

    @triton_heuristics.pointwise(
        size_hints=[1048576], 
        filename=__file__,
        triton_meta={'signature': {0: '*fp32', 1: '*fp32', 2: 'i32'}, 'device': DeviceProperties(type='cuda', index=0, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, multi_processor_count=132), 'constants': {}, 'configs': [AttrsDescriptor(divisible_by_16=(0, 1, 2), equal_to_1=())]},
        inductor_meta={'autotune_hints': set(), 'kernel_name': 'Placeholder.DESCRIPTIVE_NAME', 'mutated_arg_names': ['in_out_ptr0'], 'no_x_dim': False, 'num_load': 2, 'num_reduction': 0, 'backend_hash': '3944FAAD9D18E987535CF029C4441E186640DD453AE5AE6831EDF736C09E0A47', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': False, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False},
        min_elem_per_thread=0
    )
    @triton.jit
    def triton_(in_out_ptr0, in_ptr0, xnumel, XBLOCK : tl.constexpr):
        xnumel = 605184
        xoffset = tl.program_id(0) * XBLOCK
        xindex = xoffset + tl.arange(0, XBLOCK)[:]
        xmask = xindex < xnumel
        x2 = xindex
        x0 = xindex % 3072
        tmp0 = tl.load(in_out_ptr0 + (x2), xmask)
        tmp1 = tl.load(in_ptr0 + (x0), xmask, eviction_policy='evict_last')
        tmp2 = tmp0 + tmp1
        tmp3 = 0.5
        tmp4 = tmp2 * tmp3
        tmp5 = 0.7071067811865476
        tmp6 = tmp2 * tmp5
        tmp7 = libdevice.erf(tmp6)
        tmp8 = 1.0
        tmp9 = tmp7 + tmp8
        tmp10 = tmp4 * tmp9
        tl.store(in_out_ptr0 + (x2), tmp10, xmask)


buf67: ExternKernelSchedulerNode(ExternKernelOut)
buf67.writes = [StarDep(name='buf67', mode=None)]
buf67.unmet_dependencies = [StarDep(name='buf66', mode=None)]
buf67.met_dependencies = [StarDep(name='arg50_1', mode=None)]
buf67.users = [NodeUser(node=SchedulerNode(name='buf68'), can_inplace=False, is_weak=False), NodeUser(node=SchedulerNode(name='buf69'), can_inplace=False, is_weak=False), NodeUser(node=SchedulerNode(name='buf70'), can_inplace=False, is_weak=False), NodeUser(node=SchedulerNode(name='buf71'), can_inplace=True, is_weak=False), NodeUser(node=SchedulerNode(name='buf81'), can_inplace=True, is_weak=False)]
buf67.node.kernel = extern_kernels.mm


buf68: SchedulerNode(ComputedBuffer)
buf68.writes = [MemoryDep('buf68', c0, {c0: 197}, None)]
buf68.unmet_dependencies = 
    [   MemoryDep('buf60', c0, {c0: 151296}, None),
        MemoryDep('buf67', c0, {c0: 151296}, None)]
buf68.met_dependencies = [MemoryDep('arg51_1', c1, {c0: 197, c1: 768}, None)]
buf68.users = [NodeUser(node=SchedulerNode(name='buf71'), can_inplace=False, is_weak=False)]
buf68.group.device = cuda:0
buf68.group.iteration = (197, 768)
buf68.sizes = ([197], [768])
buf67_layout = FixedLayout('cuda', torch.float32, size=[197, 768], stride=[768, 1])
buf60_layout = FixedLayout('cuda', torch.float32, size=[1, 197, 768], stride=[151296, 768, 1])
arg51_1_layout = FixedLayout('cuda', torch.float32, size=[768], stride=[1])
buf68_layout = FixedLayout('cuda', torch.float32, size=[1, 197, 1], stride=[197, 1, 197])
class buf68_loop_body:
    var_ranges = {z0: 197, z1: 768}
    index0 = 768*z0 + z1
    index1 = z1
    index2 = z0
    def body(self, ops):
        get_index = self.get_index('index0')
        load = ops.load('buf67', get_index)
        get_index_1 = self.get_index('index1')
        load_1 = ops.load('arg51_1', get_index_1)
        add = ops.add(load, load_1)
        get_index_2 = self.get_index('index0')
        load_2 = ops.load('buf60', get_index_2)
        add_1 = ops.add(add, load_2)
        reduction = ops.reduction(torch.float32, torch.float32, 'welford_reduce', add_1)
        getitem = reduction[0]
        getitem_1 = reduction[1]
        getitem_2 = reduction[2]
        get_index_3 = self.get_index('index2')
        store_reduction = ops.store_reduction('buf68', get_index_3, getitem)
        return store_reduction
buf68 Triton code:
    import triton
    import triton.language as tl
    from triton.compiler.compiler import AttrsDescriptor

    from torch._inductor.runtime import triton_helpers, triton_heuristics
    from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
    from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, instance_descriptor, DeviceProperties

    @triton_heuristics.persistent_reduction(
        size_hints=[256, 1024],
        reduction_hint=ReductionHint.INNER,
        filename=__file__,
        triton_meta={'signature': {0: '*fp32', 1: '*fp32', 2: '*fp32', 3: '*fp32', 4: 'i32', 5: 'i32'}, 'device': DeviceProperties(type='cuda', index=0, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, multi_processor_count=132), 'constants': {}, 'configs': [AttrsDescriptor(divisible_by_16=(0, 1, 2, 3, 5), equal_to_1=())]},
        inductor_meta={'autotune_hints': set(), 'kernel_name': 'Placeholder.DESCRIPTIVE_NAME', 'mutated_arg_names': [], 'no_x_dim': True, 'num_load': 3, 'num_reduction': 3, 'backend_hash': '3944FAAD9D18E987535CF029C4441E186640DD453AE5AE6831EDF736C09E0A47', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': False, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False}
    )
    @triton.jit
    def triton_(in_ptr0, in_ptr1, in_ptr2, out_ptr0, xnumel, rnumel):
        xnumel = 197
        XBLOCK: tl.constexpr = 1
        rnumel = 768
        RBLOCK: tl.constexpr = 1024
        xoffset = tl.program_id(0) * XBLOCK
        xindex = tl.full([1], xoffset, tl.int32)
        xmask = xindex < xnumel
        rindex = tl.arange(0, RBLOCK)[:]
        roffset = 0
        rmask = rindex < rnumel
        r1 = rindex
        x0 = xindex
        tmp0 = tl.load(in_ptr0 + (r1 + (768*x0)), rmask & xmask, other=0.0)
        tmp1 = tl.load(in_ptr1 + (r1), rmask, eviction_policy='evict_last', other=0.0)
        tmp3 = tl.load(in_ptr2 + (r1 + (768*x0)), rmask & xmask, other=0.0)
        tmp2 = tmp0 + tmp1
        tmp4 = tmp2 + tmp3
        tmp5 = tl.broadcast_to(tmp4, [RBLOCK])
        tmp7 = tl.where(rmask & xmask, tmp5, 0)
        tmp8 = tl.broadcast_to(tmp5, [RBLOCK])
        tmp10 = tl.where(rmask & xmask, tmp8, 0)
        tmp11 = triton_helpers.promote_to_tensor(tl.sum(tmp10, 0))
        tmp12 = tl.full([1], 768, tl.int32)
        tmp13 = tmp12.to(tl.float32)
        tmp14 = tmp11 / tmp13
        tmp15 = tmp5 - tmp14
        tmp16 = tmp15 * tmp15
        tmp17 = tl.broadcast_to(tmp16, [RBLOCK])
        tmp19 = tl.where(rmask & xmask, tmp17, 0)
        tmp20 = triton_helpers.promote_to_tensor(tl.sum(tmp19, 0))
        tl.store(out_ptr0 + (x0), tmp14, xmask)


buf69: SchedulerNode(ComputedBuffer)
buf69.writes = [MemoryDep('buf69', c0, {c0: 197}, None)]
buf69.unmet_dependencies = 
    [   MemoryDep('buf60', c0, {c0: 151296}, None),
        MemoryDep('buf67', c0, {c0: 151296}, None)]
buf69.met_dependencies = [MemoryDep('arg51_1', c1, {c0: 197, c1: 768}, None)]
buf69.users = [NodeUser(node=SchedulerNode(name='buf71'), can_inplace=False, is_weak=False)]
buf69.group.device = cuda:0
buf69.group.iteration = (197, 768)
buf69.sizes = ([197], [768])
buf67_layout = FixedLayout('cuda', torch.float32, size=[197, 768], stride=[768, 1])
buf60_layout = FixedLayout('cuda', torch.float32, size=[1, 197, 768], stride=[151296, 768, 1])
arg51_1_layout = FixedLayout('cuda', torch.float32, size=[768], stride=[1])
buf69_layout = FixedLayout('cuda', torch.float32, size=[1, 197, 1], stride=[197, 1, 197])
class buf69_loop_body:
    var_ranges = {z0: 197, z1: 768}
    index0 = 768*z0 + z1
    index1 = z1
    index2 = z0
    def body(self, ops):
        get_index = self.get_index('index0')
        load = ops.load('buf67', get_index)
        get_index_1 = self.get_index('index1')
        load_1 = ops.load('arg51_1', get_index_1)
        add = ops.add(load, load_1)
        get_index_2 = self.get_index('index0')
        load_2 = ops.load('buf60', get_index_2)
        add_1 = ops.add(add, load_2)
        reduction = ops.reduction(torch.float32, torch.float32, 'welford_reduce', add_1)
        getitem = reduction[0]
        getitem_1 = reduction[1]
        getitem_2 = reduction[2]
        get_index_3 = self.get_index('index2')
        store_reduction = ops.store_reduction('buf69', get_index_3, getitem_1)
        return store_reduction
buf69 Triton code:
    import triton
    import triton.language as tl
    from triton.compiler.compiler import AttrsDescriptor

    from torch._inductor.runtime import triton_helpers, triton_heuristics
    from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
    from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, instance_descriptor, DeviceProperties

    @triton_heuristics.persistent_reduction(
        size_hints=[256, 1024],
        reduction_hint=ReductionHint.INNER,
        filename=__file__,
        triton_meta={'signature': {0: '*fp32', 1: '*fp32', 2: '*fp32', 3: '*fp32', 4: 'i32', 5: 'i32'}, 'device': DeviceProperties(type='cuda', index=0, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, multi_processor_count=132), 'constants': {}, 'configs': [AttrsDescriptor(divisible_by_16=(0, 1, 2, 3, 5), equal_to_1=())]},
        inductor_meta={'autotune_hints': set(), 'kernel_name': 'Placeholder.DESCRIPTIVE_NAME', 'mutated_arg_names': [], 'no_x_dim': True, 'num_load': 3, 'num_reduction': 3, 'backend_hash': '3944FAAD9D18E987535CF029C4441E186640DD453AE5AE6831EDF736C09E0A47', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': False, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False}
    )
    @triton.jit
    def triton_(in_ptr0, in_ptr1, in_ptr2, out_ptr0, xnumel, rnumel):
        xnumel = 197
        XBLOCK: tl.constexpr = 1
        rnumel = 768
        RBLOCK: tl.constexpr = 1024
        xoffset = tl.program_id(0) * XBLOCK
        xindex = tl.full([1], xoffset, tl.int32)
        xmask = xindex < xnumel
        rindex = tl.arange(0, RBLOCK)[:]
        roffset = 0
        rmask = rindex < rnumel
        r1 = rindex
        x0 = xindex
        tmp0 = tl.load(in_ptr0 + (r1 + (768*x0)), rmask & xmask, other=0.0)
        tmp1 = tl.load(in_ptr1 + (r1), rmask, eviction_policy='evict_last', other=0.0)
        tmp3 = tl.load(in_ptr2 + (r1 + (768*x0)), rmask & xmask, other=0.0)
        tmp2 = tmp0 + tmp1
        tmp4 = tmp2 + tmp3
        tmp5 = tl.broadcast_to(tmp4, [RBLOCK])
        tmp7 = tl.where(rmask & xmask, tmp5, 0)
        tmp8 = tl.broadcast_to(tmp5, [RBLOCK])
        tmp10 = tl.where(rmask & xmask, tmp8, 0)
        tmp11 = triton_helpers.promote_to_tensor(tl.sum(tmp10, 0))
        tmp12 = tl.full([1], 768, tl.int32)
        tmp13 = tmp12.to(tl.float32)
        tmp14 = tmp11 / tmp13
        tmp15 = tmp5 - tmp14
        tmp16 = tmp15 * tmp15
        tmp17 = tl.broadcast_to(tmp16, [RBLOCK])
        tmp19 = tl.where(rmask & xmask, tmp17, 0)
        tmp20 = triton_helpers.promote_to_tensor(tl.sum(tmp19, 0))
        tl.store(out_ptr0 + (x0), tmp20, xmask)


buf71: SchedulerNode(ComputedBuffer)
buf71.writes = [MemoryDep('buf71', c0, {c0: 151296}, None)]
buf71.unmet_dependencies = 
    [   MemoryDep('buf60', c0, {c0: 151296}, None),
        MemoryDep('buf67', c0, {c0: 151296}, None),
        MemoryDep('buf68', c0, {c0: 197}, None),
        MemoryDep('buf69', c0, {c0: 197}, None)]
buf71.met_dependencies = 
    [   MemoryDep('arg51_1', c1, {c0: 197, c1: 768}, None),
        MemoryDep('arg52_1', c1, {c0: 197, c1: 768}, None),
        MemoryDep('arg53_1', c1, {c0: 197, c1: 768}, None)]
buf71.users = [NodeUser(node=ExternKernelSchedulerNode(name='buf72'), can_inplace=False, is_weak=False), NodeUser(node=ExternKernelSchedulerNode(name='buf73'), can_inplace=False, is_weak=False), NodeUser(node=ExternKernelSchedulerNode(name='buf74'), can_inplace=False, is_weak=False)]
buf71.group.device = cuda:0
buf71.group.iteration = (151296, 1)
buf71.sizes = ([197, 768], [])
arg52_1_layout = FixedLayout('cuda', torch.float32, size=[768], stride=[1])
buf69_layout = FixedLayout('cuda', torch.float32, size=[1, 197, 1], stride=[197, 1, 197])
arg51_1_layout = FixedLayout('cuda', torch.float32, size=[768], stride=[1])
arg53_1_layout = FixedLayout('cuda', torch.float32, size=[768], stride=[1])
buf68_layout = FixedLayout('cuda', torch.float32, size=[1, 197, 1], stride=[197, 1, 197])
buf67_layout = FixedLayout('cuda', torch.float32, size=[197, 768], stride=[768, 1])
buf60_layout = FixedLayout('cuda', torch.float32, size=[1, 197, 768], stride=[151296, 768, 1])
buf71_layout = FixedLayout('cuda', torch.float32, size=[1, 197, 768], stride=[151296, 768, 1])
class buf71_loop_body:
    var_ranges = {z0: 197, z1: 768}
    index0 = 768*z0 + z1
    index1 = z1
    index2 = z0
    def body(self, ops):
        get_index = self.get_index('index0')
        load = ops.load('buf67', get_index)
        get_index_1 = self.get_index('index1')
        load_1 = ops.load('arg51_1', get_index_1)
        add = ops.add(load, load_1)
        get_index_2 = self.get_index('index0')
        load_2 = ops.load('buf60', get_index_2)
        add_1 = ops.add(add, load_2)
        get_index_3 = self.get_index('index2')
        load_3 = ops.load('buf68', get_index_3)
        sub = ops.sub(add_1, load_3)
        get_index_4 = self.get_index('index2')
        load_4 = ops.load('buf69', get_index_4)
        constant = ops.constant(768.0, torch.float32)
        truediv = ops.truediv(load_4, constant)
        constant_1 = ops.constant(1e-12, torch.float32)
        add_2 = ops.add(truediv, constant_1)
        rsqrt = ops.rsqrt(add_2)
        mul = ops.mul(sub, rsqrt)
        get_index_5 = self.get_index('index1')
        load_5 = ops.load('arg52_1', get_index_5)
        mul_1 = ops.mul(mul, load_5)
        get_index_6 = self.get_index('index1')
        load_6 = ops.load('arg53_1', get_index_6)
        add_3 = ops.add(mul_1, load_6)
        get_index_7 = self.get_index('index0')
        store = ops.store('buf71', get_index_7, add_3, None)
        return store
buf71 Triton code:
    import triton
    import triton.language as tl
    from triton.compiler.compiler import AttrsDescriptor

    from torch._inductor.runtime import triton_helpers, triton_heuristics
    from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
    from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, instance_descriptor, DeviceProperties

    @triton_heuristics.pointwise(
        size_hints=[262144], 
        filename=__file__,
        triton_meta={'signature': {0: '*fp32', 1: '*fp32', 2: '*fp32', 3: '*fp32', 4: '*fp32', 5: '*fp32', 6: '*fp32', 7: '*fp32', 8: 'i32'}, 'device': DeviceProperties(type='cuda', index=0, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, multi_processor_count=132), 'constants': {}, 'configs': [AttrsDescriptor(divisible_by_16=(0, 1, 2, 3, 4, 5, 6, 7, 8), equal_to_1=())]},
        inductor_meta={'autotune_hints': set(), 'kernel_name': 'Placeholder.DESCRIPTIVE_NAME', 'mutated_arg_names': [], 'no_x_dim': False, 'num_load': 7, 'num_reduction': 0, 'backend_hash': '3944FAAD9D18E987535CF029C4441E186640DD453AE5AE6831EDF736C09E0A47', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': False, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False},
        min_elem_per_thread=0
    )
    @triton.jit
    def triton_(in_ptr0, in_ptr1, in_ptr2, in_ptr3, in_ptr4, in_ptr5, in_ptr6, out_ptr0, xnumel, XBLOCK : tl.constexpr):
        xnumel = 151296
        xoffset = tl.program_id(0) * XBLOCK
        xindex = xoffset + tl.arange(0, XBLOCK)[:]
        xmask = xindex < xnumel
        x2 = xindex
        x0 = xindex % 768
        x1 = (xindex // 768)
        tmp0 = tl.load(in_ptr0 + (x2), xmask)
        tmp1 = tl.load(in_ptr1 + (x0), xmask, eviction_policy='evict_last')
        tmp3 = tl.load(in_ptr2 + (x2), xmask)
        tmp5 = tl.load(in_ptr3 + (x1), xmask, eviction_policy='evict_last')
        tmp7 = tl.load(in_ptr4 + (x1), xmask, eviction_policy='evict_last')
        tmp14 = tl.load(in_ptr5 + (x0), xmask, eviction_policy='evict_last')
        tmp16 = tl.load(in_ptr6 + (x0), xmask, eviction_policy='evict_last')
        tmp2 = tmp0 + tmp1
        tmp4 = tmp2 + tmp3
        tmp6 = tmp4 - tmp5
        tmp8 = 768.0
        tmp9 = tmp7 / tmp8
        tmp10 = 1e-12
        tmp11 = tmp9 + tmp10
        tmp12 = libdevice.rsqrt(tmp11)
        tmp13 = tmp6 * tmp12
        tmp15 = tmp13 * tmp14
        tmp17 = tmp15 + tmp16
        tl.store(out_ptr0 + (x2), tmp17, xmask)


buf72: ExternKernelSchedulerNode(ExternKernelOut)
buf72.writes = [StarDep(name='buf72', mode=None)]
buf72.unmet_dependencies = [StarDep(name='buf71', mode=None)]
buf72.met_dependencies = [StarDep(name='arg54_1', mode=None), StarDep(name='arg55_1', mode=None)]
buf72.users = [NodeUser(node=ExternKernelSchedulerNode(name='buf75'), can_inplace=False, is_weak=False)]
buf72.node.kernel = extern_kernels.addmm


buf73: ExternKernelSchedulerNode(ExternKernelOut)
buf73.writes = [StarDep(name='buf73', mode=None)]
buf73.unmet_dependencies = [StarDep(name='buf71', mode=None)]
buf73.met_dependencies = [StarDep(name='arg56_1', mode=None), StarDep(name='arg57_1', mode=None)]
buf73.users = [NodeUser(node=ExternKernelSchedulerNode(name='buf75'), can_inplace=False, is_weak=False)]
buf73.node.kernel = extern_kernels.addmm


buf74: ExternKernelSchedulerNode(ExternKernelOut)
buf74.writes = [StarDep(name='buf74', mode=None)]
buf74.unmet_dependencies = [StarDep(name='buf71', mode=None)]
buf74.met_dependencies = [StarDep(name='arg58_1', mode=None), StarDep(name='arg59_1', mode=None)]
buf74.users = [NodeUser(node=ExternKernelSchedulerNode(name='buf75'), can_inplace=False, is_weak=False)]
buf74.node.kernel = extern_kernels.addmm


buf75: ExternKernelSchedulerNode(FallbackKernel)
buf75.writes = [StarDep(name='buf75', mode=None)]
buf75.unmet_dependencies = 
    [   StarDep(name='buf72', mode=None),
        StarDep(name='buf73', mode=None),
        StarDep(name='buf74', mode=None)]
buf75.met_dependencies = []
buf75.users = [NodeUser(node=ExternKernelSchedulerNode(name='buf76'), can_inplace=False, is_weak=False), NodeUser(node=ExternKernelSchedulerNode(name='buf77'), can_inplace=False, is_weak=False), NodeUser(node=ExternKernelSchedulerNode(name='buf78'), can_inplace=False, is_weak=False), NodeUser(node=ExternKernelSchedulerNode(name='buf79'), can_inplace=False, is_weak=False)]
buf75.node.kernel = None


buf76: ExternKernelSchedulerNode(MultiOutput)
buf76.writes = [StarDep(name='buf76', mode=None)]
buf76.unmet_dependencies = [StarDep(name='buf75', mode=None)]
buf76.met_dependencies = []
buf76.users = [NodeUser(node=ExternKernelSchedulerNode(name='buf80'), can_inplace=False, is_weak=False)]
buf76.node.kernel = None


buf80: ExternKernelSchedulerNode(ExternKernelOut)
buf80.writes = [StarDep(name='buf80', mode=None)]
buf80.unmet_dependencies = [StarDep(name='buf76', mode=None)]
buf80.met_dependencies = [StarDep(name='arg60_1', mode=None)]
buf80.users = [NodeUser(node=SchedulerNode(name='buf81'), can_inplace=True, is_weak=False)]
buf80.node.kernel = extern_kernels.mm


buf81: SchedulerNode(ComputedBuffer)
buf81.writes = [MemoryDep('buf81', c0, {c0: 151296}, None)]
buf81.unmet_dependencies = 
    [   MemoryDep('buf60', c0, {c0: 151296}, None),
        MemoryDep('buf67', c0, {c0: 151296}, None),
        MemoryDep('buf80', c0, {c0: 151296}, None)]
buf81.met_dependencies = 
    [   MemoryDep('arg51_1', c1, {c0: 197, c1: 768}, None),
        MemoryDep('arg61_1', c1, {c0: 197, c1: 768}, None)]
buf81.users = [NodeUser(node=SchedulerNode(name='buf82'), can_inplace=False, is_weak=False), NodeUser(node=SchedulerNode(name='buf83'), can_inplace=False, is_weak=False), NodeUser(node=SchedulerNode(name='buf84'), can_inplace=False, is_weak=False), NodeUser(node=SchedulerNode(name='buf85'), can_inplace=True, is_weak=False), NodeUser(node=SchedulerNode(name='buf89'), can_inplace=False, is_weak=False), NodeUser(node=SchedulerNode(name='buf90'), can_inplace=False, is_weak=False), NodeUser(node=SchedulerNode(name='buf91'), can_inplace=False, is_weak=False), NodeUser(node=SchedulerNode(name='buf92'), can_inplace=True, is_weak=False), NodeUser(node=SchedulerNode(name='buf102'), can_inplace=True, is_weak=False)]
buf81.group.device = cuda:0
buf81.group.iteration = (151296, 1)
buf81.sizes = ([197, 768], [])
buf80_layout = FixedLayout('cuda', torch.float32, size=[197, 768], stride=[768, 1])
arg51_1_layout = FixedLayout('cuda', torch.float32, size=[768], stride=[1])
arg61_1_layout = FixedLayout('cuda', torch.float32, size=[768], stride=[1])
buf67_layout = FixedLayout('cuda', torch.float32, size=[197, 768], stride=[768, 1])
buf60_layout = FixedLayout('cuda', torch.float32, size=[1, 197, 768], stride=[151296, 768, 1])
buf81_layout = FixedLayout('cuda', torch.float32, size=[1, 197, 768], stride=[151296, 768, 1])
class buf81_loop_body:
    var_ranges = {z0: 197, z1: 768}
    index0 = 768*z0 + z1
    index1 = z1
    def body(self, ops):
        get_index = self.get_index('index0')
        load = ops.load('buf80', get_index)
        get_index_1 = self.get_index('index1')
        load_1 = ops.load('arg61_1', get_index_1)
        add = ops.add(load, load_1)
        get_index_2 = self.get_index('index0')
        load_2 = ops.load('buf67', get_index_2)
        get_index_3 = self.get_index('index1')
        load_3 = ops.load('arg51_1', get_index_3)
        add_1 = ops.add(load_2, load_3)
        get_index_4 = self.get_index('index0')
        load_4 = ops.load('buf60', get_index_4)
        add_2 = ops.add(add_1, load_4)
        add_3 = ops.add(add, add_2)
        get_index_5 = self.get_index('index0')
        store = ops.store('buf81', get_index_5, add_3, None)
        return store
buf81 Triton code:
    import triton
    import triton.language as tl
    from triton.compiler.compiler import AttrsDescriptor

    from torch._inductor.runtime import triton_helpers, triton_heuristics
    from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
    from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, instance_descriptor, DeviceProperties

    @triton_heuristics.pointwise(
        size_hints=[262144], 
        filename=__file__,
        triton_meta={'signature': {0: '*fp32', 1: '*fp32', 2: '*fp32', 3: '*fp32', 4: '*fp32', 5: 'i32'}, 'device': DeviceProperties(type='cuda', index=0, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, multi_processor_count=132), 'constants': {}, 'configs': [AttrsDescriptor(divisible_by_16=(0, 1, 2, 3, 4, 5), equal_to_1=())]},
        inductor_meta={'autotune_hints': set(), 'kernel_name': 'Placeholder.DESCRIPTIVE_NAME', 'mutated_arg_names': ['in_out_ptr0'], 'no_x_dim': False, 'num_load': 5, 'num_reduction': 0, 'backend_hash': '3944FAAD9D18E987535CF029C4441E186640DD453AE5AE6831EDF736C09E0A47', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': False, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False},
        min_elem_per_thread=0
    )
    @triton.jit
    def triton_(in_out_ptr0, in_ptr0, in_ptr1, in_ptr2, in_ptr3, xnumel, XBLOCK : tl.constexpr):
        xnumel = 151296
        xoffset = tl.program_id(0) * XBLOCK
        xindex = xoffset + tl.arange(0, XBLOCK)[:]
        xmask = xindex < xnumel
        x2 = xindex
        x0 = xindex % 768
        tmp0 = tl.load(in_out_ptr0 + (x2), xmask)
        tmp1 = tl.load(in_ptr0 + (x0), xmask, eviction_policy='evict_last')
        tmp3 = tl.load(in_ptr1 + (x2), xmask)
        tmp4 = tl.load(in_ptr2 + (x0), xmask, eviction_policy='evict_last')
        tmp6 = tl.load(in_ptr3 + (x2), xmask)
        tmp2 = tmp0 + tmp1
        tmp5 = tmp3 + tmp4
        tmp7 = tmp5 + tmp6
        tmp8 = tmp2 + tmp7
        tl.store(in_out_ptr0 + (x2), tmp8, xmask)


buf82: SchedulerNode(ComputedBuffer)
buf82.writes = [MemoryDep('buf82', c0, {c0: 197}, None)]
buf82.unmet_dependencies = [MemoryDep('buf81', c0, {c0: 151296}, None)]
buf82.met_dependencies = []
buf82.users = [NodeUser(node=SchedulerNode(name='buf85'), can_inplace=False, is_weak=False)]
buf82.group.device = cuda:0
buf82.group.iteration = (197, 768)
buf82.sizes = ([197], [768])
buf81_layout = FixedLayout('cuda', torch.float32, size=[1, 197, 768], stride=[151296, 768, 1])
buf82_layout = FixedLayout('cuda', torch.float32, size=[1, 197, 1], stride=[197, 1, 197])
class buf82_loop_body:
    var_ranges = {z0: 197, z1: 768}
    index0 = 768*z0 + z1
    index1 = z0
    def body(self, ops):
        get_index = self.get_index('index0')
        load = ops.load('buf81', get_index)
        reduction = ops.reduction(torch.float32, torch.float32, 'welford_reduce', load)
        getitem = reduction[0]
        getitem_1 = reduction[1]
        getitem_2 = reduction[2]
        get_index_1 = self.get_index('index1')
        store_reduction = ops.store_reduction('buf82', get_index_1, getitem)
        return store_reduction
buf82 Triton code:
    import triton
    import triton.language as tl
    from triton.compiler.compiler import AttrsDescriptor

    from torch._inductor.runtime import triton_helpers, triton_heuristics
    from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
    from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, instance_descriptor, DeviceProperties

    @triton_heuristics.persistent_reduction(
        size_hints=[256, 1024],
        reduction_hint=ReductionHint.INNER,
        filename=__file__,
        triton_meta={'signature': {0: '*fp32', 1: '*fp32', 2: 'i32', 3: 'i32'}, 'device': DeviceProperties(type='cuda', index=0, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, multi_processor_count=132), 'constants': {}, 'configs': [AttrsDescriptor(divisible_by_16=(0, 1, 3), equal_to_1=())]},
        inductor_meta={'autotune_hints': set(), 'kernel_name': 'Placeholder.DESCRIPTIVE_NAME', 'mutated_arg_names': [], 'no_x_dim': True, 'num_load': 1, 'num_reduction': 3, 'backend_hash': '3944FAAD9D18E987535CF029C4441E186640DD453AE5AE6831EDF736C09E0A47', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': False, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False}
    )
    @triton.jit
    def triton_(in_ptr0, out_ptr0, xnumel, rnumel):
        xnumel = 197
        XBLOCK: tl.constexpr = 1
        rnumel = 768
        RBLOCK: tl.constexpr = 1024
        xoffset = tl.program_id(0) * XBLOCK
        xindex = tl.full([1], xoffset, tl.int32)
        xmask = xindex < xnumel
        rindex = tl.arange(0, RBLOCK)[:]
        roffset = 0
        rmask = rindex < rnumel
        r1 = rindex
        x0 = xindex
        tmp0 = tl.load(in_ptr0 + (r1 + (768*x0)), rmask & xmask, other=0.0)
        tmp1 = tl.broadcast_to(tmp0, [RBLOCK])
        tmp3 = tl.where(rmask & xmask, tmp1, 0)
        tmp4 = tl.broadcast_to(tmp1, [RBLOCK])
        tmp6 = tl.where(rmask & xmask, tmp4, 0)
        tmp7 = triton_helpers.promote_to_tensor(tl.sum(tmp6, 0))
        tmp8 = tl.full([1], 768, tl.int32)
        tmp9 = tmp8.to(tl.float32)
        tmp10 = tmp7 / tmp9
        tmp11 = tmp1 - tmp10
        tmp12 = tmp11 * tmp11
        tmp13 = tl.broadcast_to(tmp12, [RBLOCK])
        tmp15 = tl.where(rmask & xmask, tmp13, 0)
        tmp16 = triton_helpers.promote_to_tensor(tl.sum(tmp15, 0))
        tl.store(out_ptr0 + (x0), tmp10, xmask)


buf83: SchedulerNode(ComputedBuffer)
buf83.writes = [MemoryDep('buf83', c0, {c0: 197}, None)]
buf83.unmet_dependencies = [MemoryDep('buf81', c0, {c0: 151296}, None)]
buf83.met_dependencies = []
buf83.users = [NodeUser(node=SchedulerNode(name='buf85'), can_inplace=False, is_weak=False)]
buf83.group.device = cuda:0
buf83.group.iteration = (197, 768)
buf83.sizes = ([197], [768])
buf81_layout = FixedLayout('cuda', torch.float32, size=[1, 197, 768], stride=[151296, 768, 1])
buf83_layout = FixedLayout('cuda', torch.float32, size=[1, 197, 1], stride=[197, 1, 197])
class buf83_loop_body:
    var_ranges = {z0: 197, z1: 768}
    index0 = 768*z0 + z1
    index1 = z0
    def body(self, ops):
        get_index = self.get_index('index0')
        load = ops.load('buf81', get_index)
        reduction = ops.reduction(torch.float32, torch.float32, 'welford_reduce', load)
        getitem = reduction[0]
        getitem_1 = reduction[1]
        getitem_2 = reduction[2]
        get_index_1 = self.get_index('index1')
        store_reduction = ops.store_reduction('buf83', get_index_1, getitem_1)
        return store_reduction
buf83 Triton code:
    import triton
    import triton.language as tl
    from triton.compiler.compiler import AttrsDescriptor

    from torch._inductor.runtime import triton_helpers, triton_heuristics
    from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
    from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, instance_descriptor, DeviceProperties

    @triton_heuristics.persistent_reduction(
        size_hints=[256, 1024],
        reduction_hint=ReductionHint.INNER,
        filename=__file__,
        triton_meta={'signature': {0: '*fp32', 1: '*fp32', 2: 'i32', 3: 'i32'}, 'device': DeviceProperties(type='cuda', index=0, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, multi_processor_count=132), 'constants': {}, 'configs': [AttrsDescriptor(divisible_by_16=(0, 1, 3), equal_to_1=())]},
        inductor_meta={'autotune_hints': set(), 'kernel_name': 'Placeholder.DESCRIPTIVE_NAME', 'mutated_arg_names': [], 'no_x_dim': True, 'num_load': 1, 'num_reduction': 3, 'backend_hash': '3944FAAD9D18E987535CF029C4441E186640DD453AE5AE6831EDF736C09E0A47', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': False, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False}
    )
    @triton.jit
    def triton_(in_ptr0, out_ptr0, xnumel, rnumel):
        xnumel = 197
        XBLOCK: tl.constexpr = 1
        rnumel = 768
        RBLOCK: tl.constexpr = 1024
        xoffset = tl.program_id(0) * XBLOCK
        xindex = tl.full([1], xoffset, tl.int32)
        xmask = xindex < xnumel
        rindex = tl.arange(0, RBLOCK)[:]
        roffset = 0
        rmask = rindex < rnumel
        r1 = rindex
        x0 = xindex
        tmp0 = tl.load(in_ptr0 + (r1 + (768*x0)), rmask & xmask, other=0.0)
        tmp1 = tl.broadcast_to(tmp0, [RBLOCK])
        tmp3 = tl.where(rmask & xmask, tmp1, 0)
        tmp4 = tl.broadcast_to(tmp1, [RBLOCK])
        tmp6 = tl.where(rmask & xmask, tmp4, 0)
        tmp7 = triton_helpers.promote_to_tensor(tl.sum(tmp6, 0))
        tmp8 = tl.full([1], 768, tl.int32)
        tmp9 = tmp8.to(tl.float32)
        tmp10 = tmp7 / tmp9
        tmp11 = tmp1 - tmp10
        tmp12 = tmp11 * tmp11
        tmp13 = tl.broadcast_to(tmp12, [RBLOCK])
        tmp15 = tl.where(rmask & xmask, tmp13, 0)
        tmp16 = triton_helpers.promote_to_tensor(tl.sum(tmp15, 0))
        tl.store(out_ptr0 + (x0), tmp16, xmask)


buf85: SchedulerNode(ComputedBuffer)
buf85.writes = [MemoryDep('buf85', c0, {c0: 151296}, None)]
buf85.unmet_dependencies = 
    [   MemoryDep('buf81', c0, {c0: 151296}, None),
        MemoryDep('buf82', c0, {c0: 197}, None),
        MemoryDep('buf83', c0, {c0: 197}, None)]
buf85.met_dependencies = 
    [   MemoryDep('arg62_1', c1, {c0: 197, c1: 768}, None),
        MemoryDep('arg63_1', c1, {c0: 197, c1: 768}, None)]
buf85.users = [NodeUser(node=ExternKernelSchedulerNode(name='buf86'), can_inplace=False, is_weak=False)]
buf85.group.device = cuda:0
buf85.group.iteration = (151296, 1)
buf85.sizes = ([197, 768], [])
buf82_layout = FixedLayout('cuda', torch.float32, size=[1, 197, 1], stride=[197, 1, 197])
arg63_1_layout = FixedLayout('cuda', torch.float32, size=[768], stride=[1])
arg62_1_layout = FixedLayout('cuda', torch.float32, size=[768], stride=[1])
buf83_layout = FixedLayout('cuda', torch.float32, size=[1, 197, 1], stride=[197, 1, 197])
buf81_layout = FixedLayout('cuda', torch.float32, size=[1, 197, 768], stride=[151296, 768, 1])
buf85_layout = FixedLayout('cuda', torch.float32, size=[1, 197, 768], stride=[151296, 768, 1])
class buf85_loop_body:
    var_ranges = {z0: 197, z1: 768}
    index0 = 768*z0 + z1
    index1 = z0
    index2 = z1
    def body(self, ops):
        get_index = self.get_index('index0')
        load = ops.load('buf81', get_index)
        get_index_1 = self.get_index('index1')
        load_1 = ops.load('buf82', get_index_1)
        sub = ops.sub(load, load_1)
        get_index_2 = self.get_index('index1')
        load_2 = ops.load('buf83', get_index_2)
        constant = ops.constant(768.0, torch.float32)
        truediv = ops.truediv(load_2, constant)
        constant_1 = ops.constant(1e-12, torch.float32)
        add = ops.add(truediv, constant_1)
        rsqrt = ops.rsqrt(add)
        mul = ops.mul(sub, rsqrt)
        get_index_3 = self.get_index('index2')
        load_3 = ops.load('arg62_1', get_index_3)
        mul_1 = ops.mul(mul, load_3)
        get_index_4 = self.get_index('index2')
        load_4 = ops.load('arg63_1', get_index_4)
        add_1 = ops.add(mul_1, load_4)
        get_index_5 = self.get_index('index0')
        store = ops.store('buf85', get_index_5, add_1, None)
        return store
buf85 Triton code:
    import triton
    import triton.language as tl
    from triton.compiler.compiler import AttrsDescriptor

    from torch._inductor.runtime import triton_helpers, triton_heuristics
    from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
    from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, instance_descriptor, DeviceProperties

    @triton_heuristics.pointwise(
        size_hints=[262144], 
        filename=__file__,
        triton_meta={'signature': {0: '*fp32', 1: '*fp32', 2: '*fp32', 3: '*fp32', 4: '*fp32', 5: '*fp32', 6: 'i32'}, 'device': DeviceProperties(type='cuda', index=0, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, multi_processor_count=132), 'constants': {}, 'configs': [AttrsDescriptor(divisible_by_16=(0, 1, 2, 3, 4, 5, 6), equal_to_1=())]},
        inductor_meta={'autotune_hints': set(), 'kernel_name': 'Placeholder.DESCRIPTIVE_NAME', 'mutated_arg_names': [], 'no_x_dim': False, 'num_load': 5, 'num_reduction': 0, 'backend_hash': '3944FAAD9D18E987535CF029C4441E186640DD453AE5AE6831EDF736C09E0A47', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': False, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False},
        min_elem_per_thread=0
    )
    @triton.jit
    def triton_(in_ptr0, in_ptr1, in_ptr2, in_ptr3, in_ptr4, out_ptr0, xnumel, XBLOCK : tl.constexpr):
        xnumel = 151296
        xoffset = tl.program_id(0) * XBLOCK
        xindex = xoffset + tl.arange(0, XBLOCK)[:]
        xmask = xindex < xnumel
        x2 = xindex
        x1 = (xindex // 768)
        x0 = xindex % 768
        tmp0 = tl.load(in_ptr0 + (x2), xmask)
        tmp1 = tl.load(in_ptr1 + (x1), xmask, eviction_policy='evict_last')
        tmp3 = tl.load(in_ptr2 + (x1), xmask, eviction_policy='evict_last')
        tmp10 = tl.load(in_ptr3 + (x0), xmask, eviction_policy='evict_last')
        tmp12 = tl.load(in_ptr4 + (x0), xmask, eviction_policy='evict_last')
        tmp2 = tmp0 - tmp1
        tmp4 = 768.0
        tmp5 = tmp3 / tmp4
        tmp6 = 1e-12
        tmp7 = tmp5 + tmp6
        tmp8 = libdevice.rsqrt(tmp7)
        tmp9 = tmp2 * tmp8
        tmp11 = tmp9 * tmp10
        tmp13 = tmp11 + tmp12
        tl.store(out_ptr0 + (x2), tmp13, xmask)


buf86: ExternKernelSchedulerNode(ExternKernelOut)
buf86.writes = [StarDep(name='buf86', mode=None)]
buf86.unmet_dependencies = [StarDep(name='buf85', mode=None)]
buf86.met_dependencies = [StarDep(name='arg64_1', mode=None)]
buf86.users = [NodeUser(node=SchedulerNode(name='buf87'), can_inplace=True, is_weak=False)]
buf86.node.kernel = extern_kernels.mm


buf87: SchedulerNode(ComputedBuffer)
buf87.writes = [MemoryDep('buf87', c0, {c0: 605184}, None)]
buf87.unmet_dependencies = [MemoryDep('buf86', c0, {c0: 605184}, None)]
buf87.met_dependencies = [MemoryDep('arg65_1', c1, {c0: 197, c1: 3072}, None)]
buf87.users = [NodeUser(node=ExternKernelSchedulerNode(name='buf88'), can_inplace=False, is_weak=False)]
buf87.group.device = cuda:0
buf87.group.iteration = (605184, 1)
buf87.sizes = ([197, 3072], [])
buf86_layout = FixedLayout('cuda', torch.float32, size=[197, 3072], stride=[3072, 1])
arg65_1_layout = FixedLayout('cuda', torch.float32, size=[3072], stride=[1])
buf87_layout = FixedLayout('cuda', torch.float32, size=[1, 197, 3072], stride=[605184, 3072, 1])
class buf87_loop_body:
    var_ranges = {z0: 197, z1: 3072}
    index0 = 3072*z0 + z1
    index1 = z1
    def body(self, ops):
        get_index = self.get_index('index0')
        load = ops.load('buf86', get_index)
        get_index_1 = self.get_index('index1')
        load_1 = ops.load('arg65_1', get_index_1)
        add = ops.add(load, load_1)
        constant = ops.constant(0.5, torch.float32)
        mul = ops.mul(add, constant)
        get_index_2 = self.get_index('index0')
        load_2 = ops.load('buf86', get_index_2)
        get_index_3 = self.get_index('index1')
        load_3 = ops.load('arg65_1', get_index_3)
        add_1 = ops.add(load_2, load_3)
        constant_1 = ops.constant(0.7071067811865476, torch.float32)
        mul_1 = ops.mul(add_1, constant_1)
        erf = ops.erf(mul_1)
        constant_2 = ops.constant(1.0, torch.float32)
        add_2 = ops.add(erf, constant_2)
        mul_2 = ops.mul(mul, add_2)
        get_index_4 = self.get_index('index0')
        store = ops.store('buf87', get_index_4, mul_2, None)
        return store
buf87 Triton code:
    import triton
    import triton.language as tl
    from triton.compiler.compiler import AttrsDescriptor

    from torch._inductor.runtime import triton_helpers, triton_heuristics
    from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
    from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, instance_descriptor, DeviceProperties

    @triton_heuristics.pointwise(
        size_hints=[1048576], 
        filename=__file__,
        triton_meta={'signature': {0: '*fp32', 1: '*fp32', 2: 'i32'}, 'device': DeviceProperties(type='cuda', index=0, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, multi_processor_count=132), 'constants': {}, 'configs': [AttrsDescriptor(divisible_by_16=(0, 1, 2), equal_to_1=())]},
        inductor_meta={'autotune_hints': set(), 'kernel_name': 'Placeholder.DESCRIPTIVE_NAME', 'mutated_arg_names': ['in_out_ptr0'], 'no_x_dim': False, 'num_load': 2, 'num_reduction': 0, 'backend_hash': '3944FAAD9D18E987535CF029C4441E186640DD453AE5AE6831EDF736C09E0A47', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': False, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False},
        min_elem_per_thread=0
    )
    @triton.jit
    def triton_(in_out_ptr0, in_ptr0, xnumel, XBLOCK : tl.constexpr):
        xnumel = 605184
        xoffset = tl.program_id(0) * XBLOCK
        xindex = xoffset + tl.arange(0, XBLOCK)[:]
        xmask = xindex < xnumel
        x2 = xindex
        x0 = xindex % 3072
        tmp0 = tl.load(in_out_ptr0 + (x2), xmask)
        tmp1 = tl.load(in_ptr0 + (x0), xmask, eviction_policy='evict_last')
        tmp2 = tmp0 + tmp1
        tmp3 = 0.5
        tmp4 = tmp2 * tmp3
        tmp5 = 0.7071067811865476
        tmp6 = tmp2 * tmp5
        tmp7 = libdevice.erf(tmp6)
        tmp8 = 1.0
        tmp9 = tmp7 + tmp8
        tmp10 = tmp4 * tmp9
        tl.store(in_out_ptr0 + (x2), tmp10, xmask)


buf88: ExternKernelSchedulerNode(ExternKernelOut)
buf88.writes = [StarDep(name='buf88', mode=None)]
buf88.unmet_dependencies = [StarDep(name='buf87', mode=None)]
buf88.met_dependencies = [StarDep(name='arg66_1', mode=None)]
buf88.users = [NodeUser(node=SchedulerNode(name='buf89'), can_inplace=False, is_weak=False), NodeUser(node=SchedulerNode(name='buf90'), can_inplace=False, is_weak=False), NodeUser(node=SchedulerNode(name='buf91'), can_inplace=False, is_weak=False), NodeUser(node=SchedulerNode(name='buf92'), can_inplace=True, is_weak=False), NodeUser(node=SchedulerNode(name='buf102'), can_inplace=True, is_weak=False)]
buf88.node.kernel = extern_kernels.mm


buf89: SchedulerNode(ComputedBuffer)
buf89.writes = [MemoryDep('buf89', c0, {c0: 197}, None)]
buf89.unmet_dependencies = 
    [   MemoryDep('buf81', c0, {c0: 151296}, None),
        MemoryDep('buf88', c0, {c0: 151296}, None)]
buf89.met_dependencies = [MemoryDep('arg67_1', c1, {c0: 197, c1: 768}, None)]
buf89.users = [NodeUser(node=SchedulerNode(name='buf92'), can_inplace=False, is_weak=False)]
buf89.group.device = cuda:0
buf89.group.iteration = (197, 768)
buf89.sizes = ([197], [768])
arg67_1_layout = FixedLayout('cuda', torch.float32, size=[768], stride=[1])
buf88_layout = FixedLayout('cuda', torch.float32, size=[197, 768], stride=[768, 1])
buf81_layout = FixedLayout('cuda', torch.float32, size=[1, 197, 768], stride=[151296, 768, 1])
buf89_layout = FixedLayout('cuda', torch.float32, size=[1, 197, 1], stride=[197, 1, 197])
class buf89_loop_body:
    var_ranges = {z0: 197, z1: 768}
    index0 = 768*z0 + z1
    index1 = z1
    index2 = z0
    def body(self, ops):
        get_index = self.get_index('index0')
        load = ops.load('buf88', get_index)
        get_index_1 = self.get_index('index1')
        load_1 = ops.load('arg67_1', get_index_1)
        add = ops.add(load, load_1)
        get_index_2 = self.get_index('index0')
        load_2 = ops.load('buf81', get_index_2)
        add_1 = ops.add(add, load_2)
        reduction = ops.reduction(torch.float32, torch.float32, 'welford_reduce', add_1)
        getitem = reduction[0]
        getitem_1 = reduction[1]
        getitem_2 = reduction[2]
        get_index_3 = self.get_index('index2')
        store_reduction = ops.store_reduction('buf89', get_index_3, getitem)
        return store_reduction
buf89 Triton code:
    import triton
    import triton.language as tl
    from triton.compiler.compiler import AttrsDescriptor

    from torch._inductor.runtime import triton_helpers, triton_heuristics
    from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
    from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, instance_descriptor, DeviceProperties

    @triton_heuristics.persistent_reduction(
        size_hints=[256, 1024],
        reduction_hint=ReductionHint.INNER,
        filename=__file__,
        triton_meta={'signature': {0: '*fp32', 1: '*fp32', 2: '*fp32', 3: '*fp32', 4: 'i32', 5: 'i32'}, 'device': DeviceProperties(type='cuda', index=0, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, multi_processor_count=132), 'constants': {}, 'configs': [AttrsDescriptor(divisible_by_16=(0, 1, 2, 3, 5), equal_to_1=())]},
        inductor_meta={'autotune_hints': set(), 'kernel_name': 'Placeholder.DESCRIPTIVE_NAME', 'mutated_arg_names': [], 'no_x_dim': True, 'num_load': 3, 'num_reduction': 3, 'backend_hash': '3944FAAD9D18E987535CF029C4441E186640DD453AE5AE6831EDF736C09E0A47', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': False, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False}
    )
    @triton.jit
    def triton_(in_ptr0, in_ptr1, in_ptr2, out_ptr0, xnumel, rnumel):
        xnumel = 197
        XBLOCK: tl.constexpr = 1
        rnumel = 768
        RBLOCK: tl.constexpr = 1024
        xoffset = tl.program_id(0) * XBLOCK
        xindex = tl.full([1], xoffset, tl.int32)
        xmask = xindex < xnumel
        rindex = tl.arange(0, RBLOCK)[:]
        roffset = 0
        rmask = rindex < rnumel
        r1 = rindex
        x0 = xindex
        tmp0 = tl.load(in_ptr0 + (r1 + (768*x0)), rmask & xmask, other=0.0)
        tmp1 = tl.load(in_ptr1 + (r1), rmask, eviction_policy='evict_last', other=0.0)
        tmp3 = tl.load(in_ptr2 + (r1 + (768*x0)), rmask & xmask, other=0.0)
        tmp2 = tmp0 + tmp1
        tmp4 = tmp2 + tmp3
        tmp5 = tl.broadcast_to(tmp4, [RBLOCK])
        tmp7 = tl.where(rmask & xmask, tmp5, 0)
        tmp8 = tl.broadcast_to(tmp5, [RBLOCK])
        tmp10 = tl.where(rmask & xmask, tmp8, 0)
        tmp11 = triton_helpers.promote_to_tensor(tl.sum(tmp10, 0))
        tmp12 = tl.full([1], 768, tl.int32)
        tmp13 = tmp12.to(tl.float32)
        tmp14 = tmp11 / tmp13
        tmp15 = tmp5 - tmp14
        tmp16 = tmp15 * tmp15
        tmp17 = tl.broadcast_to(tmp16, [RBLOCK])
        tmp19 = tl.where(rmask & xmask, tmp17, 0)
        tmp20 = triton_helpers.promote_to_tensor(tl.sum(tmp19, 0))
        tl.store(out_ptr0 + (x0), tmp14, xmask)


buf90: SchedulerNode(ComputedBuffer)
buf90.writes = [MemoryDep('buf90', c0, {c0: 197}, None)]
buf90.unmet_dependencies = 
    [   MemoryDep('buf81', c0, {c0: 151296}, None),
        MemoryDep('buf88', c0, {c0: 151296}, None)]
buf90.met_dependencies = [MemoryDep('arg67_1', c1, {c0: 197, c1: 768}, None)]
buf90.users = [NodeUser(node=SchedulerNode(name='buf92'), can_inplace=False, is_weak=False)]
buf90.group.device = cuda:0
buf90.group.iteration = (197, 768)
buf90.sizes = ([197], [768])
arg67_1_layout = FixedLayout('cuda', torch.float32, size=[768], stride=[1])
buf88_layout = FixedLayout('cuda', torch.float32, size=[197, 768], stride=[768, 1])
buf81_layout = FixedLayout('cuda', torch.float32, size=[1, 197, 768], stride=[151296, 768, 1])
buf90_layout = FixedLayout('cuda', torch.float32, size=[1, 197, 1], stride=[197, 1, 197])
class buf90_loop_body:
    var_ranges = {z0: 197, z1: 768}
    index0 = 768*z0 + z1
    index1 = z1
    index2 = z0
    def body(self, ops):
        get_index = self.get_index('index0')
        load = ops.load('buf88', get_index)
        get_index_1 = self.get_index('index1')
        load_1 = ops.load('arg67_1', get_index_1)
        add = ops.add(load, load_1)
        get_index_2 = self.get_index('index0')
        load_2 = ops.load('buf81', get_index_2)
        add_1 = ops.add(add, load_2)
        reduction = ops.reduction(torch.float32, torch.float32, 'welford_reduce', add_1)
        getitem = reduction[0]
        getitem_1 = reduction[1]
        getitem_2 = reduction[2]
        get_index_3 = self.get_index('index2')
        store_reduction = ops.store_reduction('buf90', get_index_3, getitem_1)
        return store_reduction
buf90 Triton code:
    import triton
    import triton.language as tl
    from triton.compiler.compiler import AttrsDescriptor

    from torch._inductor.runtime import triton_helpers, triton_heuristics
    from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
    from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, instance_descriptor, DeviceProperties

    @triton_heuristics.persistent_reduction(
        size_hints=[256, 1024],
        reduction_hint=ReductionHint.INNER,
        filename=__file__,
        triton_meta={'signature': {0: '*fp32', 1: '*fp32', 2: '*fp32', 3: '*fp32', 4: 'i32', 5: 'i32'}, 'device': DeviceProperties(type='cuda', index=0, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, multi_processor_count=132), 'constants': {}, 'configs': [AttrsDescriptor(divisible_by_16=(0, 1, 2, 3, 5), equal_to_1=())]},
        inductor_meta={'autotune_hints': set(), 'kernel_name': 'Placeholder.DESCRIPTIVE_NAME', 'mutated_arg_names': [], 'no_x_dim': True, 'num_load': 3, 'num_reduction': 3, 'backend_hash': '3944FAAD9D18E987535CF029C4441E186640DD453AE5AE6831EDF736C09E0A47', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': False, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False}
    )
    @triton.jit
    def triton_(in_ptr0, in_ptr1, in_ptr2, out_ptr0, xnumel, rnumel):
        xnumel = 197
        XBLOCK: tl.constexpr = 1
        rnumel = 768
        RBLOCK: tl.constexpr = 1024
        xoffset = tl.program_id(0) * XBLOCK
        xindex = tl.full([1], xoffset, tl.int32)
        xmask = xindex < xnumel
        rindex = tl.arange(0, RBLOCK)[:]
        roffset = 0
        rmask = rindex < rnumel
        r1 = rindex
        x0 = xindex
        tmp0 = tl.load(in_ptr0 + (r1 + (768*x0)), rmask & xmask, other=0.0)
        tmp1 = tl.load(in_ptr1 + (r1), rmask, eviction_policy='evict_last', other=0.0)
        tmp3 = tl.load(in_ptr2 + (r1 + (768*x0)), rmask & xmask, other=0.0)
        tmp2 = tmp0 + tmp1
        tmp4 = tmp2 + tmp3
        tmp5 = tl.broadcast_to(tmp4, [RBLOCK])
        tmp7 = tl.where(rmask & xmask, tmp5, 0)
        tmp8 = tl.broadcast_to(tmp5, [RBLOCK])
        tmp10 = tl.where(rmask & xmask, tmp8, 0)
        tmp11 = triton_helpers.promote_to_tensor(tl.sum(tmp10, 0))
        tmp12 = tl.full([1], 768, tl.int32)
        tmp13 = tmp12.to(tl.float32)
        tmp14 = tmp11 / tmp13
        tmp15 = tmp5 - tmp14
        tmp16 = tmp15 * tmp15
        tmp17 = tl.broadcast_to(tmp16, [RBLOCK])
        tmp19 = tl.where(rmask & xmask, tmp17, 0)
        tmp20 = triton_helpers.promote_to_tensor(tl.sum(tmp19, 0))
        tl.store(out_ptr0 + (x0), tmp20, xmask)


buf92: SchedulerNode(ComputedBuffer)
buf92.writes = [MemoryDep('buf92', c0, {c0: 151296}, None)]
buf92.unmet_dependencies = 
    [   MemoryDep('buf81', c0, {c0: 151296}, None),
        MemoryDep('buf88', c0, {c0: 151296}, None),
        MemoryDep('buf89', c0, {c0: 197}, None),
        MemoryDep('buf90', c0, {c0: 197}, None)]
buf92.met_dependencies = 
    [   MemoryDep('arg67_1', c1, {c0: 197, c1: 768}, None),
        MemoryDep('arg68_1', c1, {c0: 197, c1: 768}, None),
        MemoryDep('arg69_1', c1, {c0: 197, c1: 768}, None)]
buf92.users = [NodeUser(node=ExternKernelSchedulerNode(name='buf93'), can_inplace=False, is_weak=False), NodeUser(node=ExternKernelSchedulerNode(name='buf94'), can_inplace=False, is_weak=False), NodeUser(node=ExternKernelSchedulerNode(name='buf95'), can_inplace=False, is_weak=False)]
buf92.group.device = cuda:0
buf92.group.iteration = (151296, 1)
buf92.sizes = ([197, 768], [])
buf90_layout = FixedLayout('cuda', torch.float32, size=[1, 197, 1], stride=[197, 1, 197])
arg68_1_layout = FixedLayout('cuda', torch.float32, size=[768], stride=[1])
buf88_layout = FixedLayout('cuda', torch.float32, size=[197, 768], stride=[768, 1])
buf89_layout = FixedLayout('cuda', torch.float32, size=[1, 197, 1], stride=[197, 1, 197])
arg69_1_layout = FixedLayout('cuda', torch.float32, size=[768], stride=[1])
arg67_1_layout = FixedLayout('cuda', torch.float32, size=[768], stride=[1])
buf81_layout = FixedLayout('cuda', torch.float32, size=[1, 197, 768], stride=[151296, 768, 1])
buf92_layout = FixedLayout('cuda', torch.float32, size=[1, 197, 768], stride=[151296, 768, 1])
class buf92_loop_body:
    var_ranges = {z0: 197, z1: 768}
    index0 = 768*z0 + z1
    index1 = z1
    index2 = z0
    def body(self, ops):
        get_index = self.get_index('index0')
        load = ops.load('buf88', get_index)
        get_index_1 = self.get_index('index1')
        load_1 = ops.load('arg67_1', get_index_1)
        add = ops.add(load, load_1)
        get_index_2 = self.get_index('index0')
        load_2 = ops.load('buf81', get_index_2)
        add_1 = ops.add(add, load_2)
        get_index_3 = self.get_index('index2')
        load_3 = ops.load('buf89', get_index_3)
        sub = ops.sub(add_1, load_3)
        get_index_4 = self.get_index('index2')
        load_4 = ops.load('buf90', get_index_4)
        constant = ops.constant(768.0, torch.float32)
        truediv = ops.truediv(load_4, constant)
        constant_1 = ops.constant(1e-12, torch.float32)
        add_2 = ops.add(truediv, constant_1)
        rsqrt = ops.rsqrt(add_2)
        mul = ops.mul(sub, rsqrt)
        get_index_5 = self.get_index('index1')
        load_5 = ops.load('arg68_1', get_index_5)
        mul_1 = ops.mul(mul, load_5)
        get_index_6 = self.get_index('index1')
        load_6 = ops.load('arg69_1', get_index_6)
        add_3 = ops.add(mul_1, load_6)
        get_index_7 = self.get_index('index0')
        store = ops.store('buf92', get_index_7, add_3, None)
        return store
buf92 Triton code:
    import triton
    import triton.language as tl
    from triton.compiler.compiler import AttrsDescriptor

    from torch._inductor.runtime import triton_helpers, triton_heuristics
    from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
    from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, instance_descriptor, DeviceProperties

    @triton_heuristics.pointwise(
        size_hints=[262144], 
        filename=__file__,
        triton_meta={'signature': {0: '*fp32', 1: '*fp32', 2: '*fp32', 3: '*fp32', 4: '*fp32', 5: '*fp32', 6: '*fp32', 7: '*fp32', 8: 'i32'}, 'device': DeviceProperties(type='cuda', index=0, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, multi_processor_count=132), 'constants': {}, 'configs': [AttrsDescriptor(divisible_by_16=(0, 1, 2, 3, 4, 5, 6, 7, 8), equal_to_1=())]},
        inductor_meta={'autotune_hints': set(), 'kernel_name': 'Placeholder.DESCRIPTIVE_NAME', 'mutated_arg_names': [], 'no_x_dim': False, 'num_load': 7, 'num_reduction': 0, 'backend_hash': '3944FAAD9D18E987535CF029C4441E186640DD453AE5AE6831EDF736C09E0A47', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': False, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False},
        min_elem_per_thread=0
    )
    @triton.jit
    def triton_(in_ptr0, in_ptr1, in_ptr2, in_ptr3, in_ptr4, in_ptr5, in_ptr6, out_ptr0, xnumel, XBLOCK : tl.constexpr):
        xnumel = 151296
        xoffset = tl.program_id(0) * XBLOCK
        xindex = xoffset + tl.arange(0, XBLOCK)[:]
        xmask = xindex < xnumel
        x2 = xindex
        x0 = xindex % 768
        x1 = (xindex // 768)
        tmp0 = tl.load(in_ptr0 + (x2), xmask)
        tmp1 = tl.load(in_ptr1 + (x0), xmask, eviction_policy='evict_last')
        tmp3 = tl.load(in_ptr2 + (x2), xmask)
        tmp5 = tl.load(in_ptr3 + (x1), xmask, eviction_policy='evict_last')
        tmp7 = tl.load(in_ptr4 + (x1), xmask, eviction_policy='evict_last')
        tmp14 = tl.load(in_ptr5 + (x0), xmask, eviction_policy='evict_last')
        tmp16 = tl.load(in_ptr6 + (x0), xmask, eviction_policy='evict_last')
        tmp2 = tmp0 + tmp1
        tmp4 = tmp2 + tmp3
        tmp6 = tmp4 - tmp5
        tmp8 = 768.0
        tmp9 = tmp7 / tmp8
        tmp10 = 1e-12
        tmp11 = tmp9 + tmp10
        tmp12 = libdevice.rsqrt(tmp11)
        tmp13 = tmp6 * tmp12
        tmp15 = tmp13 * tmp14
        tmp17 = tmp15 + tmp16
        tl.store(out_ptr0 + (x2), tmp17, xmask)


buf93: ExternKernelSchedulerNode(ExternKernelOut)
buf93.writes = [StarDep(name='buf93', mode=None)]
buf93.unmet_dependencies = [StarDep(name='buf92', mode=None)]
buf93.met_dependencies = [StarDep(name='arg70_1', mode=None), StarDep(name='arg71_1', mode=None)]
buf93.users = [NodeUser(node=ExternKernelSchedulerNode(name='buf96'), can_inplace=False, is_weak=False)]
buf93.node.kernel = extern_kernels.addmm


buf94: ExternKernelSchedulerNode(ExternKernelOut)
buf94.writes = [StarDep(name='buf94', mode=None)]
buf94.unmet_dependencies = [StarDep(name='buf92', mode=None)]
buf94.met_dependencies = [StarDep(name='arg72_1', mode=None), StarDep(name='arg73_1', mode=None)]
buf94.users = [NodeUser(node=ExternKernelSchedulerNode(name='buf96'), can_inplace=False, is_weak=False)]
buf94.node.kernel = extern_kernels.addmm


buf95: ExternKernelSchedulerNode(ExternKernelOut)
buf95.writes = [StarDep(name='buf95', mode=None)]
buf95.unmet_dependencies = [StarDep(name='buf92', mode=None)]
buf95.met_dependencies = [StarDep(name='arg74_1', mode=None), StarDep(name='arg75_1', mode=None)]
buf95.users = [NodeUser(node=ExternKernelSchedulerNode(name='buf96'), can_inplace=False, is_weak=False)]
buf95.node.kernel = extern_kernels.addmm


buf96: ExternKernelSchedulerNode(FallbackKernel)
buf96.writes = [StarDep(name='buf96', mode=None)]
buf96.unmet_dependencies = 
    [   StarDep(name='buf93', mode=None),
        StarDep(name='buf94', mode=None),
        StarDep(name='buf95', mode=None)]
buf96.met_dependencies = []
buf96.users = [NodeUser(node=ExternKernelSchedulerNode(name='buf97'), can_inplace=False, is_weak=False), NodeUser(node=ExternKernelSchedulerNode(name='buf98'), can_inplace=False, is_weak=False), NodeUser(node=ExternKernelSchedulerNode(name='buf99'), can_inplace=False, is_weak=False), NodeUser(node=ExternKernelSchedulerNode(name='buf100'), can_inplace=False, is_weak=False)]
buf96.node.kernel = None


buf97: ExternKernelSchedulerNode(MultiOutput)
buf97.writes = [StarDep(name='buf97', mode=None)]
buf97.unmet_dependencies = [StarDep(name='buf96', mode=None)]
buf97.met_dependencies = []
buf97.users = [NodeUser(node=ExternKernelSchedulerNode(name='buf101'), can_inplace=False, is_weak=False)]
buf97.node.kernel = None


buf101: ExternKernelSchedulerNode(ExternKernelOut)
buf101.writes = [StarDep(name='buf101', mode=None)]
buf101.unmet_dependencies = [StarDep(name='buf97', mode=None)]
buf101.met_dependencies = [StarDep(name='arg76_1', mode=None)]
buf101.users = [NodeUser(node=SchedulerNode(name='buf102'), can_inplace=True, is_weak=False)]
buf101.node.kernel = extern_kernels.mm


buf102: SchedulerNode(ComputedBuffer)
buf102.writes = [MemoryDep('buf102', c0, {c0: 151296}, None)]
buf102.unmet_dependencies = 
    [   MemoryDep('buf101', c0, {c0: 151296}, None),
        MemoryDep('buf81', c0, {c0: 151296}, None),
        MemoryDep('buf88', c0, {c0: 151296}, None)]
buf102.met_dependencies = 
    [   MemoryDep('arg67_1', c1, {c0: 197, c1: 768}, None),
        MemoryDep('arg77_1', c1, {c0: 197, c1: 768}, None)]
buf102.users = [NodeUser(node=SchedulerNode(name='buf103'), can_inplace=False, is_weak=False), NodeUser(node=SchedulerNode(name='buf104'), can_inplace=False, is_weak=False), NodeUser(node=SchedulerNode(name='buf105'), can_inplace=False, is_weak=False), NodeUser(node=SchedulerNode(name='buf106'), can_inplace=True, is_weak=False), NodeUser(node=SchedulerNode(name='buf110'), can_inplace=False, is_weak=False), NodeUser(node=SchedulerNode(name='buf111'), can_inplace=False, is_weak=False), NodeUser(node=SchedulerNode(name='buf112'), can_inplace=False, is_weak=False), NodeUser(node=SchedulerNode(name='buf113'), can_inplace=True, is_weak=False), NodeUser(node=SchedulerNode(name='buf123'), can_inplace=True, is_weak=False)]
buf102.group.device = cuda:0
buf102.group.iteration = (151296, 1)
buf102.sizes = ([197, 768], [])
buf88_layout = FixedLayout('cuda', torch.float32, size=[197, 768], stride=[768, 1])
buf101_layout = FixedLayout('cuda', torch.float32, size=[197, 768], stride=[768, 1])
arg67_1_layout = FixedLayout('cuda', torch.float32, size=[768], stride=[1])
arg77_1_layout = FixedLayout('cuda', torch.float32, size=[768], stride=[1])
buf81_layout = FixedLayout('cuda', torch.float32, size=[1, 197, 768], stride=[151296, 768, 1])
buf102_layout = FixedLayout('cuda', torch.float32, size=[1, 197, 768], stride=[151296, 768, 1])
class buf102_loop_body:
    var_ranges = {z0: 197, z1: 768}
    index0 = 768*z0 + z1
    index1 = z1
    def body(self, ops):
        get_index = self.get_index('index0')
        load = ops.load('buf101', get_index)
        get_index_1 = self.get_index('index1')
        load_1 = ops.load('arg77_1', get_index_1)
        add = ops.add(load, load_1)
        get_index_2 = self.get_index('index0')
        load_2 = ops.load('buf88', get_index_2)
        get_index_3 = self.get_index('index1')
        load_3 = ops.load('arg67_1', get_index_3)
        add_1 = ops.add(load_2, load_3)
        get_index_4 = self.get_index('index0')
        load_4 = ops.load('buf81', get_index_4)
        add_2 = ops.add(add_1, load_4)
        add_3 = ops.add(add, add_2)
        get_index_5 = self.get_index('index0')
        store = ops.store('buf102', get_index_5, add_3, None)
        return store
buf102 Triton code:
    import triton
    import triton.language as tl
    from triton.compiler.compiler import AttrsDescriptor

    from torch._inductor.runtime import triton_helpers, triton_heuristics
    from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
    from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, instance_descriptor, DeviceProperties

    @triton_heuristics.pointwise(
        size_hints=[262144], 
        filename=__file__,
        triton_meta={'signature': {0: '*fp32', 1: '*fp32', 2: '*fp32', 3: '*fp32', 4: '*fp32', 5: 'i32'}, 'device': DeviceProperties(type='cuda', index=0, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, multi_processor_count=132), 'constants': {}, 'configs': [AttrsDescriptor(divisible_by_16=(0, 1, 2, 3, 4, 5), equal_to_1=())]},
        inductor_meta={'autotune_hints': set(), 'kernel_name': 'Placeholder.DESCRIPTIVE_NAME', 'mutated_arg_names': ['in_out_ptr0'], 'no_x_dim': False, 'num_load': 5, 'num_reduction': 0, 'backend_hash': '3944FAAD9D18E987535CF029C4441E186640DD453AE5AE6831EDF736C09E0A47', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': False, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False},
        min_elem_per_thread=0
    )
    @triton.jit
    def triton_(in_out_ptr0, in_ptr0, in_ptr1, in_ptr2, in_ptr3, xnumel, XBLOCK : tl.constexpr):
        xnumel = 151296
        xoffset = tl.program_id(0) * XBLOCK
        xindex = xoffset + tl.arange(0, XBLOCK)[:]
        xmask = xindex < xnumel
        x2 = xindex
        x0 = xindex % 768
        tmp0 = tl.load(in_out_ptr0 + (x2), xmask)
        tmp1 = tl.load(in_ptr0 + (x0), xmask, eviction_policy='evict_last')
        tmp3 = tl.load(in_ptr1 + (x2), xmask)
        tmp4 = tl.load(in_ptr2 + (x0), xmask, eviction_policy='evict_last')
        tmp6 = tl.load(in_ptr3 + (x2), xmask)
        tmp2 = tmp0 + tmp1
        tmp5 = tmp3 + tmp4
        tmp7 = tmp5 + tmp6
        tmp8 = tmp2 + tmp7
        tl.store(in_out_ptr0 + (x2), tmp8, xmask)


buf103: SchedulerNode(ComputedBuffer)
buf103.writes = [MemoryDep('buf103', c0, {c0: 197}, None)]
buf103.unmet_dependencies = [MemoryDep('buf102', c0, {c0: 151296}, None)]
buf103.met_dependencies = []
buf103.users = [NodeUser(node=SchedulerNode(name='buf106'), can_inplace=False, is_weak=False)]
buf103.group.device = cuda:0
buf103.group.iteration = (197, 768)
buf103.sizes = ([197], [768])
buf102_layout = FixedLayout('cuda', torch.float32, size=[1, 197, 768], stride=[151296, 768, 1])
buf103_layout = FixedLayout('cuda', torch.float32, size=[1, 197, 1], stride=[197, 1, 197])
class buf103_loop_body:
    var_ranges = {z0: 197, z1: 768}
    index0 = 768*z0 + z1
    index1 = z0
    def body(self, ops):
        get_index = self.get_index('index0')
        load = ops.load('buf102', get_index)
        reduction = ops.reduction(torch.float32, torch.float32, 'welford_reduce', load)
        getitem = reduction[0]
        getitem_1 = reduction[1]
        getitem_2 = reduction[2]
        get_index_1 = self.get_index('index1')
        store_reduction = ops.store_reduction('buf103', get_index_1, getitem)
        return store_reduction
buf103 Triton code:
    import triton
    import triton.language as tl
    from triton.compiler.compiler import AttrsDescriptor

    from torch._inductor.runtime import triton_helpers, triton_heuristics
    from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
    from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, instance_descriptor, DeviceProperties

    @triton_heuristics.persistent_reduction(
        size_hints=[256, 1024],
        reduction_hint=ReductionHint.INNER,
        filename=__file__,
        triton_meta={'signature': {0: '*fp32', 1: '*fp32', 2: 'i32', 3: 'i32'}, 'device': DeviceProperties(type='cuda', index=0, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, multi_processor_count=132), 'constants': {}, 'configs': [AttrsDescriptor(divisible_by_16=(0, 1, 3), equal_to_1=())]},
        inductor_meta={'autotune_hints': set(), 'kernel_name': 'Placeholder.DESCRIPTIVE_NAME', 'mutated_arg_names': [], 'no_x_dim': True, 'num_load': 1, 'num_reduction': 3, 'backend_hash': '3944FAAD9D18E987535CF029C4441E186640DD453AE5AE6831EDF736C09E0A47', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': False, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False}
    )
    @triton.jit
    def triton_(in_ptr0, out_ptr0, xnumel, rnumel):
        xnumel = 197
        XBLOCK: tl.constexpr = 1
        rnumel = 768
        RBLOCK: tl.constexpr = 1024
        xoffset = tl.program_id(0) * XBLOCK
        xindex = tl.full([1], xoffset, tl.int32)
        xmask = xindex < xnumel
        rindex = tl.arange(0, RBLOCK)[:]
        roffset = 0
        rmask = rindex < rnumel
        r1 = rindex
        x0 = xindex
        tmp0 = tl.load(in_ptr0 + (r1 + (768*x0)), rmask & xmask, other=0.0)
        tmp1 = tl.broadcast_to(tmp0, [RBLOCK])
        tmp3 = tl.where(rmask & xmask, tmp1, 0)
        tmp4 = tl.broadcast_to(tmp1, [RBLOCK])
        tmp6 = tl.where(rmask & xmask, tmp4, 0)
        tmp7 = triton_helpers.promote_to_tensor(tl.sum(tmp6, 0))
        tmp8 = tl.full([1], 768, tl.int32)
        tmp9 = tmp8.to(tl.float32)
        tmp10 = tmp7 / tmp9
        tmp11 = tmp1 - tmp10
        tmp12 = tmp11 * tmp11
        tmp13 = tl.broadcast_to(tmp12, [RBLOCK])
        tmp15 = tl.where(rmask & xmask, tmp13, 0)
        tmp16 = triton_helpers.promote_to_tensor(tl.sum(tmp15, 0))
        tl.store(out_ptr0 + (x0), tmp10, xmask)


buf104: SchedulerNode(ComputedBuffer)
buf104.writes = [MemoryDep('buf104', c0, {c0: 197}, None)]
buf104.unmet_dependencies = [MemoryDep('buf102', c0, {c0: 151296}, None)]
buf104.met_dependencies = []
buf104.users = [NodeUser(node=SchedulerNode(name='buf106'), can_inplace=False, is_weak=False)]
buf104.group.device = cuda:0
buf104.group.iteration = (197, 768)
buf104.sizes = ([197], [768])
buf102_layout = FixedLayout('cuda', torch.float32, size=[1, 197, 768], stride=[151296, 768, 1])
buf104_layout = FixedLayout('cuda', torch.float32, size=[1, 197, 1], stride=[197, 1, 197])
class buf104_loop_body:
    var_ranges = {z0: 197, z1: 768}
    index0 = 768*z0 + z1
    index1 = z0
    def body(self, ops):
        get_index = self.get_index('index0')
        load = ops.load('buf102', get_index)
        reduction = ops.reduction(torch.float32, torch.float32, 'welford_reduce', load)
        getitem = reduction[0]
        getitem_1 = reduction[1]
        getitem_2 = reduction[2]
        get_index_1 = self.get_index('index1')
        store_reduction = ops.store_reduction('buf104', get_index_1, getitem_1)
        return store_reduction
buf104 Triton code:
    import triton
    import triton.language as tl
    from triton.compiler.compiler import AttrsDescriptor

    from torch._inductor.runtime import triton_helpers, triton_heuristics
    from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
    from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, instance_descriptor, DeviceProperties

    @triton_heuristics.persistent_reduction(
        size_hints=[256, 1024],
        reduction_hint=ReductionHint.INNER,
        filename=__file__,
        triton_meta={'signature': {0: '*fp32', 1: '*fp32', 2: 'i32', 3: 'i32'}, 'device': DeviceProperties(type='cuda', index=0, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, multi_processor_count=132), 'constants': {}, 'configs': [AttrsDescriptor(divisible_by_16=(0, 1, 3), equal_to_1=())]},
        inductor_meta={'autotune_hints': set(), 'kernel_name': 'Placeholder.DESCRIPTIVE_NAME', 'mutated_arg_names': [], 'no_x_dim': True, 'num_load': 1, 'num_reduction': 3, 'backend_hash': '3944FAAD9D18E987535CF029C4441E186640DD453AE5AE6831EDF736C09E0A47', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': False, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False}
    )
    @triton.jit
    def triton_(in_ptr0, out_ptr0, xnumel, rnumel):
        xnumel = 197
        XBLOCK: tl.constexpr = 1
        rnumel = 768
        RBLOCK: tl.constexpr = 1024
        xoffset = tl.program_id(0) * XBLOCK
        xindex = tl.full([1], xoffset, tl.int32)
        xmask = xindex < xnumel
        rindex = tl.arange(0, RBLOCK)[:]
        roffset = 0
        rmask = rindex < rnumel
        r1 = rindex
        x0 = xindex
        tmp0 = tl.load(in_ptr0 + (r1 + (768*x0)), rmask & xmask, other=0.0)
        tmp1 = tl.broadcast_to(tmp0, [RBLOCK])
        tmp3 = tl.where(rmask & xmask, tmp1, 0)
        tmp4 = tl.broadcast_to(tmp1, [RBLOCK])
        tmp6 = tl.where(rmask & xmask, tmp4, 0)
        tmp7 = triton_helpers.promote_to_tensor(tl.sum(tmp6, 0))
        tmp8 = tl.full([1], 768, tl.int32)
        tmp9 = tmp8.to(tl.float32)
        tmp10 = tmp7 / tmp9
        tmp11 = tmp1 - tmp10
        tmp12 = tmp11 * tmp11
        tmp13 = tl.broadcast_to(tmp12, [RBLOCK])
        tmp15 = tl.where(rmask & xmask, tmp13, 0)
        tmp16 = triton_helpers.promote_to_tensor(tl.sum(tmp15, 0))
        tl.store(out_ptr0 + (x0), tmp16, xmask)


buf106: SchedulerNode(ComputedBuffer)
buf106.writes = [MemoryDep('buf106', c0, {c0: 151296}, None)]
buf106.unmet_dependencies = 
    [   MemoryDep('buf102', c0, {c0: 151296}, None),
        MemoryDep('buf103', c0, {c0: 197}, None),
        MemoryDep('buf104', c0, {c0: 197}, None)]
buf106.met_dependencies = 
    [   MemoryDep('arg78_1', c1, {c0: 197, c1: 768}, None),
        MemoryDep('arg79_1', c1, {c0: 197, c1: 768}, None)]
buf106.users = [NodeUser(node=ExternKernelSchedulerNode(name='buf107'), can_inplace=False, is_weak=False)]
buf106.group.device = cuda:0
buf106.group.iteration = (151296, 1)
buf106.sizes = ([197, 768], [])
arg78_1_layout = FixedLayout('cuda', torch.float32, size=[768], stride=[1])
buf104_layout = FixedLayout('cuda', torch.float32, size=[1, 197, 1], stride=[197, 1, 197])
buf102_layout = FixedLayout('cuda', torch.float32, size=[1, 197, 768], stride=[151296, 768, 1])
buf103_layout = FixedLayout('cuda', torch.float32, size=[1, 197, 1], stride=[197, 1, 197])
arg79_1_layout = FixedLayout('cuda', torch.float32, size=[768], stride=[1])
buf106_layout = FixedLayout('cuda', torch.float32, size=[1, 197, 768], stride=[151296, 768, 1])
class buf106_loop_body:
    var_ranges = {z0: 197, z1: 768}
    index0 = 768*z0 + z1
    index1 = z0
    index2 = z1
    def body(self, ops):
        get_index = self.get_index('index0')
        load = ops.load('buf102', get_index)
        get_index_1 = self.get_index('index1')
        load_1 = ops.load('buf103', get_index_1)
        sub = ops.sub(load, load_1)
        get_index_2 = self.get_index('index1')
        load_2 = ops.load('buf104', get_index_2)
        constant = ops.constant(768.0, torch.float32)
        truediv = ops.truediv(load_2, constant)
        constant_1 = ops.constant(1e-12, torch.float32)
        add = ops.add(truediv, constant_1)
        rsqrt = ops.rsqrt(add)
        mul = ops.mul(sub, rsqrt)
        get_index_3 = self.get_index('index2')
        load_3 = ops.load('arg78_1', get_index_3)
        mul_1 = ops.mul(mul, load_3)
        get_index_4 = self.get_index('index2')
        load_4 = ops.load('arg79_1', get_index_4)
        add_1 = ops.add(mul_1, load_4)
        get_index_5 = self.get_index('index0')
        store = ops.store('buf106', get_index_5, add_1, None)
        return store
buf106 Triton code:
    import triton
    import triton.language as tl
    from triton.compiler.compiler import AttrsDescriptor

    from torch._inductor.runtime import triton_helpers, triton_heuristics
    from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
    from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, instance_descriptor, DeviceProperties

    @triton_heuristics.pointwise(
        size_hints=[262144], 
        filename=__file__,
        triton_meta={'signature': {0: '*fp32', 1: '*fp32', 2: '*fp32', 3: '*fp32', 4: '*fp32', 5: '*fp32', 6: 'i32'}, 'device': DeviceProperties(type='cuda', index=0, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, multi_processor_count=132), 'constants': {}, 'configs': [AttrsDescriptor(divisible_by_16=(0, 1, 2, 3, 4, 5, 6), equal_to_1=())]},
        inductor_meta={'autotune_hints': set(), 'kernel_name': 'Placeholder.DESCRIPTIVE_NAME', 'mutated_arg_names': [], 'no_x_dim': False, 'num_load': 5, 'num_reduction': 0, 'backend_hash': '3944FAAD9D18E987535CF029C4441E186640DD453AE5AE6831EDF736C09E0A47', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': False, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False},
        min_elem_per_thread=0
    )
    @triton.jit
    def triton_(in_ptr0, in_ptr1, in_ptr2, in_ptr3, in_ptr4, out_ptr0, xnumel, XBLOCK : tl.constexpr):
        xnumel = 151296
        xoffset = tl.program_id(0) * XBLOCK
        xindex = xoffset + tl.arange(0, XBLOCK)[:]
        xmask = xindex < xnumel
        x2 = xindex
        x1 = (xindex // 768)
        x0 = xindex % 768
        tmp0 = tl.load(in_ptr0 + (x2), xmask)
        tmp1 = tl.load(in_ptr1 + (x1), xmask, eviction_policy='evict_last')
        tmp3 = tl.load(in_ptr2 + (x1), xmask, eviction_policy='evict_last')
        tmp10 = tl.load(in_ptr3 + (x0), xmask, eviction_policy='evict_last')
        tmp12 = tl.load(in_ptr4 + (x0), xmask, eviction_policy='evict_last')
        tmp2 = tmp0 - tmp1
        tmp4 = 768.0
        tmp5 = tmp3 / tmp4
        tmp6 = 1e-12
        tmp7 = tmp5 + tmp6
        tmp8 = libdevice.rsqrt(tmp7)
        tmp9 = tmp2 * tmp8
        tmp11 = tmp9 * tmp10
        tmp13 = tmp11 + tmp12
        tl.store(out_ptr0 + (x2), tmp13, xmask)


buf107: ExternKernelSchedulerNode(ExternKernelOut)
buf107.writes = [StarDep(name='buf107', mode=None)]
buf107.unmet_dependencies = [StarDep(name='buf106', mode=None)]
buf107.met_dependencies = [StarDep(name='arg80_1', mode=None)]
buf107.users = [NodeUser(node=SchedulerNode(name='buf108'), can_inplace=True, is_weak=False)]
buf107.node.kernel = extern_kernels.mm


buf108: SchedulerNode(ComputedBuffer)
buf108.writes = [MemoryDep('buf108', c0, {c0: 605184}, None)]
buf108.unmet_dependencies = [MemoryDep('buf107', c0, {c0: 605184}, None)]
buf108.met_dependencies = [MemoryDep('arg81_1', c1, {c0: 197, c1: 3072}, None)]
buf108.users = [NodeUser(node=ExternKernelSchedulerNode(name='buf109'), can_inplace=False, is_weak=False)]
buf108.group.device = cuda:0
buf108.group.iteration = (605184, 1)
buf108.sizes = ([197, 3072], [])
arg81_1_layout = FixedLayout('cuda', torch.float32, size=[3072], stride=[1])
buf107_layout = FixedLayout('cuda', torch.float32, size=[197, 3072], stride=[3072, 1])
buf108_layout = FixedLayout('cuda', torch.float32, size=[1, 197, 3072], stride=[605184, 3072, 1])
class buf108_loop_body:
    var_ranges = {z0: 197, z1: 3072}
    index0 = 3072*z0 + z1
    index1 = z1
    def body(self, ops):
        get_index = self.get_index('index0')
        load = ops.load('buf107', get_index)
        get_index_1 = self.get_index('index1')
        load_1 = ops.load('arg81_1', get_index_1)
        add = ops.add(load, load_1)
        constant = ops.constant(0.5, torch.float32)
        mul = ops.mul(add, constant)
        get_index_2 = self.get_index('index0')
        load_2 = ops.load('buf107', get_index_2)
        get_index_3 = self.get_index('index1')
        load_3 = ops.load('arg81_1', get_index_3)
        add_1 = ops.add(load_2, load_3)
        constant_1 = ops.constant(0.7071067811865476, torch.float32)
        mul_1 = ops.mul(add_1, constant_1)
        erf = ops.erf(mul_1)
        constant_2 = ops.constant(1.0, torch.float32)
        add_2 = ops.add(erf, constant_2)
        mul_2 = ops.mul(mul, add_2)
        get_index_4 = self.get_index('index0')
        store = ops.store('buf108', get_index_4, mul_2, None)
        return store
buf108 Triton code:
    import triton
    import triton.language as tl
    from triton.compiler.compiler import AttrsDescriptor

    from torch._inductor.runtime import triton_helpers, triton_heuristics
    from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
    from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, instance_descriptor, DeviceProperties

    @triton_heuristics.pointwise(
        size_hints=[1048576], 
        filename=__file__,
        triton_meta={'signature': {0: '*fp32', 1: '*fp32', 2: 'i32'}, 'device': DeviceProperties(type='cuda', index=0, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, multi_processor_count=132), 'constants': {}, 'configs': [AttrsDescriptor(divisible_by_16=(0, 1, 2), equal_to_1=())]},
        inductor_meta={'autotune_hints': set(), 'kernel_name': 'Placeholder.DESCRIPTIVE_NAME', 'mutated_arg_names': ['in_out_ptr0'], 'no_x_dim': False, 'num_load': 2, 'num_reduction': 0, 'backend_hash': '3944FAAD9D18E987535CF029C4441E186640DD453AE5AE6831EDF736C09E0A47', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': False, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False},
        min_elem_per_thread=0
    )
    @triton.jit
    def triton_(in_out_ptr0, in_ptr0, xnumel, XBLOCK : tl.constexpr):
        xnumel = 605184
        xoffset = tl.program_id(0) * XBLOCK
        xindex = xoffset + tl.arange(0, XBLOCK)[:]
        xmask = xindex < xnumel
        x2 = xindex
        x0 = xindex % 3072
        tmp0 = tl.load(in_out_ptr0 + (x2), xmask)
        tmp1 = tl.load(in_ptr0 + (x0), xmask, eviction_policy='evict_last')
        tmp2 = tmp0 + tmp1
        tmp3 = 0.5
        tmp4 = tmp2 * tmp3
        tmp5 = 0.7071067811865476
        tmp6 = tmp2 * tmp5
        tmp7 = libdevice.erf(tmp6)
        tmp8 = 1.0
        tmp9 = tmp7 + tmp8
        tmp10 = tmp4 * tmp9
        tl.store(in_out_ptr0 + (x2), tmp10, xmask)


buf109: ExternKernelSchedulerNode(ExternKernelOut)
buf109.writes = [StarDep(name='buf109', mode=None)]
buf109.unmet_dependencies = [StarDep(name='buf108', mode=None)]
buf109.met_dependencies = [StarDep(name='arg82_1', mode=None)]
buf109.users = [NodeUser(node=SchedulerNode(name='buf110'), can_inplace=False, is_weak=False), NodeUser(node=SchedulerNode(name='buf111'), can_inplace=False, is_weak=False), NodeUser(node=SchedulerNode(name='buf112'), can_inplace=False, is_weak=False), NodeUser(node=SchedulerNode(name='buf113'), can_inplace=True, is_weak=False), NodeUser(node=SchedulerNode(name='buf123'), can_inplace=True, is_weak=False)]
buf109.node.kernel = extern_kernels.mm


buf110: SchedulerNode(ComputedBuffer)
buf110.writes = [MemoryDep('buf110', c0, {c0: 197}, None)]
buf110.unmet_dependencies = 
    [   MemoryDep('buf102', c0, {c0: 151296}, None),
        MemoryDep('buf109', c0, {c0: 151296}, None)]
buf110.met_dependencies = [MemoryDep('arg83_1', c1, {c0: 197, c1: 768}, None)]
buf110.users = [NodeUser(node=SchedulerNode(name='buf113'), can_inplace=False, is_weak=False)]
buf110.group.device = cuda:0
buf110.group.iteration = (197, 768)
buf110.sizes = ([197], [768])
arg83_1_layout = FixedLayout('cuda', torch.float32, size=[768], stride=[1])
buf102_layout = FixedLayout('cuda', torch.float32, size=[1, 197, 768], stride=[151296, 768, 1])
buf109_layout = FixedLayout('cuda', torch.float32, size=[197, 768], stride=[768, 1])
buf110_layout = FixedLayout('cuda', torch.float32, size=[1, 197, 1], stride=[197, 1, 197])
class buf110_loop_body:
    var_ranges = {z0: 197, z1: 768}
    index0 = 768*z0 + z1
    index1 = z1
    index2 = z0
    def body(self, ops):
        get_index = self.get_index('index0')
        load = ops.load('buf109', get_index)
        get_index_1 = self.get_index('index1')
        load_1 = ops.load('arg83_1', get_index_1)
        add = ops.add(load, load_1)
        get_index_2 = self.get_index('index0')
        load_2 = ops.load('buf102', get_index_2)
        add_1 = ops.add(add, load_2)
        reduction = ops.reduction(torch.float32, torch.float32, 'welford_reduce', add_1)
        getitem = reduction[0]
        getitem_1 = reduction[1]
        getitem_2 = reduction[2]
        get_index_3 = self.get_index('index2')
        store_reduction = ops.store_reduction('buf110', get_index_3, getitem)
        return store_reduction
buf110 Triton code:
    import triton
    import triton.language as tl
    from triton.compiler.compiler import AttrsDescriptor

    from torch._inductor.runtime import triton_helpers, triton_heuristics
    from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
    from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, instance_descriptor, DeviceProperties

    @triton_heuristics.persistent_reduction(
        size_hints=[256, 1024],
        reduction_hint=ReductionHint.INNER,
        filename=__file__,
        triton_meta={'signature': {0: '*fp32', 1: '*fp32', 2: '*fp32', 3: '*fp32', 4: 'i32', 5: 'i32'}, 'device': DeviceProperties(type='cuda', index=0, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, multi_processor_count=132), 'constants': {}, 'configs': [AttrsDescriptor(divisible_by_16=(0, 1, 2, 3, 5), equal_to_1=())]},
        inductor_meta={'autotune_hints': set(), 'kernel_name': 'Placeholder.DESCRIPTIVE_NAME', 'mutated_arg_names': [], 'no_x_dim': True, 'num_load': 3, 'num_reduction': 3, 'backend_hash': '3944FAAD9D18E987535CF029C4441E186640DD453AE5AE6831EDF736C09E0A47', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': False, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False}
    )
    @triton.jit
    def triton_(in_ptr0, in_ptr1, in_ptr2, out_ptr0, xnumel, rnumel):
        xnumel = 197
        XBLOCK: tl.constexpr = 1
        rnumel = 768
        RBLOCK: tl.constexpr = 1024
        xoffset = tl.program_id(0) * XBLOCK
        xindex = tl.full([1], xoffset, tl.int32)
        xmask = xindex < xnumel
        rindex = tl.arange(0, RBLOCK)[:]
        roffset = 0
        rmask = rindex < rnumel
        r1 = rindex
        x0 = xindex
        tmp0 = tl.load(in_ptr0 + (r1 + (768*x0)), rmask & xmask, other=0.0)
        tmp1 = tl.load(in_ptr1 + (r1), rmask, eviction_policy='evict_last', other=0.0)
        tmp3 = tl.load(in_ptr2 + (r1 + (768*x0)), rmask & xmask, other=0.0)
        tmp2 = tmp0 + tmp1
        tmp4 = tmp2 + tmp3
        tmp5 = tl.broadcast_to(tmp4, [RBLOCK])
        tmp7 = tl.where(rmask & xmask, tmp5, 0)
        tmp8 = tl.broadcast_to(tmp5, [RBLOCK])
        tmp10 = tl.where(rmask & xmask, tmp8, 0)
        tmp11 = triton_helpers.promote_to_tensor(tl.sum(tmp10, 0))
        tmp12 = tl.full([1], 768, tl.int32)
        tmp13 = tmp12.to(tl.float32)
        tmp14 = tmp11 / tmp13
        tmp15 = tmp5 - tmp14
        tmp16 = tmp15 * tmp15
        tmp17 = tl.broadcast_to(tmp16, [RBLOCK])
        tmp19 = tl.where(rmask & xmask, tmp17, 0)
        tmp20 = triton_helpers.promote_to_tensor(tl.sum(tmp19, 0))
        tl.store(out_ptr0 + (x0), tmp14, xmask)


buf111: SchedulerNode(ComputedBuffer)
buf111.writes = [MemoryDep('buf111', c0, {c0: 197}, None)]
buf111.unmet_dependencies = 
    [   MemoryDep('buf102', c0, {c0: 151296}, None),
        MemoryDep('buf109', c0, {c0: 151296}, None)]
buf111.met_dependencies = [MemoryDep('arg83_1', c1, {c0: 197, c1: 768}, None)]
buf111.users = [NodeUser(node=SchedulerNode(name='buf113'), can_inplace=False, is_weak=False)]
buf111.group.device = cuda:0
buf111.group.iteration = (197, 768)
buf111.sizes = ([197], [768])
arg83_1_layout = FixedLayout('cuda', torch.float32, size=[768], stride=[1])
buf102_layout = FixedLayout('cuda', torch.float32, size=[1, 197, 768], stride=[151296, 768, 1])
buf109_layout = FixedLayout('cuda', torch.float32, size=[197, 768], stride=[768, 1])
buf111_layout = FixedLayout('cuda', torch.float32, size=[1, 197, 1], stride=[197, 1, 197])
class buf111_loop_body:
    var_ranges = {z0: 197, z1: 768}
    index0 = 768*z0 + z1
    index1 = z1
    index2 = z0
    def body(self, ops):
        get_index = self.get_index('index0')
        load = ops.load('buf109', get_index)
        get_index_1 = self.get_index('index1')
        load_1 = ops.load('arg83_1', get_index_1)
        add = ops.add(load, load_1)
        get_index_2 = self.get_index('index0')
        load_2 = ops.load('buf102', get_index_2)
        add_1 = ops.add(add, load_2)
        reduction = ops.reduction(torch.float32, torch.float32, 'welford_reduce', add_1)
        getitem = reduction[0]
        getitem_1 = reduction[1]
        getitem_2 = reduction[2]
        get_index_3 = self.get_index('index2')
        store_reduction = ops.store_reduction('buf111', get_index_3, getitem_1)
        return store_reduction
buf111 Triton code:
    import triton
    import triton.language as tl
    from triton.compiler.compiler import AttrsDescriptor

    from torch._inductor.runtime import triton_helpers, triton_heuristics
    from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
    from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, instance_descriptor, DeviceProperties

    @triton_heuristics.persistent_reduction(
        size_hints=[256, 1024],
        reduction_hint=ReductionHint.INNER,
        filename=__file__,
        triton_meta={'signature': {0: '*fp32', 1: '*fp32', 2: '*fp32', 3: '*fp32', 4: 'i32', 5: 'i32'}, 'device': DeviceProperties(type='cuda', index=0, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, multi_processor_count=132), 'constants': {}, 'configs': [AttrsDescriptor(divisible_by_16=(0, 1, 2, 3, 5), equal_to_1=())]},
        inductor_meta={'autotune_hints': set(), 'kernel_name': 'Placeholder.DESCRIPTIVE_NAME', 'mutated_arg_names': [], 'no_x_dim': True, 'num_load': 3, 'num_reduction': 3, 'backend_hash': '3944FAAD9D18E987535CF029C4441E186640DD453AE5AE6831EDF736C09E0A47', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': False, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False}
    )
    @triton.jit
    def triton_(in_ptr0, in_ptr1, in_ptr2, out_ptr0, xnumel, rnumel):
        xnumel = 197
        XBLOCK: tl.constexpr = 1
        rnumel = 768
        RBLOCK: tl.constexpr = 1024
        xoffset = tl.program_id(0) * XBLOCK
        xindex = tl.full([1], xoffset, tl.int32)
        xmask = xindex < xnumel
        rindex = tl.arange(0, RBLOCK)[:]
        roffset = 0
        rmask = rindex < rnumel
        r1 = rindex
        x0 = xindex
        tmp0 = tl.load(in_ptr0 + (r1 + (768*x0)), rmask & xmask, other=0.0)
        tmp1 = tl.load(in_ptr1 + (r1), rmask, eviction_policy='evict_last', other=0.0)
        tmp3 = tl.load(in_ptr2 + (r1 + (768*x0)), rmask & xmask, other=0.0)
        tmp2 = tmp0 + tmp1
        tmp4 = tmp2 + tmp3
        tmp5 = tl.broadcast_to(tmp4, [RBLOCK])
        tmp7 = tl.where(rmask & xmask, tmp5, 0)
        tmp8 = tl.broadcast_to(tmp5, [RBLOCK])
        tmp10 = tl.where(rmask & xmask, tmp8, 0)
        tmp11 = triton_helpers.promote_to_tensor(tl.sum(tmp10, 0))
        tmp12 = tl.full([1], 768, tl.int32)
        tmp13 = tmp12.to(tl.float32)
        tmp14 = tmp11 / tmp13
        tmp15 = tmp5 - tmp14
        tmp16 = tmp15 * tmp15
        tmp17 = tl.broadcast_to(tmp16, [RBLOCK])
        tmp19 = tl.where(rmask & xmask, tmp17, 0)
        tmp20 = triton_helpers.promote_to_tensor(tl.sum(tmp19, 0))
        tl.store(out_ptr0 + (x0), tmp20, xmask)


buf113: SchedulerNode(ComputedBuffer)
buf113.writes = [MemoryDep('buf113', c0, {c0: 151296}, None)]
buf113.unmet_dependencies = 
    [   MemoryDep('buf102', c0, {c0: 151296}, None),
        MemoryDep('buf109', c0, {c0: 151296}, None),
        MemoryDep('buf110', c0, {c0: 197}, None),
        MemoryDep('buf111', c0, {c0: 197}, None)]
buf113.met_dependencies = 
    [   MemoryDep('arg83_1', c1, {c0: 197, c1: 768}, None),
        MemoryDep('arg84_1', c1, {c0: 197, c1: 768}, None),
        MemoryDep('arg85_1', c1, {c0: 197, c1: 768}, None)]
buf113.users = [NodeUser(node=ExternKernelSchedulerNode(name='buf114'), can_inplace=False, is_weak=False), NodeUser(node=ExternKernelSchedulerNode(name='buf115'), can_inplace=False, is_weak=False), NodeUser(node=ExternKernelSchedulerNode(name='buf116'), can_inplace=False, is_weak=False)]
buf113.group.device = cuda:0
buf113.group.iteration = (151296, 1)
buf113.sizes = ([197, 768], [])
arg84_1_layout = FixedLayout('cuda', torch.float32, size=[768], stride=[1])
buf102_layout = FixedLayout('cuda', torch.float32, size=[1, 197, 768], stride=[151296, 768, 1])
arg85_1_layout = FixedLayout('cuda', torch.float32, size=[768], stride=[1])
buf109_layout = FixedLayout('cuda', torch.float32, size=[197, 768], stride=[768, 1])
arg83_1_layout = FixedLayout('cuda', torch.float32, size=[768], stride=[1])
buf111_layout = FixedLayout('cuda', torch.float32, size=[1, 197, 1], stride=[197, 1, 197])
buf110_layout = FixedLayout('cuda', torch.float32, size=[1, 197, 1], stride=[197, 1, 197])
buf113_layout = FixedLayout('cuda', torch.float32, size=[1, 197, 768], stride=[151296, 768, 1])
class buf113_loop_body:
    var_ranges = {z0: 197, z1: 768}
    index0 = 768*z0 + z1
    index1 = z1
    index2 = z0
    def body(self, ops):
        get_index = self.get_index('index0')
        load = ops.load('buf109', get_index)
        get_index_1 = self.get_index('index1')
        load_1 = ops.load('arg83_1', get_index_1)
        add = ops.add(load, load_1)
        get_index_2 = self.get_index('index0')
        load_2 = ops.load('buf102', get_index_2)
        add_1 = ops.add(add, load_2)
        get_index_3 = self.get_index('index2')
        load_3 = ops.load('buf110', get_index_3)
        sub = ops.sub(add_1, load_3)
        get_index_4 = self.get_index('index2')
        load_4 = ops.load('buf111', get_index_4)
        constant = ops.constant(768.0, torch.float32)
        truediv = ops.truediv(load_4, constant)
        constant_1 = ops.constant(1e-12, torch.float32)
        add_2 = ops.add(truediv, constant_1)
        rsqrt = ops.rsqrt(add_2)
        mul = ops.mul(sub, rsqrt)
        get_index_5 = self.get_index('index1')
        load_5 = ops.load('arg84_1', get_index_5)
        mul_1 = ops.mul(mul, load_5)
        get_index_6 = self.get_index('index1')
        load_6 = ops.load('arg85_1', get_index_6)
        add_3 = ops.add(mul_1, load_6)
        get_index_7 = self.get_index('index0')
        store = ops.store('buf113', get_index_7, add_3, None)
        return store
buf113 Triton code:
    import triton
    import triton.language as tl
    from triton.compiler.compiler import AttrsDescriptor

    from torch._inductor.runtime import triton_helpers, triton_heuristics
    from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
    from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, instance_descriptor, DeviceProperties

    @triton_heuristics.pointwise(
        size_hints=[262144], 
        filename=__file__,
        triton_meta={'signature': {0: '*fp32', 1: '*fp32', 2: '*fp32', 3: '*fp32', 4: '*fp32', 5: '*fp32', 6: '*fp32', 7: '*fp32', 8: 'i32'}, 'device': DeviceProperties(type='cuda', index=0, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, multi_processor_count=132), 'constants': {}, 'configs': [AttrsDescriptor(divisible_by_16=(0, 1, 2, 3, 4, 5, 6, 7, 8), equal_to_1=())]},
        inductor_meta={'autotune_hints': set(), 'kernel_name': 'Placeholder.DESCRIPTIVE_NAME', 'mutated_arg_names': [], 'no_x_dim': False, 'num_load': 7, 'num_reduction': 0, 'backend_hash': '3944FAAD9D18E987535CF029C4441E186640DD453AE5AE6831EDF736C09E0A47', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': False, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False},
        min_elem_per_thread=0
    )
    @triton.jit
    def triton_(in_ptr0, in_ptr1, in_ptr2, in_ptr3, in_ptr4, in_ptr5, in_ptr6, out_ptr0, xnumel, XBLOCK : tl.constexpr):
        xnumel = 151296
        xoffset = tl.program_id(0) * XBLOCK
        xindex = xoffset + tl.arange(0, XBLOCK)[:]
        xmask = xindex < xnumel
        x2 = xindex
        x0 = xindex % 768
        x1 = (xindex // 768)
        tmp0 = tl.load(in_ptr0 + (x2), xmask)
        tmp1 = tl.load(in_ptr1 + (x0), xmask, eviction_policy='evict_last')
        tmp3 = tl.load(in_ptr2 + (x2), xmask)
        tmp5 = tl.load(in_ptr3 + (x1), xmask, eviction_policy='evict_last')
        tmp7 = tl.load(in_ptr4 + (x1), xmask, eviction_policy='evict_last')
        tmp14 = tl.load(in_ptr5 + (x0), xmask, eviction_policy='evict_last')
        tmp16 = tl.load(in_ptr6 + (x0), xmask, eviction_policy='evict_last')
        tmp2 = tmp0 + tmp1
        tmp4 = tmp2 + tmp3
        tmp6 = tmp4 - tmp5
        tmp8 = 768.0
        tmp9 = tmp7 / tmp8
        tmp10 = 1e-12
        tmp11 = tmp9 + tmp10
        tmp12 = libdevice.rsqrt(tmp11)
        tmp13 = tmp6 * tmp12
        tmp15 = tmp13 * tmp14
        tmp17 = tmp15 + tmp16
        tl.store(out_ptr0 + (x2), tmp17, xmask)


buf114: ExternKernelSchedulerNode(ExternKernelOut)
buf114.writes = [StarDep(name='buf114', mode=None)]
buf114.unmet_dependencies = [StarDep(name='buf113', mode=None)]
buf114.met_dependencies = [StarDep(name='arg86_1', mode=None), StarDep(name='arg87_1', mode=None)]
buf114.users = [NodeUser(node=ExternKernelSchedulerNode(name='buf117'), can_inplace=False, is_weak=False)]
buf114.node.kernel = extern_kernels.addmm


buf115: ExternKernelSchedulerNode(ExternKernelOut)
buf115.writes = [StarDep(name='buf115', mode=None)]
buf115.unmet_dependencies = [StarDep(name='buf113', mode=None)]
buf115.met_dependencies = [StarDep(name='arg88_1', mode=None), StarDep(name='arg89_1', mode=None)]
buf115.users = [NodeUser(node=ExternKernelSchedulerNode(name='buf117'), can_inplace=False, is_weak=False)]
buf115.node.kernel = extern_kernels.addmm


buf116: ExternKernelSchedulerNode(ExternKernelOut)
buf116.writes = [StarDep(name='buf116', mode=None)]
buf116.unmet_dependencies = [StarDep(name='buf113', mode=None)]
buf116.met_dependencies = [StarDep(name='arg90_1', mode=None), StarDep(name='arg91_1', mode=None)]
buf116.users = [NodeUser(node=ExternKernelSchedulerNode(name='buf117'), can_inplace=False, is_weak=False)]
buf116.node.kernel = extern_kernels.addmm


buf117: ExternKernelSchedulerNode(FallbackKernel)
buf117.writes = [StarDep(name='buf117', mode=None)]
buf117.unmet_dependencies = 
    [   StarDep(name='buf114', mode=None),
        StarDep(name='buf115', mode=None),
        StarDep(name='buf116', mode=None)]
buf117.met_dependencies = []
buf117.users = [NodeUser(node=ExternKernelSchedulerNode(name='buf118'), can_inplace=False, is_weak=False), NodeUser(node=ExternKernelSchedulerNode(name='buf119'), can_inplace=False, is_weak=False), NodeUser(node=ExternKernelSchedulerNode(name='buf120'), can_inplace=False, is_weak=False), NodeUser(node=ExternKernelSchedulerNode(name='buf121'), can_inplace=False, is_weak=False)]
buf117.node.kernel = None


buf118: ExternKernelSchedulerNode(MultiOutput)
buf118.writes = [StarDep(name='buf118', mode=None)]
buf118.unmet_dependencies = [StarDep(name='buf117', mode=None)]
buf118.met_dependencies = []
buf118.users = [NodeUser(node=ExternKernelSchedulerNode(name='buf122'), can_inplace=False, is_weak=False)]
buf118.node.kernel = None


buf122: ExternKernelSchedulerNode(ExternKernelOut)
buf122.writes = [StarDep(name='buf122', mode=None)]
buf122.unmet_dependencies = [StarDep(name='buf118', mode=None)]
buf122.met_dependencies = [StarDep(name='arg92_1', mode=None)]
buf122.users = [NodeUser(node=SchedulerNode(name='buf123'), can_inplace=True, is_weak=False)]
buf122.node.kernel = extern_kernels.mm


buf123: SchedulerNode(ComputedBuffer)
buf123.writes = [MemoryDep('buf123', c0, {c0: 151296}, None)]
buf123.unmet_dependencies = 
    [   MemoryDep('buf102', c0, {c0: 151296}, None),
        MemoryDep('buf109', c0, {c0: 151296}, None),
        MemoryDep('buf122', c0, {c0: 151296}, None)]
buf123.met_dependencies = 
    [   MemoryDep('arg83_1', c1, {c0: 197, c1: 768}, None),
        MemoryDep('arg93_1', c1, {c0: 197, c1: 768}, None)]
buf123.users = [NodeUser(node=SchedulerNode(name='buf124'), can_inplace=False, is_weak=False), NodeUser(node=SchedulerNode(name='buf125'), can_inplace=False, is_weak=False), NodeUser(node=SchedulerNode(name='buf126'), can_inplace=False, is_weak=False), NodeUser(node=SchedulerNode(name='buf127'), can_inplace=True, is_weak=False), NodeUser(node=SchedulerNode(name='buf131'), can_inplace=False, is_weak=False), NodeUser(node=SchedulerNode(name='buf132'), can_inplace=False, is_weak=False), NodeUser(node=SchedulerNode(name='buf133'), can_inplace=False, is_weak=False), NodeUser(node=SchedulerNode(name='buf134'), can_inplace=True, is_weak=False), NodeUser(node=SchedulerNode(name='buf144'), can_inplace=True, is_weak=False)]
buf123.group.device = cuda:0
buf123.group.iteration = (151296, 1)
buf123.sizes = ([197, 768], [])
buf122_layout = FixedLayout('cuda', torch.float32, size=[197, 768], stride=[768, 1])
arg93_1_layout = FixedLayout('cuda', torch.float32, size=[768], stride=[1])
buf102_layout = FixedLayout('cuda', torch.float32, size=[1, 197, 768], stride=[151296, 768, 1])
buf109_layout = FixedLayout('cuda', torch.float32, size=[197, 768], stride=[768, 1])
arg83_1_layout = FixedLayout('cuda', torch.float32, size=[768], stride=[1])
buf123_layout = FixedLayout('cuda', torch.float32, size=[1, 197, 768], stride=[151296, 768, 1])
class buf123_loop_body:
    var_ranges = {z0: 197, z1: 768}
    index0 = 768*z0 + z1
    index1 = z1
    def body(self, ops):
        get_index = self.get_index('index0')
        load = ops.load('buf122', get_index)
        get_index_1 = self.get_index('index1')
        load_1 = ops.load('arg93_1', get_index_1)
        add = ops.add(load, load_1)
        get_index_2 = self.get_index('index0')
        load_2 = ops.load('buf109', get_index_2)
        get_index_3 = self.get_index('index1')
        load_3 = ops.load('arg83_1', get_index_3)
        add_1 = ops.add(load_2, load_3)
        get_index_4 = self.get_index('index0')
        load_4 = ops.load('buf102', get_index_4)
        add_2 = ops.add(add_1, load_4)
        add_3 = ops.add(add, add_2)
        get_index_5 = self.get_index('index0')
        store = ops.store('buf123', get_index_5, add_3, None)
        return store
buf123 Triton code:
    import triton
    import triton.language as tl
    from triton.compiler.compiler import AttrsDescriptor

    from torch._inductor.runtime import triton_helpers, triton_heuristics
    from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
    from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, instance_descriptor, DeviceProperties

    @triton_heuristics.pointwise(
        size_hints=[262144], 
        filename=__file__,
        triton_meta={'signature': {0: '*fp32', 1: '*fp32', 2: '*fp32', 3: '*fp32', 4: '*fp32', 5: 'i32'}, 'device': DeviceProperties(type='cuda', index=0, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, multi_processor_count=132), 'constants': {}, 'configs': [AttrsDescriptor(divisible_by_16=(0, 1, 2, 3, 4, 5), equal_to_1=())]},
        inductor_meta={'autotune_hints': set(), 'kernel_name': 'Placeholder.DESCRIPTIVE_NAME', 'mutated_arg_names': ['in_out_ptr0'], 'no_x_dim': False, 'num_load': 5, 'num_reduction': 0, 'backend_hash': '3944FAAD9D18E987535CF029C4441E186640DD453AE5AE6831EDF736C09E0A47', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': False, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False},
        min_elem_per_thread=0
    )
    @triton.jit
    def triton_(in_out_ptr0, in_ptr0, in_ptr1, in_ptr2, in_ptr3, xnumel, XBLOCK : tl.constexpr):
        xnumel = 151296
        xoffset = tl.program_id(0) * XBLOCK
        xindex = xoffset + tl.arange(0, XBLOCK)[:]
        xmask = xindex < xnumel
        x2 = xindex
        x0 = xindex % 768
        tmp0 = tl.load(in_out_ptr0 + (x2), xmask)
        tmp1 = tl.load(in_ptr0 + (x0), xmask, eviction_policy='evict_last')
        tmp3 = tl.load(in_ptr1 + (x2), xmask)
        tmp4 = tl.load(in_ptr2 + (x0), xmask, eviction_policy='evict_last')
        tmp6 = tl.load(in_ptr3 + (x2), xmask)
        tmp2 = tmp0 + tmp1
        tmp5 = tmp3 + tmp4
        tmp7 = tmp5 + tmp6
        tmp8 = tmp2 + tmp7
        tl.store(in_out_ptr0 + (x2), tmp8, xmask)


buf124: SchedulerNode(ComputedBuffer)
buf124.writes = [MemoryDep('buf124', c0, {c0: 197}, None)]
buf124.unmet_dependencies = [MemoryDep('buf123', c0, {c0: 151296}, None)]
buf124.met_dependencies = []
buf124.users = [NodeUser(node=SchedulerNode(name='buf127'), can_inplace=False, is_weak=False)]
buf124.group.device = cuda:0
buf124.group.iteration = (197, 768)
buf124.sizes = ([197], [768])
buf123_layout = FixedLayout('cuda', torch.float32, size=[1, 197, 768], stride=[151296, 768, 1])
buf124_layout = FixedLayout('cuda', torch.float32, size=[1, 197, 1], stride=[197, 1, 197])
class buf124_loop_body:
    var_ranges = {z0: 197, z1: 768}
    index0 = 768*z0 + z1
    index1 = z0
    def body(self, ops):
        get_index = self.get_index('index0')
        load = ops.load('buf123', get_index)
        reduction = ops.reduction(torch.float32, torch.float32, 'welford_reduce', load)
        getitem = reduction[0]
        getitem_1 = reduction[1]
        getitem_2 = reduction[2]
        get_index_1 = self.get_index('index1')
        store_reduction = ops.store_reduction('buf124', get_index_1, getitem)
        return store_reduction
buf124 Triton code:
    import triton
    import triton.language as tl
    from triton.compiler.compiler import AttrsDescriptor

    from torch._inductor.runtime import triton_helpers, triton_heuristics
    from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
    from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, instance_descriptor, DeviceProperties

    @triton_heuristics.persistent_reduction(
        size_hints=[256, 1024],
        reduction_hint=ReductionHint.INNER,
        filename=__file__,
        triton_meta={'signature': {0: '*fp32', 1: '*fp32', 2: 'i32', 3: 'i32'}, 'device': DeviceProperties(type='cuda', index=0, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, multi_processor_count=132), 'constants': {}, 'configs': [AttrsDescriptor(divisible_by_16=(0, 1, 3), equal_to_1=())]},
        inductor_meta={'autotune_hints': set(), 'kernel_name': 'Placeholder.DESCRIPTIVE_NAME', 'mutated_arg_names': [], 'no_x_dim': True, 'num_load': 1, 'num_reduction': 3, 'backend_hash': '3944FAAD9D18E987535CF029C4441E186640DD453AE5AE6831EDF736C09E0A47', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': False, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False}
    )
    @triton.jit
    def triton_(in_ptr0, out_ptr0, xnumel, rnumel):
        xnumel = 197
        XBLOCK: tl.constexpr = 1
        rnumel = 768
        RBLOCK: tl.constexpr = 1024
        xoffset = tl.program_id(0) * XBLOCK
        xindex = tl.full([1], xoffset, tl.int32)
        xmask = xindex < xnumel
        rindex = tl.arange(0, RBLOCK)[:]
        roffset = 0
        rmask = rindex < rnumel
        r1 = rindex
        x0 = xindex
        tmp0 = tl.load(in_ptr0 + (r1 + (768*x0)), rmask & xmask, other=0.0)
        tmp1 = tl.broadcast_to(tmp0, [RBLOCK])
        tmp3 = tl.where(rmask & xmask, tmp1, 0)
        tmp4 = tl.broadcast_to(tmp1, [RBLOCK])
        tmp6 = tl.where(rmask & xmask, tmp4, 0)
        tmp7 = triton_helpers.promote_to_tensor(tl.sum(tmp6, 0))
        tmp8 = tl.full([1], 768, tl.int32)
        tmp9 = tmp8.to(tl.float32)
        tmp10 = tmp7 / tmp9
        tmp11 = tmp1 - tmp10
        tmp12 = tmp11 * tmp11
        tmp13 = tl.broadcast_to(tmp12, [RBLOCK])
        tmp15 = tl.where(rmask & xmask, tmp13, 0)
        tmp16 = triton_helpers.promote_to_tensor(tl.sum(tmp15, 0))
        tl.store(out_ptr0 + (x0), tmp10, xmask)


buf125: SchedulerNode(ComputedBuffer)
buf125.writes = [MemoryDep('buf125', c0, {c0: 197}, None)]
buf125.unmet_dependencies = [MemoryDep('buf123', c0, {c0: 151296}, None)]
buf125.met_dependencies = []
buf125.users = [NodeUser(node=SchedulerNode(name='buf127'), can_inplace=False, is_weak=False)]
buf125.group.device = cuda:0
buf125.group.iteration = (197, 768)
buf125.sizes = ([197], [768])
buf123_layout = FixedLayout('cuda', torch.float32, size=[1, 197, 768], stride=[151296, 768, 1])
buf125_layout = FixedLayout('cuda', torch.float32, size=[1, 197, 1], stride=[197, 1, 197])
class buf125_loop_body:
    var_ranges = {z0: 197, z1: 768}
    index0 = 768*z0 + z1
    index1 = z0
    def body(self, ops):
        get_index = self.get_index('index0')
        load = ops.load('buf123', get_index)
        reduction = ops.reduction(torch.float32, torch.float32, 'welford_reduce', load)
        getitem = reduction[0]
        getitem_1 = reduction[1]
        getitem_2 = reduction[2]
        get_index_1 = self.get_index('index1')
        store_reduction = ops.store_reduction('buf125', get_index_1, getitem_1)
        return store_reduction
buf125 Triton code:
    import triton
    import triton.language as tl
    from triton.compiler.compiler import AttrsDescriptor

    from torch._inductor.runtime import triton_helpers, triton_heuristics
    from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
    from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, instance_descriptor, DeviceProperties

    @triton_heuristics.persistent_reduction(
        size_hints=[256, 1024],
        reduction_hint=ReductionHint.INNER,
        filename=__file__,
        triton_meta={'signature': {0: '*fp32', 1: '*fp32', 2: 'i32', 3: 'i32'}, 'device': DeviceProperties(type='cuda', index=0, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, multi_processor_count=132), 'constants': {}, 'configs': [AttrsDescriptor(divisible_by_16=(0, 1, 3), equal_to_1=())]},
        inductor_meta={'autotune_hints': set(), 'kernel_name': 'Placeholder.DESCRIPTIVE_NAME', 'mutated_arg_names': [], 'no_x_dim': True, 'num_load': 1, 'num_reduction': 3, 'backend_hash': '3944FAAD9D18E987535CF029C4441E186640DD453AE5AE6831EDF736C09E0A47', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': False, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False}
    )
    @triton.jit
    def triton_(in_ptr0, out_ptr0, xnumel, rnumel):
        xnumel = 197
        XBLOCK: tl.constexpr = 1
        rnumel = 768
        RBLOCK: tl.constexpr = 1024
        xoffset = tl.program_id(0) * XBLOCK
        xindex = tl.full([1], xoffset, tl.int32)
        xmask = xindex < xnumel
        rindex = tl.arange(0, RBLOCK)[:]
        roffset = 0
        rmask = rindex < rnumel
        r1 = rindex
        x0 = xindex
        tmp0 = tl.load(in_ptr0 + (r1 + (768*x0)), rmask & xmask, other=0.0)
        tmp1 = tl.broadcast_to(tmp0, [RBLOCK])
        tmp3 = tl.where(rmask & xmask, tmp1, 0)
        tmp4 = tl.broadcast_to(tmp1, [RBLOCK])
        tmp6 = tl.where(rmask & xmask, tmp4, 0)
        tmp7 = triton_helpers.promote_to_tensor(tl.sum(tmp6, 0))
        tmp8 = tl.full([1], 768, tl.int32)
        tmp9 = tmp8.to(tl.float32)
        tmp10 = tmp7 / tmp9
        tmp11 = tmp1 - tmp10
        tmp12 = tmp11 * tmp11
        tmp13 = tl.broadcast_to(tmp12, [RBLOCK])
        tmp15 = tl.where(rmask & xmask, tmp13, 0)
        tmp16 = triton_helpers.promote_to_tensor(tl.sum(tmp15, 0))
        tl.store(out_ptr0 + (x0), tmp16, xmask)


buf127: SchedulerNode(ComputedBuffer)
buf127.writes = [MemoryDep('buf127', c0, {c0: 151296}, None)]
buf127.unmet_dependencies = 
    [   MemoryDep('buf123', c0, {c0: 151296}, None),
        MemoryDep('buf124', c0, {c0: 197}, None),
        MemoryDep('buf125', c0, {c0: 197}, None)]
buf127.met_dependencies = 
    [   MemoryDep('arg94_1', c1, {c0: 197, c1: 768}, None),
        MemoryDep('arg95_1', c1, {c0: 197, c1: 768}, None)]
buf127.users = [NodeUser(node=ExternKernelSchedulerNode(name='buf128'), can_inplace=False, is_weak=False)]
buf127.group.device = cuda:0
buf127.group.iteration = (151296, 1)
buf127.sizes = ([197, 768], [])
buf124_layout = FixedLayout('cuda', torch.float32, size=[1, 197, 1], stride=[197, 1, 197])
arg94_1_layout = FixedLayout('cuda', torch.float32, size=[768], stride=[1])
arg95_1_layout = FixedLayout('cuda', torch.float32, size=[768], stride=[1])
buf125_layout = FixedLayout('cuda', torch.float32, size=[1, 197, 1], stride=[197, 1, 197])
buf123_layout = FixedLayout('cuda', torch.float32, size=[1, 197, 768], stride=[151296, 768, 1])
buf127_layout = FixedLayout('cuda', torch.float32, size=[1, 197, 768], stride=[151296, 768, 1])
class buf127_loop_body:
    var_ranges = {z0: 197, z1: 768}
    index0 = 768*z0 + z1
    index1 = z0
    index2 = z1
    def body(self, ops):
        get_index = self.get_index('index0')
        load = ops.load('buf123', get_index)
        get_index_1 = self.get_index('index1')
        load_1 = ops.load('buf124', get_index_1)
        sub = ops.sub(load, load_1)
        get_index_2 = self.get_index('index1')
        load_2 = ops.load('buf125', get_index_2)
        constant = ops.constant(768.0, torch.float32)
        truediv = ops.truediv(load_2, constant)
        constant_1 = ops.constant(1e-12, torch.float32)
        add = ops.add(truediv, constant_1)
        rsqrt = ops.rsqrt(add)
        mul = ops.mul(sub, rsqrt)
        get_index_3 = self.get_index('index2')
        load_3 = ops.load('arg94_1', get_index_3)
        mul_1 = ops.mul(mul, load_3)
        get_index_4 = self.get_index('index2')
        load_4 = ops.load('arg95_1', get_index_4)
        add_1 = ops.add(mul_1, load_4)
        get_index_5 = self.get_index('index0')
        store = ops.store('buf127', get_index_5, add_1, None)
        return store
buf127 Triton code:
    import triton
    import triton.language as tl
    from triton.compiler.compiler import AttrsDescriptor

    from torch._inductor.runtime import triton_helpers, triton_heuristics
    from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
    from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, instance_descriptor, DeviceProperties

    @triton_heuristics.pointwise(
        size_hints=[262144], 
        filename=__file__,
        triton_meta={'signature': {0: '*fp32', 1: '*fp32', 2: '*fp32', 3: '*fp32', 4: '*fp32', 5: '*fp32', 6: 'i32'}, 'device': DeviceProperties(type='cuda', index=0, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, multi_processor_count=132), 'constants': {}, 'configs': [AttrsDescriptor(divisible_by_16=(0, 1, 2, 3, 4, 5, 6), equal_to_1=())]},
        inductor_meta={'autotune_hints': set(), 'kernel_name': 'Placeholder.DESCRIPTIVE_NAME', 'mutated_arg_names': [], 'no_x_dim': False, 'num_load': 5, 'num_reduction': 0, 'backend_hash': '3944FAAD9D18E987535CF029C4441E186640DD453AE5AE6831EDF736C09E0A47', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': False, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False},
        min_elem_per_thread=0
    )
    @triton.jit
    def triton_(in_ptr0, in_ptr1, in_ptr2, in_ptr3, in_ptr4, out_ptr0, xnumel, XBLOCK : tl.constexpr):
        xnumel = 151296
        xoffset = tl.program_id(0) * XBLOCK
        xindex = xoffset + tl.arange(0, XBLOCK)[:]
        xmask = xindex < xnumel
        x2 = xindex
        x1 = (xindex // 768)
        x0 = xindex % 768
        tmp0 = tl.load(in_ptr0 + (x2), xmask)
        tmp1 = tl.load(in_ptr1 + (x1), xmask, eviction_policy='evict_last')
        tmp3 = tl.load(in_ptr2 + (x1), xmask, eviction_policy='evict_last')
        tmp10 = tl.load(in_ptr3 + (x0), xmask, eviction_policy='evict_last')
        tmp12 = tl.load(in_ptr4 + (x0), xmask, eviction_policy='evict_last')
        tmp2 = tmp0 - tmp1
        tmp4 = 768.0
        tmp5 = tmp3 / tmp4
        tmp6 = 1e-12
        tmp7 = tmp5 + tmp6
        tmp8 = libdevice.rsqrt(tmp7)
        tmp9 = tmp2 * tmp8
        tmp11 = tmp9 * tmp10
        tmp13 = tmp11 + tmp12
        tl.store(out_ptr0 + (x2), tmp13, xmask)


buf128: ExternKernelSchedulerNode(ExternKernelOut)
buf128.writes = [StarDep(name='buf128', mode=None)]
buf128.unmet_dependencies = [StarDep(name='buf127', mode=None)]
buf128.met_dependencies = [StarDep(name='arg96_1', mode=None)]
buf128.users = [NodeUser(node=SchedulerNode(name='buf129'), can_inplace=True, is_weak=False)]
buf128.node.kernel = extern_kernels.mm


buf129: SchedulerNode(ComputedBuffer)
buf129.writes = [MemoryDep('buf129', c0, {c0: 605184}, None)]
buf129.unmet_dependencies = [MemoryDep('buf128', c0, {c0: 605184}, None)]
buf129.met_dependencies = [MemoryDep('arg97_1', c1, {c0: 197, c1: 3072}, None)]
buf129.users = [NodeUser(node=ExternKernelSchedulerNode(name='buf130'), can_inplace=False, is_weak=False)]
buf129.group.device = cuda:0
buf129.group.iteration = (605184, 1)
buf129.sizes = ([197, 3072], [])
arg97_1_layout = FixedLayout('cuda', torch.float32, size=[3072], stride=[1])
buf128_layout = FixedLayout('cuda', torch.float32, size=[197, 3072], stride=[3072, 1])
buf129_layout = FixedLayout('cuda', torch.float32, size=[1, 197, 3072], stride=[605184, 3072, 1])
class buf129_loop_body:
    var_ranges = {z0: 197, z1: 3072}
    index0 = 3072*z0 + z1
    index1 = z1
    def body(self, ops):
        get_index = self.get_index('index0')
        load = ops.load('buf128', get_index)
        get_index_1 = self.get_index('index1')
        load_1 = ops.load('arg97_1', get_index_1)
        add = ops.add(load, load_1)
        constant = ops.constant(0.5, torch.float32)
        mul = ops.mul(add, constant)
        get_index_2 = self.get_index('index0')
        load_2 = ops.load('buf128', get_index_2)
        get_index_3 = self.get_index('index1')
        load_3 = ops.load('arg97_1', get_index_3)
        add_1 = ops.add(load_2, load_3)
        constant_1 = ops.constant(0.7071067811865476, torch.float32)
        mul_1 = ops.mul(add_1, constant_1)
        erf = ops.erf(mul_1)
        constant_2 = ops.constant(1.0, torch.float32)
        add_2 = ops.add(erf, constant_2)
        mul_2 = ops.mul(mul, add_2)
        get_index_4 = self.get_index('index0')
        store = ops.store('buf129', get_index_4, mul_2, None)
        return store
buf129 Triton code:
    import triton
    import triton.language as tl
    from triton.compiler.compiler import AttrsDescriptor

    from torch._inductor.runtime import triton_helpers, triton_heuristics
    from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
    from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, instance_descriptor, DeviceProperties

    @triton_heuristics.pointwise(
        size_hints=[1048576], 
        filename=__file__,
        triton_meta={'signature': {0: '*fp32', 1: '*fp32', 2: 'i32'}, 'device': DeviceProperties(type='cuda', index=0, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, multi_processor_count=132), 'constants': {}, 'configs': [AttrsDescriptor(divisible_by_16=(0, 1, 2), equal_to_1=())]},
        inductor_meta={'autotune_hints': set(), 'kernel_name': 'Placeholder.DESCRIPTIVE_NAME', 'mutated_arg_names': ['in_out_ptr0'], 'no_x_dim': False, 'num_load': 2, 'num_reduction': 0, 'backend_hash': '3944FAAD9D18E987535CF029C4441E186640DD453AE5AE6831EDF736C09E0A47', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': False, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False},
        min_elem_per_thread=0
    )
    @triton.jit
    def triton_(in_out_ptr0, in_ptr0, xnumel, XBLOCK : tl.constexpr):
        xnumel = 605184
        xoffset = tl.program_id(0) * XBLOCK
        xindex = xoffset + tl.arange(0, XBLOCK)[:]
        xmask = xindex < xnumel
        x2 = xindex
        x0 = xindex % 3072
        tmp0 = tl.load(in_out_ptr0 + (x2), xmask)
        tmp1 = tl.load(in_ptr0 + (x0), xmask, eviction_policy='evict_last')
        tmp2 = tmp0 + tmp1
        tmp3 = 0.5
        tmp4 = tmp2 * tmp3
        tmp5 = 0.7071067811865476
        tmp6 = tmp2 * tmp5
        tmp7 = libdevice.erf(tmp6)
        tmp8 = 1.0
        tmp9 = tmp7 + tmp8
        tmp10 = tmp4 * tmp9
        tl.store(in_out_ptr0 + (x2), tmp10, xmask)


buf130: ExternKernelSchedulerNode(ExternKernelOut)
buf130.writes = [StarDep(name='buf130', mode=None)]
buf130.unmet_dependencies = [StarDep(name='buf129', mode=None)]
buf130.met_dependencies = [StarDep(name='arg98_1', mode=None)]
buf130.users = [NodeUser(node=SchedulerNode(name='buf131'), can_inplace=False, is_weak=False), NodeUser(node=SchedulerNode(name='buf132'), can_inplace=False, is_weak=False), NodeUser(node=SchedulerNode(name='buf133'), can_inplace=False, is_weak=False), NodeUser(node=SchedulerNode(name='buf134'), can_inplace=True, is_weak=False), NodeUser(node=SchedulerNode(name='buf144'), can_inplace=True, is_weak=False)]
buf130.node.kernel = extern_kernels.mm


buf131: SchedulerNode(ComputedBuffer)
buf131.writes = [MemoryDep('buf131', c0, {c0: 197}, None)]
buf131.unmet_dependencies = 
    [   MemoryDep('buf123', c0, {c0: 151296}, None),
        MemoryDep('buf130', c0, {c0: 151296}, None)]
buf131.met_dependencies = [MemoryDep('arg99_1', c1, {c0: 197, c1: 768}, None)]
buf131.users = [NodeUser(node=SchedulerNode(name='buf134'), can_inplace=False, is_weak=False)]
buf131.group.device = cuda:0
buf131.group.iteration = (197, 768)
buf131.sizes = ([197], [768])
arg99_1_layout = FixedLayout('cuda', torch.float32, size=[768], stride=[1])
buf130_layout = FixedLayout('cuda', torch.float32, size=[197, 768], stride=[768, 1])
buf123_layout = FixedLayout('cuda', torch.float32, size=[1, 197, 768], stride=[151296, 768, 1])
buf131_layout = FixedLayout('cuda', torch.float32, size=[1, 197, 1], stride=[197, 1, 197])
class buf131_loop_body:
    var_ranges = {z0: 197, z1: 768}
    index0 = 768*z0 + z1
    index1 = z1
    index2 = z0
    def body(self, ops):
        get_index = self.get_index('index0')
        load = ops.load('buf130', get_index)
        get_index_1 = self.get_index('index1')
        load_1 = ops.load('arg99_1', get_index_1)
        add = ops.add(load, load_1)
        get_index_2 = self.get_index('index0')
        load_2 = ops.load('buf123', get_index_2)
        add_1 = ops.add(add, load_2)
        reduction = ops.reduction(torch.float32, torch.float32, 'welford_reduce', add_1)
        getitem = reduction[0]
        getitem_1 = reduction[1]
        getitem_2 = reduction[2]
        get_index_3 = self.get_index('index2')
        store_reduction = ops.store_reduction('buf131', get_index_3, getitem)
        return store_reduction
buf131 Triton code:
    import triton
    import triton.language as tl
    from triton.compiler.compiler import AttrsDescriptor

    from torch._inductor.runtime import triton_helpers, triton_heuristics
    from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
    from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, instance_descriptor, DeviceProperties

    @triton_heuristics.persistent_reduction(
        size_hints=[256, 1024],
        reduction_hint=ReductionHint.INNER,
        filename=__file__,
        triton_meta={'signature': {0: '*fp32', 1: '*fp32', 2: '*fp32', 3: '*fp32', 4: 'i32', 5: 'i32'}, 'device': DeviceProperties(type='cuda', index=0, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, multi_processor_count=132), 'constants': {}, 'configs': [AttrsDescriptor(divisible_by_16=(0, 1, 2, 3, 5), equal_to_1=())]},
        inductor_meta={'autotune_hints': set(), 'kernel_name': 'Placeholder.DESCRIPTIVE_NAME', 'mutated_arg_names': [], 'no_x_dim': True, 'num_load': 3, 'num_reduction': 3, 'backend_hash': '3944FAAD9D18E987535CF029C4441E186640DD453AE5AE6831EDF736C09E0A47', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': False, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False}
    )
    @triton.jit
    def triton_(in_ptr0, in_ptr1, in_ptr2, out_ptr0, xnumel, rnumel):
        xnumel = 197
        XBLOCK: tl.constexpr = 1
        rnumel = 768
        RBLOCK: tl.constexpr = 1024
        xoffset = tl.program_id(0) * XBLOCK
        xindex = tl.full([1], xoffset, tl.int32)
        xmask = xindex < xnumel
        rindex = tl.arange(0, RBLOCK)[:]
        roffset = 0
        rmask = rindex < rnumel
        r1 = rindex
        x0 = xindex
        tmp0 = tl.load(in_ptr0 + (r1 + (768*x0)), rmask & xmask, other=0.0)
        tmp1 = tl.load(in_ptr1 + (r1), rmask, eviction_policy='evict_last', other=0.0)
        tmp3 = tl.load(in_ptr2 + (r1 + (768*x0)), rmask & xmask, other=0.0)
        tmp2 = tmp0 + tmp1
        tmp4 = tmp2 + tmp3
        tmp5 = tl.broadcast_to(tmp4, [RBLOCK])
        tmp7 = tl.where(rmask & xmask, tmp5, 0)
        tmp8 = tl.broadcast_to(tmp5, [RBLOCK])
        tmp10 = tl.where(rmask & xmask, tmp8, 0)
        tmp11 = triton_helpers.promote_to_tensor(tl.sum(tmp10, 0))
        tmp12 = tl.full([1], 768, tl.int32)
        tmp13 = tmp12.to(tl.float32)
        tmp14 = tmp11 / tmp13
        tmp15 = tmp5 - tmp14
        tmp16 = tmp15 * tmp15
        tmp17 = tl.broadcast_to(tmp16, [RBLOCK])
        tmp19 = tl.where(rmask & xmask, tmp17, 0)
        tmp20 = triton_helpers.promote_to_tensor(tl.sum(tmp19, 0))
        tl.store(out_ptr0 + (x0), tmp14, xmask)


buf132: SchedulerNode(ComputedBuffer)
buf132.writes = [MemoryDep('buf132', c0, {c0: 197}, None)]
buf132.unmet_dependencies = 
    [   MemoryDep('buf123', c0, {c0: 151296}, None),
        MemoryDep('buf130', c0, {c0: 151296}, None)]
buf132.met_dependencies = [MemoryDep('arg99_1', c1, {c0: 197, c1: 768}, None)]
buf132.users = [NodeUser(node=SchedulerNode(name='buf134'), can_inplace=False, is_weak=False)]
buf132.group.device = cuda:0
buf132.group.iteration = (197, 768)
buf132.sizes = ([197], [768])
arg99_1_layout = FixedLayout('cuda', torch.float32, size=[768], stride=[1])
buf130_layout = FixedLayout('cuda', torch.float32, size=[197, 768], stride=[768, 1])
buf123_layout = FixedLayout('cuda', torch.float32, size=[1, 197, 768], stride=[151296, 768, 1])
buf132_layout = FixedLayout('cuda', torch.float32, size=[1, 197, 1], stride=[197, 1, 197])
class buf132_loop_body:
    var_ranges = {z0: 197, z1: 768}
    index0 = 768*z0 + z1
    index1 = z1
    index2 = z0
    def body(self, ops):
        get_index = self.get_index('index0')
        load = ops.load('buf130', get_index)
        get_index_1 = self.get_index('index1')
        load_1 = ops.load('arg99_1', get_index_1)
        add = ops.add(load, load_1)
        get_index_2 = self.get_index('index0')
        load_2 = ops.load('buf123', get_index_2)
        add_1 = ops.add(add, load_2)
        reduction = ops.reduction(torch.float32, torch.float32, 'welford_reduce', add_1)
        getitem = reduction[0]
        getitem_1 = reduction[1]
        getitem_2 = reduction[2]
        get_index_3 = self.get_index('index2')
        store_reduction = ops.store_reduction('buf132', get_index_3, getitem_1)
        return store_reduction
buf132 Triton code:
    import triton
    import triton.language as tl
    from triton.compiler.compiler import AttrsDescriptor

    from torch._inductor.runtime import triton_helpers, triton_heuristics
    from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
    from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, instance_descriptor, DeviceProperties

    @triton_heuristics.persistent_reduction(
        size_hints=[256, 1024],
        reduction_hint=ReductionHint.INNER,
        filename=__file__,
        triton_meta={'signature': {0: '*fp32', 1: '*fp32', 2: '*fp32', 3: '*fp32', 4: 'i32', 5: 'i32'}, 'device': DeviceProperties(type='cuda', index=0, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, multi_processor_count=132), 'constants': {}, 'configs': [AttrsDescriptor(divisible_by_16=(0, 1, 2, 3, 5), equal_to_1=())]},
        inductor_meta={'autotune_hints': set(), 'kernel_name': 'Placeholder.DESCRIPTIVE_NAME', 'mutated_arg_names': [], 'no_x_dim': True, 'num_load': 3, 'num_reduction': 3, 'backend_hash': '3944FAAD9D18E987535CF029C4441E186640DD453AE5AE6831EDF736C09E0A47', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': False, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False}
    )
    @triton.jit
    def triton_(in_ptr0, in_ptr1, in_ptr2, out_ptr0, xnumel, rnumel):
        xnumel = 197
        XBLOCK: tl.constexpr = 1
        rnumel = 768
        RBLOCK: tl.constexpr = 1024
        xoffset = tl.program_id(0) * XBLOCK
        xindex = tl.full([1], xoffset, tl.int32)
        xmask = xindex < xnumel
        rindex = tl.arange(0, RBLOCK)[:]
        roffset = 0
        rmask = rindex < rnumel
        r1 = rindex
        x0 = xindex
        tmp0 = tl.load(in_ptr0 + (r1 + (768*x0)), rmask & xmask, other=0.0)
        tmp1 = tl.load(in_ptr1 + (r1), rmask, eviction_policy='evict_last', other=0.0)
        tmp3 = tl.load(in_ptr2 + (r1 + (768*x0)), rmask & xmask, other=0.0)
        tmp2 = tmp0 + tmp1
        tmp4 = tmp2 + tmp3
        tmp5 = tl.broadcast_to(tmp4, [RBLOCK])
        tmp7 = tl.where(rmask & xmask, tmp5, 0)
        tmp8 = tl.broadcast_to(tmp5, [RBLOCK])
        tmp10 = tl.where(rmask & xmask, tmp8, 0)
        tmp11 = triton_helpers.promote_to_tensor(tl.sum(tmp10, 0))
        tmp12 = tl.full([1], 768, tl.int32)
        tmp13 = tmp12.to(tl.float32)
        tmp14 = tmp11 / tmp13
        tmp15 = tmp5 - tmp14
        tmp16 = tmp15 * tmp15
        tmp17 = tl.broadcast_to(tmp16, [RBLOCK])
        tmp19 = tl.where(rmask & xmask, tmp17, 0)
        tmp20 = triton_helpers.promote_to_tensor(tl.sum(tmp19, 0))
        tl.store(out_ptr0 + (x0), tmp20, xmask)


buf134: SchedulerNode(ComputedBuffer)
buf134.writes = [MemoryDep('buf134', c0, {c0: 151296}, None)]
buf134.unmet_dependencies = 
    [   MemoryDep('buf123', c0, {c0: 151296}, None),
        MemoryDep('buf130', c0, {c0: 151296}, None),
        MemoryDep('buf131', c0, {c0: 197}, None),
        MemoryDep('buf132', c0, {c0: 197}, None)]
buf134.met_dependencies = 
    [   MemoryDep('arg100_1', c1, {c0: 197, c1: 768}, None),
        MemoryDep('arg101_1', c1, {c0: 197, c1: 768}, None),
        MemoryDep('arg99_1', c1, {c0: 197, c1: 768}, None)]
buf134.users = [NodeUser(node=ExternKernelSchedulerNode(name='buf135'), can_inplace=False, is_weak=False), NodeUser(node=ExternKernelSchedulerNode(name='buf136'), can_inplace=False, is_weak=False), NodeUser(node=ExternKernelSchedulerNode(name='buf137'), can_inplace=False, is_weak=False)]
buf134.group.device = cuda:0
buf134.group.iteration = (151296, 1)
buf134.sizes = ([197, 768], [])
buf132_layout = FixedLayout('cuda', torch.float32, size=[1, 197, 1], stride=[197, 1, 197])
arg99_1_layout = FixedLayout('cuda', torch.float32, size=[768], stride=[1])
buf130_layout = FixedLayout('cuda', torch.float32, size=[197, 768], stride=[768, 1])
buf131_layout = FixedLayout('cuda', torch.float32, size=[1, 197, 1], stride=[197, 1, 197])
arg100_1_layout = FixedLayout('cuda', torch.float32, size=[768], stride=[1])
arg101_1_layout = FixedLayout('cuda', torch.float32, size=[768], stride=[1])
buf123_layout = FixedLayout('cuda', torch.float32, size=[1, 197, 768], stride=[151296, 768, 1])
buf134_layout = FixedLayout('cuda', torch.float32, size=[1, 197, 768], stride=[151296, 768, 1])
class buf134_loop_body:
    var_ranges = {z0: 197, z1: 768}
    index0 = 768*z0 + z1
    index1 = z1
    index2 = z0
    def body(self, ops):
        get_index = self.get_index('index0')
        load = ops.load('buf130', get_index)
        get_index_1 = self.get_index('index1')
        load_1 = ops.load('arg99_1', get_index_1)
        add = ops.add(load, load_1)
        get_index_2 = self.get_index('index0')
        load_2 = ops.load('buf123', get_index_2)
        add_1 = ops.add(add, load_2)
        get_index_3 = self.get_index('index2')
        load_3 = ops.load('buf131', get_index_3)
        sub = ops.sub(add_1, load_3)
        get_index_4 = self.get_index('index2')
        load_4 = ops.load('buf132', get_index_4)
        constant = ops.constant(768.0, torch.float32)
        truediv = ops.truediv(load_4, constant)
        constant_1 = ops.constant(1e-12, torch.float32)
        add_2 = ops.add(truediv, constant_1)
        rsqrt = ops.rsqrt(add_2)
        mul = ops.mul(sub, rsqrt)
        get_index_5 = self.get_index('index1')
        load_5 = ops.load('arg100_1', get_index_5)
        mul_1 = ops.mul(mul, load_5)
        get_index_6 = self.get_index('index1')
        load_6 = ops.load('arg101_1', get_index_6)
        add_3 = ops.add(mul_1, load_6)
        get_index_7 = self.get_index('index0')
        store = ops.store('buf134', get_index_7, add_3, None)
        return store
buf134 Triton code:
    import triton
    import triton.language as tl
    from triton.compiler.compiler import AttrsDescriptor

    from torch._inductor.runtime import triton_helpers, triton_heuristics
    from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
    from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, instance_descriptor, DeviceProperties

    @triton_heuristics.pointwise(
        size_hints=[262144], 
        filename=__file__,
        triton_meta={'signature': {0: '*fp32', 1: '*fp32', 2: '*fp32', 3: '*fp32', 4: '*fp32', 5: '*fp32', 6: '*fp32', 7: '*fp32', 8: 'i32'}, 'device': DeviceProperties(type='cuda', index=0, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, multi_processor_count=132), 'constants': {}, 'configs': [AttrsDescriptor(divisible_by_16=(0, 1, 2, 3, 4, 5, 6, 7, 8), equal_to_1=())]},
        inductor_meta={'autotune_hints': set(), 'kernel_name': 'Placeholder.DESCRIPTIVE_NAME', 'mutated_arg_names': [], 'no_x_dim': False, 'num_load': 7, 'num_reduction': 0, 'backend_hash': '3944FAAD9D18E987535CF029C4441E186640DD453AE5AE6831EDF736C09E0A47', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': False, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False},
        min_elem_per_thread=0
    )
    @triton.jit
    def triton_(in_ptr0, in_ptr1, in_ptr2, in_ptr3, in_ptr4, in_ptr5, in_ptr6, out_ptr0, xnumel, XBLOCK : tl.constexpr):
        xnumel = 151296
        xoffset = tl.program_id(0) * XBLOCK
        xindex = xoffset + tl.arange(0, XBLOCK)[:]
        xmask = xindex < xnumel
        x2 = xindex
        x0 = xindex % 768
        x1 = (xindex // 768)
        tmp0 = tl.load(in_ptr0 + (x2), xmask)
        tmp1 = tl.load(in_ptr1 + (x0), xmask, eviction_policy='evict_last')
        tmp3 = tl.load(in_ptr2 + (x2), xmask)
        tmp5 = tl.load(in_ptr3 + (x1), xmask, eviction_policy='evict_last')
        tmp7 = tl.load(in_ptr4 + (x1), xmask, eviction_policy='evict_last')
        tmp14 = tl.load(in_ptr5 + (x0), xmask, eviction_policy='evict_last')
        tmp16 = tl.load(in_ptr6 + (x0), xmask, eviction_policy='evict_last')
        tmp2 = tmp0 + tmp1
        tmp4 = tmp2 + tmp3
        tmp6 = tmp4 - tmp5
        tmp8 = 768.0
        tmp9 = tmp7 / tmp8
        tmp10 = 1e-12
        tmp11 = tmp9 + tmp10
        tmp12 = libdevice.rsqrt(tmp11)
        tmp13 = tmp6 * tmp12
        tmp15 = tmp13 * tmp14
        tmp17 = tmp15 + tmp16
        tl.store(out_ptr0 + (x2), tmp17, xmask)


buf135: ExternKernelSchedulerNode(ExternKernelOut)
buf135.writes = [StarDep(name='buf135', mode=None)]
buf135.unmet_dependencies = [StarDep(name='buf134', mode=None)]
buf135.met_dependencies = [StarDep(name='arg102_1', mode=None), StarDep(name='arg103_1', mode=None)]
buf135.users = [NodeUser(node=ExternKernelSchedulerNode(name='buf138'), can_inplace=False, is_weak=False)]
buf135.node.kernel = extern_kernels.addmm


buf136: ExternKernelSchedulerNode(ExternKernelOut)
buf136.writes = [StarDep(name='buf136', mode=None)]
buf136.unmet_dependencies = [StarDep(name='buf134', mode=None)]
buf136.met_dependencies = [StarDep(name='arg104_1', mode=None), StarDep(name='arg105_1', mode=None)]
buf136.users = [NodeUser(node=ExternKernelSchedulerNode(name='buf138'), can_inplace=False, is_weak=False)]
buf136.node.kernel = extern_kernels.addmm


buf137: ExternKernelSchedulerNode(ExternKernelOut)
buf137.writes = [StarDep(name='buf137', mode=None)]
buf137.unmet_dependencies = [StarDep(name='buf134', mode=None)]
buf137.met_dependencies = [StarDep(name='arg106_1', mode=None), StarDep(name='arg107_1', mode=None)]
buf137.users = [NodeUser(node=ExternKernelSchedulerNode(name='buf138'), can_inplace=False, is_weak=False)]
buf137.node.kernel = extern_kernels.addmm


buf138: ExternKernelSchedulerNode(FallbackKernel)
buf138.writes = [StarDep(name='buf138', mode=None)]
buf138.unmet_dependencies = 
    [   StarDep(name='buf135', mode=None),
        StarDep(name='buf136', mode=None),
        StarDep(name='buf137', mode=None)]
buf138.met_dependencies = []
buf138.users = [NodeUser(node=ExternKernelSchedulerNode(name='buf139'), can_inplace=False, is_weak=False), NodeUser(node=ExternKernelSchedulerNode(name='buf140'), can_inplace=False, is_weak=False), NodeUser(node=ExternKernelSchedulerNode(name='buf141'), can_inplace=False, is_weak=False), NodeUser(node=ExternKernelSchedulerNode(name='buf142'), can_inplace=False, is_weak=False)]
buf138.node.kernel = None


buf139: ExternKernelSchedulerNode(MultiOutput)
buf139.writes = [StarDep(name='buf139', mode=None)]
buf139.unmet_dependencies = [StarDep(name='buf138', mode=None)]
buf139.met_dependencies = []
buf139.users = [NodeUser(node=ExternKernelSchedulerNode(name='buf143'), can_inplace=False, is_weak=False)]
buf139.node.kernel = None


buf143: ExternKernelSchedulerNode(ExternKernelOut)
buf143.writes = [StarDep(name='buf143', mode=None)]
buf143.unmet_dependencies = [StarDep(name='buf139', mode=None)]
buf143.met_dependencies = [StarDep(name='arg108_1', mode=None)]
buf143.users = [NodeUser(node=SchedulerNode(name='buf144'), can_inplace=True, is_weak=False)]
buf143.node.kernel = extern_kernels.mm


buf144: SchedulerNode(ComputedBuffer)
buf144.writes = [MemoryDep('buf144', c0, {c0: 151296}, None)]
buf144.unmet_dependencies = 
    [   MemoryDep('buf123', c0, {c0: 151296}, None),
        MemoryDep('buf130', c0, {c0: 151296}, None),
        MemoryDep('buf143', c0, {c0: 151296}, None)]
buf144.met_dependencies = 
    [   MemoryDep('arg109_1', c1, {c0: 197, c1: 768}, None),
        MemoryDep('arg99_1', c1, {c0: 197, c1: 768}, None)]
buf144.users = [NodeUser(node=SchedulerNode(name='buf145'), can_inplace=False, is_weak=False), NodeUser(node=SchedulerNode(name='buf146'), can_inplace=False, is_weak=False), NodeUser(node=SchedulerNode(name='buf147'), can_inplace=False, is_weak=False), NodeUser(node=SchedulerNode(name='buf148'), can_inplace=True, is_weak=False), NodeUser(node=SchedulerNode(name='buf152'), can_inplace=False, is_weak=False), NodeUser(node=SchedulerNode(name='buf153'), can_inplace=False, is_weak=False), NodeUser(node=SchedulerNode(name='buf154'), can_inplace=False, is_weak=False), NodeUser(node=SchedulerNode(name='buf155'), can_inplace=True, is_weak=False), NodeUser(node=SchedulerNode(name='buf165'), can_inplace=True, is_weak=False)]
buf144.group.device = cuda:0
buf144.group.iteration = (151296, 1)
buf144.sizes = ([197, 768], [])
buf143_layout = FixedLayout('cuda', torch.float32, size=[197, 768], stride=[768, 1])
arg99_1_layout = FixedLayout('cuda', torch.float32, size=[768], stride=[1])
buf130_layout = FixedLayout('cuda', torch.float32, size=[197, 768], stride=[768, 1])
arg109_1_layout = FixedLayout('cuda', torch.float32, size=[768], stride=[1])
buf123_layout = FixedLayout('cuda', torch.float32, size=[1, 197, 768], stride=[151296, 768, 1])
buf144_layout = FixedLayout('cuda', torch.float32, size=[1, 197, 768], stride=[151296, 768, 1])
class buf144_loop_body:
    var_ranges = {z0: 197, z1: 768}
    index0 = 768*z0 + z1
    index1 = z1
    def body(self, ops):
        get_index = self.get_index('index0')
        load = ops.load('buf143', get_index)
        get_index_1 = self.get_index('index1')
        load_1 = ops.load('arg109_1', get_index_1)
        add = ops.add(load, load_1)
        get_index_2 = self.get_index('index0')
        load_2 = ops.load('buf130', get_index_2)
        get_index_3 = self.get_index('index1')
        load_3 = ops.load('arg99_1', get_index_3)
        add_1 = ops.add(load_2, load_3)
        get_index_4 = self.get_index('index0')
        load_4 = ops.load('buf123', get_index_4)
        add_2 = ops.add(add_1, load_4)
        add_3 = ops.add(add, add_2)
        get_index_5 = self.get_index('index0')
        store = ops.store('buf144', get_index_5, add_3, None)
        return store
buf144 Triton code:
    import triton
    import triton.language as tl
    from triton.compiler.compiler import AttrsDescriptor

    from torch._inductor.runtime import triton_helpers, triton_heuristics
    from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
    from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, instance_descriptor, DeviceProperties

    @triton_heuristics.pointwise(
        size_hints=[262144], 
        filename=__file__,
        triton_meta={'signature': {0: '*fp32', 1: '*fp32', 2: '*fp32', 3: '*fp32', 4: '*fp32', 5: 'i32'}, 'device': DeviceProperties(type='cuda', index=0, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, multi_processor_count=132), 'constants': {}, 'configs': [AttrsDescriptor(divisible_by_16=(0, 1, 2, 3, 4, 5), equal_to_1=())]},
        inductor_meta={'autotune_hints': set(), 'kernel_name': 'Placeholder.DESCRIPTIVE_NAME', 'mutated_arg_names': ['in_out_ptr0'], 'no_x_dim': False, 'num_load': 5, 'num_reduction': 0, 'backend_hash': '3944FAAD9D18E987535CF029C4441E186640DD453AE5AE6831EDF736C09E0A47', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': False, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False},
        min_elem_per_thread=0
    )
    @triton.jit
    def triton_(in_out_ptr0, in_ptr0, in_ptr1, in_ptr2, in_ptr3, xnumel, XBLOCK : tl.constexpr):
        xnumel = 151296
        xoffset = tl.program_id(0) * XBLOCK
        xindex = xoffset + tl.arange(0, XBLOCK)[:]
        xmask = xindex < xnumel
        x2 = xindex
        x0 = xindex % 768
        tmp0 = tl.load(in_out_ptr0 + (x2), xmask)
        tmp1 = tl.load(in_ptr0 + (x0), xmask, eviction_policy='evict_last')
        tmp3 = tl.load(in_ptr1 + (x2), xmask)
        tmp4 = tl.load(in_ptr2 + (x0), xmask, eviction_policy='evict_last')
        tmp6 = tl.load(in_ptr3 + (x2), xmask)
        tmp2 = tmp0 + tmp1
        tmp5 = tmp3 + tmp4
        tmp7 = tmp5 + tmp6
        tmp8 = tmp2 + tmp7
        tl.store(in_out_ptr0 + (x2), tmp8, xmask)


buf145: SchedulerNode(ComputedBuffer)
buf145.writes = [MemoryDep('buf145', c0, {c0: 197}, None)]
buf145.unmet_dependencies = [MemoryDep('buf144', c0, {c0: 151296}, None)]
buf145.met_dependencies = []
buf145.users = [NodeUser(node=SchedulerNode(name='buf148'), can_inplace=False, is_weak=False)]
buf145.group.device = cuda:0
buf145.group.iteration = (197, 768)
buf145.sizes = ([197], [768])
buf144_layout = FixedLayout('cuda', torch.float32, size=[1, 197, 768], stride=[151296, 768, 1])
buf145_layout = FixedLayout('cuda', torch.float32, size=[1, 197, 1], stride=[197, 1, 197])
class buf145_loop_body:
    var_ranges = {z0: 197, z1: 768}
    index0 = 768*z0 + z1
    index1 = z0
    def body(self, ops):
        get_index = self.get_index('index0')
        load = ops.load('buf144', get_index)
        reduction = ops.reduction(torch.float32, torch.float32, 'welford_reduce', load)
        getitem = reduction[0]
        getitem_1 = reduction[1]
        getitem_2 = reduction[2]
        get_index_1 = self.get_index('index1')
        store_reduction = ops.store_reduction('buf145', get_index_1, getitem)
        return store_reduction
buf145 Triton code:
    import triton
    import triton.language as tl
    from triton.compiler.compiler import AttrsDescriptor

    from torch._inductor.runtime import triton_helpers, triton_heuristics
    from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
    from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, instance_descriptor, DeviceProperties

    @triton_heuristics.persistent_reduction(
        size_hints=[256, 1024],
        reduction_hint=ReductionHint.INNER,
        filename=__file__,
        triton_meta={'signature': {0: '*fp32', 1: '*fp32', 2: 'i32', 3: 'i32'}, 'device': DeviceProperties(type='cuda', index=0, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, multi_processor_count=132), 'constants': {}, 'configs': [AttrsDescriptor(divisible_by_16=(0, 1, 3), equal_to_1=())]},
        inductor_meta={'autotune_hints': set(), 'kernel_name': 'Placeholder.DESCRIPTIVE_NAME', 'mutated_arg_names': [], 'no_x_dim': True, 'num_load': 1, 'num_reduction': 3, 'backend_hash': '3944FAAD9D18E987535CF029C4441E186640DD453AE5AE6831EDF736C09E0A47', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': False, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False}
    )
    @triton.jit
    def triton_(in_ptr0, out_ptr0, xnumel, rnumel):
        xnumel = 197
        XBLOCK: tl.constexpr = 1
        rnumel = 768
        RBLOCK: tl.constexpr = 1024
        xoffset = tl.program_id(0) * XBLOCK
        xindex = tl.full([1], xoffset, tl.int32)
        xmask = xindex < xnumel
        rindex = tl.arange(0, RBLOCK)[:]
        roffset = 0
        rmask = rindex < rnumel
        r1 = rindex
        x0 = xindex
        tmp0 = tl.load(in_ptr0 + (r1 + (768*x0)), rmask & xmask, other=0.0)
        tmp1 = tl.broadcast_to(tmp0, [RBLOCK])
        tmp3 = tl.where(rmask & xmask, tmp1, 0)
        tmp4 = tl.broadcast_to(tmp1, [RBLOCK])
        tmp6 = tl.where(rmask & xmask, tmp4, 0)
        tmp7 = triton_helpers.promote_to_tensor(tl.sum(tmp6, 0))
        tmp8 = tl.full([1], 768, tl.int32)
        tmp9 = tmp8.to(tl.float32)
        tmp10 = tmp7 / tmp9
        tmp11 = tmp1 - tmp10
        tmp12 = tmp11 * tmp11
        tmp13 = tl.broadcast_to(tmp12, [RBLOCK])
        tmp15 = tl.where(rmask & xmask, tmp13, 0)
        tmp16 = triton_helpers.promote_to_tensor(tl.sum(tmp15, 0))
        tl.store(out_ptr0 + (x0), tmp10, xmask)


buf146: SchedulerNode(ComputedBuffer)
buf146.writes = [MemoryDep('buf146', c0, {c0: 197}, None)]
buf146.unmet_dependencies = [MemoryDep('buf144', c0, {c0: 151296}, None)]
buf146.met_dependencies = []
buf146.users = [NodeUser(node=SchedulerNode(name='buf148'), can_inplace=False, is_weak=False)]
buf146.group.device = cuda:0
buf146.group.iteration = (197, 768)
buf146.sizes = ([197], [768])
buf144_layout = FixedLayout('cuda', torch.float32, size=[1, 197, 768], stride=[151296, 768, 1])
buf146_layout = FixedLayout('cuda', torch.float32, size=[1, 197, 1], stride=[197, 1, 197])
class buf146_loop_body:
    var_ranges = {z0: 197, z1: 768}
    index0 = 768*z0 + z1
    index1 = z0
    def body(self, ops):
        get_index = self.get_index('index0')
        load = ops.load('buf144', get_index)
        reduction = ops.reduction(torch.float32, torch.float32, 'welford_reduce', load)
        getitem = reduction[0]
        getitem_1 = reduction[1]
        getitem_2 = reduction[2]
        get_index_1 = self.get_index('index1')
        store_reduction = ops.store_reduction('buf146', get_index_1, getitem_1)
        return store_reduction
buf146 Triton code:
    import triton
    import triton.language as tl
    from triton.compiler.compiler import AttrsDescriptor

    from torch._inductor.runtime import triton_helpers, triton_heuristics
    from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
    from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, instance_descriptor, DeviceProperties

    @triton_heuristics.persistent_reduction(
        size_hints=[256, 1024],
        reduction_hint=ReductionHint.INNER,
        filename=__file__,
        triton_meta={'signature': {0: '*fp32', 1: '*fp32', 2: 'i32', 3: 'i32'}, 'device': DeviceProperties(type='cuda', index=0, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, multi_processor_count=132), 'constants': {}, 'configs': [AttrsDescriptor(divisible_by_16=(0, 1, 3), equal_to_1=())]},
        inductor_meta={'autotune_hints': set(), 'kernel_name': 'Placeholder.DESCRIPTIVE_NAME', 'mutated_arg_names': [], 'no_x_dim': True, 'num_load': 1, 'num_reduction': 3, 'backend_hash': '3944FAAD9D18E987535CF029C4441E186640DD453AE5AE6831EDF736C09E0A47', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': False, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False}
    )
    @triton.jit
    def triton_(in_ptr0, out_ptr0, xnumel, rnumel):
        xnumel = 197
        XBLOCK: tl.constexpr = 1
        rnumel = 768
        RBLOCK: tl.constexpr = 1024
        xoffset = tl.program_id(0) * XBLOCK
        xindex = tl.full([1], xoffset, tl.int32)
        xmask = xindex < xnumel
        rindex = tl.arange(0, RBLOCK)[:]
        roffset = 0
        rmask = rindex < rnumel
        r1 = rindex
        x0 = xindex
        tmp0 = tl.load(in_ptr0 + (r1 + (768*x0)), rmask & xmask, other=0.0)
        tmp1 = tl.broadcast_to(tmp0, [RBLOCK])
        tmp3 = tl.where(rmask & xmask, tmp1, 0)
        tmp4 = tl.broadcast_to(tmp1, [RBLOCK])
        tmp6 = tl.where(rmask & xmask, tmp4, 0)
        tmp7 = triton_helpers.promote_to_tensor(tl.sum(tmp6, 0))
        tmp8 = tl.full([1], 768, tl.int32)
        tmp9 = tmp8.to(tl.float32)
        tmp10 = tmp7 / tmp9
        tmp11 = tmp1 - tmp10
        tmp12 = tmp11 * tmp11
        tmp13 = tl.broadcast_to(tmp12, [RBLOCK])
        tmp15 = tl.where(rmask & xmask, tmp13, 0)
        tmp16 = triton_helpers.promote_to_tensor(tl.sum(tmp15, 0))
        tl.store(out_ptr0 + (x0), tmp16, xmask)


buf148: SchedulerNode(ComputedBuffer)
buf148.writes = [MemoryDep('buf148', c0, {c0: 151296}, None)]
buf148.unmet_dependencies = 
    [   MemoryDep('buf144', c0, {c0: 151296}, None),
        MemoryDep('buf145', c0, {c0: 197}, None),
        MemoryDep('buf146', c0, {c0: 197}, None)]
buf148.met_dependencies = 
    [   MemoryDep('arg110_1', c1, {c0: 197, c1: 768}, None),
        MemoryDep('arg111_1', c1, {c0: 197, c1: 768}, None)]
buf148.users = [NodeUser(node=ExternKernelSchedulerNode(name='buf149'), can_inplace=False, is_weak=False)]
buf148.group.device = cuda:0
buf148.group.iteration = (151296, 1)
buf148.sizes = ([197, 768], [])
buf146_layout = FixedLayout('cuda', torch.float32, size=[1, 197, 1], stride=[197, 1, 197])
arg111_1_layout = FixedLayout('cuda', torch.float32, size=[768], stride=[1])
buf145_layout = FixedLayout('cuda', torch.float32, size=[1, 197, 1], stride=[197, 1, 197])
arg110_1_layout = FixedLayout('cuda', torch.float32, size=[768], stride=[1])
buf144_layout = FixedLayout('cuda', torch.float32, size=[1, 197, 768], stride=[151296, 768, 1])
buf148_layout = FixedLayout('cuda', torch.float32, size=[1, 197, 768], stride=[151296, 768, 1])
class buf148_loop_body:
    var_ranges = {z0: 197, z1: 768}
    index0 = 768*z0 + z1
    index1 = z0
    index2 = z1
    def body(self, ops):
        get_index = self.get_index('index0')
        load = ops.load('buf144', get_index)
        get_index_1 = self.get_index('index1')
        load_1 = ops.load('buf145', get_index_1)
        sub = ops.sub(load, load_1)
        get_index_2 = self.get_index('index1')
        load_2 = ops.load('buf146', get_index_2)
        constant = ops.constant(768.0, torch.float32)
        truediv = ops.truediv(load_2, constant)
        constant_1 = ops.constant(1e-12, torch.float32)
        add = ops.add(truediv, constant_1)
        rsqrt = ops.rsqrt(add)
        mul = ops.mul(sub, rsqrt)
        get_index_3 = self.get_index('index2')
        load_3 = ops.load('arg110_1', get_index_3)
        mul_1 = ops.mul(mul, load_3)
        get_index_4 = self.get_index('index2')
        load_4 = ops.load('arg111_1', get_index_4)
        add_1 = ops.add(mul_1, load_4)
        get_index_5 = self.get_index('index0')
        store = ops.store('buf148', get_index_5, add_1, None)
        return store
buf148 Triton code:
    import triton
    import triton.language as tl
    from triton.compiler.compiler import AttrsDescriptor

    from torch._inductor.runtime import triton_helpers, triton_heuristics
    from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
    from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, instance_descriptor, DeviceProperties

    @triton_heuristics.pointwise(
        size_hints=[262144], 
        filename=__file__,
        triton_meta={'signature': {0: '*fp32', 1: '*fp32', 2: '*fp32', 3: '*fp32', 4: '*fp32', 5: '*fp32', 6: 'i32'}, 'device': DeviceProperties(type='cuda', index=0, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, multi_processor_count=132), 'constants': {}, 'configs': [AttrsDescriptor(divisible_by_16=(0, 1, 2, 3, 4, 5, 6), equal_to_1=())]},
        inductor_meta={'autotune_hints': set(), 'kernel_name': 'Placeholder.DESCRIPTIVE_NAME', 'mutated_arg_names': [], 'no_x_dim': False, 'num_load': 5, 'num_reduction': 0, 'backend_hash': '3944FAAD9D18E987535CF029C4441E186640DD453AE5AE6831EDF736C09E0A47', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': False, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False},
        min_elem_per_thread=0
    )
    @triton.jit
    def triton_(in_ptr0, in_ptr1, in_ptr2, in_ptr3, in_ptr4, out_ptr0, xnumel, XBLOCK : tl.constexpr):
        xnumel = 151296
        xoffset = tl.program_id(0) * XBLOCK
        xindex = xoffset + tl.arange(0, XBLOCK)[:]
        xmask = xindex < xnumel
        x2 = xindex
        x1 = (xindex // 768)
        x0 = xindex % 768
        tmp0 = tl.load(in_ptr0 + (x2), xmask)
        tmp1 = tl.load(in_ptr1 + (x1), xmask, eviction_policy='evict_last')
        tmp3 = tl.load(in_ptr2 + (x1), xmask, eviction_policy='evict_last')
        tmp10 = tl.load(in_ptr3 + (x0), xmask, eviction_policy='evict_last')
        tmp12 = tl.load(in_ptr4 + (x0), xmask, eviction_policy='evict_last')
        tmp2 = tmp0 - tmp1
        tmp4 = 768.0
        tmp5 = tmp3 / tmp4
        tmp6 = 1e-12
        tmp7 = tmp5 + tmp6
        tmp8 = libdevice.rsqrt(tmp7)
        tmp9 = tmp2 * tmp8
        tmp11 = tmp9 * tmp10
        tmp13 = tmp11 + tmp12
        tl.store(out_ptr0 + (x2), tmp13, xmask)


buf149: ExternKernelSchedulerNode(ExternKernelOut)
buf149.writes = [StarDep(name='buf149', mode=None)]
buf149.unmet_dependencies = [StarDep(name='buf148', mode=None)]
buf149.met_dependencies = [StarDep(name='arg112_1', mode=None)]
buf149.users = [NodeUser(node=SchedulerNode(name='buf150'), can_inplace=True, is_weak=False)]
buf149.node.kernel = extern_kernels.mm


buf150: SchedulerNode(ComputedBuffer)
buf150.writes = [MemoryDep('buf150', c0, {c0: 605184}, None)]
buf150.unmet_dependencies = [MemoryDep('buf149', c0, {c0: 605184}, None)]
buf150.met_dependencies = [MemoryDep('arg113_1', c1, {c0: 197, c1: 3072}, None)]
buf150.users = [NodeUser(node=ExternKernelSchedulerNode(name='buf151'), can_inplace=False, is_weak=False)]
buf150.group.device = cuda:0
buf150.group.iteration = (605184, 1)
buf150.sizes = ([197, 3072], [])
buf149_layout = FixedLayout('cuda', torch.float32, size=[197, 3072], stride=[3072, 1])
arg113_1_layout = FixedLayout('cuda', torch.float32, size=[3072], stride=[1])
buf150_layout = FixedLayout('cuda', torch.float32, size=[1, 197, 3072], stride=[605184, 3072, 1])
class buf150_loop_body:
    var_ranges = {z0: 197, z1: 3072}
    index0 = 3072*z0 + z1
    index1 = z1
    def body(self, ops):
        get_index = self.get_index('index0')
        load = ops.load('buf149', get_index)
        get_index_1 = self.get_index('index1')
        load_1 = ops.load('arg113_1', get_index_1)
        add = ops.add(load, load_1)
        constant = ops.constant(0.5, torch.float32)
        mul = ops.mul(add, constant)
        get_index_2 = self.get_index('index0')
        load_2 = ops.load('buf149', get_index_2)
        get_index_3 = self.get_index('index1')
        load_3 = ops.load('arg113_1', get_index_3)
        add_1 = ops.add(load_2, load_3)
        constant_1 = ops.constant(0.7071067811865476, torch.float32)
        mul_1 = ops.mul(add_1, constant_1)
        erf = ops.erf(mul_1)
        constant_2 = ops.constant(1.0, torch.float32)
        add_2 = ops.add(erf, constant_2)
        mul_2 = ops.mul(mul, add_2)
        get_index_4 = self.get_index('index0')
        store = ops.store('buf150', get_index_4, mul_2, None)
        return store
buf150 Triton code:
    import triton
    import triton.language as tl
    from triton.compiler.compiler import AttrsDescriptor

    from torch._inductor.runtime import triton_helpers, triton_heuristics
    from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
    from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, instance_descriptor, DeviceProperties

    @triton_heuristics.pointwise(
        size_hints=[1048576], 
        filename=__file__,
        triton_meta={'signature': {0: '*fp32', 1: '*fp32', 2: 'i32'}, 'device': DeviceProperties(type='cuda', index=0, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, multi_processor_count=132), 'constants': {}, 'configs': [AttrsDescriptor(divisible_by_16=(0, 1, 2), equal_to_1=())]},
        inductor_meta={'autotune_hints': set(), 'kernel_name': 'Placeholder.DESCRIPTIVE_NAME', 'mutated_arg_names': ['in_out_ptr0'], 'no_x_dim': False, 'num_load': 2, 'num_reduction': 0, 'backend_hash': '3944FAAD9D18E987535CF029C4441E186640DD453AE5AE6831EDF736C09E0A47', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': False, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False},
        min_elem_per_thread=0
    )
    @triton.jit
    def triton_(in_out_ptr0, in_ptr0, xnumel, XBLOCK : tl.constexpr):
        xnumel = 605184
        xoffset = tl.program_id(0) * XBLOCK
        xindex = xoffset + tl.arange(0, XBLOCK)[:]
        xmask = xindex < xnumel
        x2 = xindex
        x0 = xindex % 3072
        tmp0 = tl.load(in_out_ptr0 + (x2), xmask)
        tmp1 = tl.load(in_ptr0 + (x0), xmask, eviction_policy='evict_last')
        tmp2 = tmp0 + tmp1
        tmp3 = 0.5
        tmp4 = tmp2 * tmp3
        tmp5 = 0.7071067811865476
        tmp6 = tmp2 * tmp5
        tmp7 = libdevice.erf(tmp6)
        tmp8 = 1.0
        tmp9 = tmp7 + tmp8
        tmp10 = tmp4 * tmp9
        tl.store(in_out_ptr0 + (x2), tmp10, xmask)


buf151: ExternKernelSchedulerNode(ExternKernelOut)
buf151.writes = [StarDep(name='buf151', mode=None)]
buf151.unmet_dependencies = [StarDep(name='buf150', mode=None)]
buf151.met_dependencies = [StarDep(name='arg114_1', mode=None)]
buf151.users = [NodeUser(node=SchedulerNode(name='buf152'), can_inplace=False, is_weak=False), NodeUser(node=SchedulerNode(name='buf153'), can_inplace=False, is_weak=False), NodeUser(node=SchedulerNode(name='buf154'), can_inplace=False, is_weak=False), NodeUser(node=SchedulerNode(name='buf155'), can_inplace=True, is_weak=False), NodeUser(node=SchedulerNode(name='buf165'), can_inplace=True, is_weak=False)]
buf151.node.kernel = extern_kernels.mm


buf152: SchedulerNode(ComputedBuffer)
buf152.writes = [MemoryDep('buf152', c0, {c0: 197}, None)]
buf152.unmet_dependencies = 
    [   MemoryDep('buf144', c0, {c0: 151296}, None),
        MemoryDep('buf151', c0, {c0: 151296}, None)]
buf152.met_dependencies = [MemoryDep('arg115_1', c1, {c0: 197, c1: 768}, None)]
buf152.users = [NodeUser(node=SchedulerNode(name='buf155'), can_inplace=False, is_weak=False)]
buf152.group.device = cuda:0
buf152.group.iteration = (197, 768)
buf152.sizes = ([197], [768])
buf151_layout = FixedLayout('cuda', torch.float32, size=[197, 768], stride=[768, 1])
arg115_1_layout = FixedLayout('cuda', torch.float32, size=[768], stride=[1])
buf144_layout = FixedLayout('cuda', torch.float32, size=[1, 197, 768], stride=[151296, 768, 1])
buf152_layout = FixedLayout('cuda', torch.float32, size=[1, 197, 1], stride=[197, 1, 197])
class buf152_loop_body:
    var_ranges = {z0: 197, z1: 768}
    index0 = 768*z0 + z1
    index1 = z1
    index2 = z0
    def body(self, ops):
        get_index = self.get_index('index0')
        load = ops.load('buf151', get_index)
        get_index_1 = self.get_index('index1')
        load_1 = ops.load('arg115_1', get_index_1)
        add = ops.add(load, load_1)
        get_index_2 = self.get_index('index0')
        load_2 = ops.load('buf144', get_index_2)
        add_1 = ops.add(add, load_2)
        reduction = ops.reduction(torch.float32, torch.float32, 'welford_reduce', add_1)
        getitem = reduction[0]
        getitem_1 = reduction[1]
        getitem_2 = reduction[2]
        get_index_3 = self.get_index('index2')
        store_reduction = ops.store_reduction('buf152', get_index_3, getitem)
        return store_reduction
buf152 Triton code:
    import triton
    import triton.language as tl
    from triton.compiler.compiler import AttrsDescriptor

    from torch._inductor.runtime import triton_helpers, triton_heuristics
    from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
    from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, instance_descriptor, DeviceProperties

    @triton_heuristics.persistent_reduction(
        size_hints=[256, 1024],
        reduction_hint=ReductionHint.INNER,
        filename=__file__,
        triton_meta={'signature': {0: '*fp32', 1: '*fp32', 2: '*fp32', 3: '*fp32', 4: 'i32', 5: 'i32'}, 'device': DeviceProperties(type='cuda', index=0, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, multi_processor_count=132), 'constants': {}, 'configs': [AttrsDescriptor(divisible_by_16=(0, 1, 2, 3, 5), equal_to_1=())]},
        inductor_meta={'autotune_hints': set(), 'kernel_name': 'Placeholder.DESCRIPTIVE_NAME', 'mutated_arg_names': [], 'no_x_dim': True, 'num_load': 3, 'num_reduction': 3, 'backend_hash': '3944FAAD9D18E987535CF029C4441E186640DD453AE5AE6831EDF736C09E0A47', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': False, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False}
    )
    @triton.jit
    def triton_(in_ptr0, in_ptr1, in_ptr2, out_ptr0, xnumel, rnumel):
        xnumel = 197
        XBLOCK: tl.constexpr = 1
        rnumel = 768
        RBLOCK: tl.constexpr = 1024
        xoffset = tl.program_id(0) * XBLOCK
        xindex = tl.full([1], xoffset, tl.int32)
        xmask = xindex < xnumel
        rindex = tl.arange(0, RBLOCK)[:]
        roffset = 0
        rmask = rindex < rnumel
        r1 = rindex
        x0 = xindex
        tmp0 = tl.load(in_ptr0 + (r1 + (768*x0)), rmask & xmask, other=0.0)
        tmp1 = tl.load(in_ptr1 + (r1), rmask, eviction_policy='evict_last', other=0.0)
        tmp3 = tl.load(in_ptr2 + (r1 + (768*x0)), rmask & xmask, other=0.0)
        tmp2 = tmp0 + tmp1
        tmp4 = tmp2 + tmp3
        tmp5 = tl.broadcast_to(tmp4, [RBLOCK])
        tmp7 = tl.where(rmask & xmask, tmp5, 0)
        tmp8 = tl.broadcast_to(tmp5, [RBLOCK])
        tmp10 = tl.where(rmask & xmask, tmp8, 0)
        tmp11 = triton_helpers.promote_to_tensor(tl.sum(tmp10, 0))
        tmp12 = tl.full([1], 768, tl.int32)
        tmp13 = tmp12.to(tl.float32)
        tmp14 = tmp11 / tmp13
        tmp15 = tmp5 - tmp14
        tmp16 = tmp15 * tmp15
        tmp17 = tl.broadcast_to(tmp16, [RBLOCK])
        tmp19 = tl.where(rmask & xmask, tmp17, 0)
        tmp20 = triton_helpers.promote_to_tensor(tl.sum(tmp19, 0))
        tl.store(out_ptr0 + (x0), tmp14, xmask)


buf153: SchedulerNode(ComputedBuffer)
buf153.writes = [MemoryDep('buf153', c0, {c0: 197}, None)]
buf153.unmet_dependencies = 
    [   MemoryDep('buf144', c0, {c0: 151296}, None),
        MemoryDep('buf151', c0, {c0: 151296}, None)]
buf153.met_dependencies = [MemoryDep('arg115_1', c1, {c0: 197, c1: 768}, None)]
buf153.users = [NodeUser(node=SchedulerNode(name='buf155'), can_inplace=False, is_weak=False)]
buf153.group.device = cuda:0
buf153.group.iteration = (197, 768)
buf153.sizes = ([197], [768])
buf151_layout = FixedLayout('cuda', torch.float32, size=[197, 768], stride=[768, 1])
arg115_1_layout = FixedLayout('cuda', torch.float32, size=[768], stride=[1])
buf144_layout = FixedLayout('cuda', torch.float32, size=[1, 197, 768], stride=[151296, 768, 1])
buf153_layout = FixedLayout('cuda', torch.float32, size=[1, 197, 1], stride=[197, 1, 197])
class buf153_loop_body:
    var_ranges = {z0: 197, z1: 768}
    index0 = 768*z0 + z1
    index1 = z1
    index2 = z0
    def body(self, ops):
        get_index = self.get_index('index0')
        load = ops.load('buf151', get_index)
        get_index_1 = self.get_index('index1')
        load_1 = ops.load('arg115_1', get_index_1)
        add = ops.add(load, load_1)
        get_index_2 = self.get_index('index0')
        load_2 = ops.load('buf144', get_index_2)
        add_1 = ops.add(add, load_2)
        reduction = ops.reduction(torch.float32, torch.float32, 'welford_reduce', add_1)
        getitem = reduction[0]
        getitem_1 = reduction[1]
        getitem_2 = reduction[2]
        get_index_3 = self.get_index('index2')
        store_reduction = ops.store_reduction('buf153', get_index_3, getitem_1)
        return store_reduction
buf153 Triton code:
    import triton
    import triton.language as tl
    from triton.compiler.compiler import AttrsDescriptor

    from torch._inductor.runtime import triton_helpers, triton_heuristics
    from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
    from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, instance_descriptor, DeviceProperties

    @triton_heuristics.persistent_reduction(
        size_hints=[256, 1024],
        reduction_hint=ReductionHint.INNER,
        filename=__file__,
        triton_meta={'signature': {0: '*fp32', 1: '*fp32', 2: '*fp32', 3: '*fp32', 4: 'i32', 5: 'i32'}, 'device': DeviceProperties(type='cuda', index=0, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, multi_processor_count=132), 'constants': {}, 'configs': [AttrsDescriptor(divisible_by_16=(0, 1, 2, 3, 5), equal_to_1=())]},
        inductor_meta={'autotune_hints': set(), 'kernel_name': 'Placeholder.DESCRIPTIVE_NAME', 'mutated_arg_names': [], 'no_x_dim': True, 'num_load': 3, 'num_reduction': 3, 'backend_hash': '3944FAAD9D18E987535CF029C4441E186640DD453AE5AE6831EDF736C09E0A47', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': False, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False}
    )
    @triton.jit
    def triton_(in_ptr0, in_ptr1, in_ptr2, out_ptr0, xnumel, rnumel):
        xnumel = 197
        XBLOCK: tl.constexpr = 1
        rnumel = 768
        RBLOCK: tl.constexpr = 1024
        xoffset = tl.program_id(0) * XBLOCK
        xindex = tl.full([1], xoffset, tl.int32)
        xmask = xindex < xnumel
        rindex = tl.arange(0, RBLOCK)[:]
        roffset = 0
        rmask = rindex < rnumel
        r1 = rindex
        x0 = xindex
        tmp0 = tl.load(in_ptr0 + (r1 + (768*x0)), rmask & xmask, other=0.0)
        tmp1 = tl.load(in_ptr1 + (r1), rmask, eviction_policy='evict_last', other=0.0)
        tmp3 = tl.load(in_ptr2 + (r1 + (768*x0)), rmask & xmask, other=0.0)
        tmp2 = tmp0 + tmp1
        tmp4 = tmp2 + tmp3
        tmp5 = tl.broadcast_to(tmp4, [RBLOCK])
        tmp7 = tl.where(rmask & xmask, tmp5, 0)
        tmp8 = tl.broadcast_to(tmp5, [RBLOCK])
        tmp10 = tl.where(rmask & xmask, tmp8, 0)
        tmp11 = triton_helpers.promote_to_tensor(tl.sum(tmp10, 0))
        tmp12 = tl.full([1], 768, tl.int32)
        tmp13 = tmp12.to(tl.float32)
        tmp14 = tmp11 / tmp13
        tmp15 = tmp5 - tmp14
        tmp16 = tmp15 * tmp15
        tmp17 = tl.broadcast_to(tmp16, [RBLOCK])
        tmp19 = tl.where(rmask & xmask, tmp17, 0)
        tmp20 = triton_helpers.promote_to_tensor(tl.sum(tmp19, 0))
        tl.store(out_ptr0 + (x0), tmp20, xmask)


buf155: SchedulerNode(ComputedBuffer)
buf155.writes = [MemoryDep('buf155', c0, {c0: 151296}, None)]
buf155.unmet_dependencies = 
    [   MemoryDep('buf144', c0, {c0: 151296}, None),
        MemoryDep('buf151', c0, {c0: 151296}, None),
        MemoryDep('buf152', c0, {c0: 197}, None),
        MemoryDep('buf153', c0, {c0: 197}, None)]
buf155.met_dependencies = 
    [   MemoryDep('arg115_1', c1, {c0: 197, c1: 768}, None),
        MemoryDep('arg116_1', c1, {c0: 197, c1: 768}, None),
        MemoryDep('arg117_1', c1, {c0: 197, c1: 768}, None)]
buf155.users = [NodeUser(node=ExternKernelSchedulerNode(name='buf156'), can_inplace=False, is_weak=False), NodeUser(node=ExternKernelSchedulerNode(name='buf157'), can_inplace=False, is_weak=False), NodeUser(node=ExternKernelSchedulerNode(name='buf158'), can_inplace=False, is_weak=False)]
buf155.group.device = cuda:0
buf155.group.iteration = (151296, 1)
buf155.sizes = ([197, 768], [])
arg116_1_layout = FixedLayout('cuda', torch.float32, size=[768], stride=[1])
buf152_layout = FixedLayout('cuda', torch.float32, size=[1, 197, 1], stride=[197, 1, 197])
arg115_1_layout = FixedLayout('cuda', torch.float32, size=[768], stride=[1])
arg117_1_layout = FixedLayout('cuda', torch.float32, size=[768], stride=[1])
buf151_layout = FixedLayout('cuda', torch.float32, size=[197, 768], stride=[768, 1])
buf153_layout = FixedLayout('cuda', torch.float32, size=[1, 197, 1], stride=[197, 1, 197])
buf144_layout = FixedLayout('cuda', torch.float32, size=[1, 197, 768], stride=[151296, 768, 1])
buf155_layout = FixedLayout('cuda', torch.float32, size=[1, 197, 768], stride=[151296, 768, 1])
class buf155_loop_body:
    var_ranges = {z0: 197, z1: 768}
    index0 = 768*z0 + z1
    index1 = z1
    index2 = z0
    def body(self, ops):
        get_index = self.get_index('index0')
        load = ops.load('buf151', get_index)
        get_index_1 = self.get_index('index1')
        load_1 = ops.load('arg115_1', get_index_1)
        add = ops.add(load, load_1)
        get_index_2 = self.get_index('index0')
        load_2 = ops.load('buf144', get_index_2)
        add_1 = ops.add(add, load_2)
        get_index_3 = self.get_index('index2')
        load_3 = ops.load('buf152', get_index_3)
        sub = ops.sub(add_1, load_3)
        get_index_4 = self.get_index('index2')
        load_4 = ops.load('buf153', get_index_4)
        constant = ops.constant(768.0, torch.float32)
        truediv = ops.truediv(load_4, constant)
        constant_1 = ops.constant(1e-12, torch.float32)
        add_2 = ops.add(truediv, constant_1)
        rsqrt = ops.rsqrt(add_2)
        mul = ops.mul(sub, rsqrt)
        get_index_5 = self.get_index('index1')
        load_5 = ops.load('arg116_1', get_index_5)
        mul_1 = ops.mul(mul, load_5)
        get_index_6 = self.get_index('index1')
        load_6 = ops.load('arg117_1', get_index_6)
        add_3 = ops.add(mul_1, load_6)
        get_index_7 = self.get_index('index0')
        store = ops.store('buf155', get_index_7, add_3, None)
        return store
buf155 Triton code:
    import triton
    import triton.language as tl
    from triton.compiler.compiler import AttrsDescriptor

    from torch._inductor.runtime import triton_helpers, triton_heuristics
    from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
    from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, instance_descriptor, DeviceProperties

    @triton_heuristics.pointwise(
        size_hints=[262144], 
        filename=__file__,
        triton_meta={'signature': {0: '*fp32', 1: '*fp32', 2: '*fp32', 3: '*fp32', 4: '*fp32', 5: '*fp32', 6: '*fp32', 7: '*fp32', 8: 'i32'}, 'device': DeviceProperties(type='cuda', index=0, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, multi_processor_count=132), 'constants': {}, 'configs': [AttrsDescriptor(divisible_by_16=(0, 1, 2, 3, 4, 5, 6, 7, 8), equal_to_1=())]},
        inductor_meta={'autotune_hints': set(), 'kernel_name': 'Placeholder.DESCRIPTIVE_NAME', 'mutated_arg_names': [], 'no_x_dim': False, 'num_load': 7, 'num_reduction': 0, 'backend_hash': '3944FAAD9D18E987535CF029C4441E186640DD453AE5AE6831EDF736C09E0A47', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': False, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False},
        min_elem_per_thread=0
    )
    @triton.jit
    def triton_(in_ptr0, in_ptr1, in_ptr2, in_ptr3, in_ptr4, in_ptr5, in_ptr6, out_ptr0, xnumel, XBLOCK : tl.constexpr):
        xnumel = 151296
        xoffset = tl.program_id(0) * XBLOCK
        xindex = xoffset + tl.arange(0, XBLOCK)[:]
        xmask = xindex < xnumel
        x2 = xindex
        x0 = xindex % 768
        x1 = (xindex // 768)
        tmp0 = tl.load(in_ptr0 + (x2), xmask)
        tmp1 = tl.load(in_ptr1 + (x0), xmask, eviction_policy='evict_last')
        tmp3 = tl.load(in_ptr2 + (x2), xmask)
        tmp5 = tl.load(in_ptr3 + (x1), xmask, eviction_policy='evict_last')
        tmp7 = tl.load(in_ptr4 + (x1), xmask, eviction_policy='evict_last')
        tmp14 = tl.load(in_ptr5 + (x0), xmask, eviction_policy='evict_last')
        tmp16 = tl.load(in_ptr6 + (x0), xmask, eviction_policy='evict_last')
        tmp2 = tmp0 + tmp1
        tmp4 = tmp2 + tmp3
        tmp6 = tmp4 - tmp5
        tmp8 = 768.0
        tmp9 = tmp7 / tmp8
        tmp10 = 1e-12
        tmp11 = tmp9 + tmp10
        tmp12 = libdevice.rsqrt(tmp11)
        tmp13 = tmp6 * tmp12
        tmp15 = tmp13 * tmp14
        tmp17 = tmp15 + tmp16
        tl.store(out_ptr0 + (x2), tmp17, xmask)


buf156: ExternKernelSchedulerNode(ExternKernelOut)
buf156.writes = [StarDep(name='buf156', mode=None)]
buf156.unmet_dependencies = [StarDep(name='buf155', mode=None)]
buf156.met_dependencies = [StarDep(name='arg118_1', mode=None), StarDep(name='arg119_1', mode=None)]
buf156.users = [NodeUser(node=ExternKernelSchedulerNode(name='buf159'), can_inplace=False, is_weak=False)]
buf156.node.kernel = extern_kernels.addmm


buf157: ExternKernelSchedulerNode(ExternKernelOut)
buf157.writes = [StarDep(name='buf157', mode=None)]
buf157.unmet_dependencies = [StarDep(name='buf155', mode=None)]
buf157.met_dependencies = [StarDep(name='arg120_1', mode=None), StarDep(name='arg121_1', mode=None)]
buf157.users = [NodeUser(node=ExternKernelSchedulerNode(name='buf159'), can_inplace=False, is_weak=False)]
buf157.node.kernel = extern_kernels.addmm


buf158: ExternKernelSchedulerNode(ExternKernelOut)
buf158.writes = [StarDep(name='buf158', mode=None)]
buf158.unmet_dependencies = [StarDep(name='buf155', mode=None)]
buf158.met_dependencies = [StarDep(name='arg122_1', mode=None), StarDep(name='arg123_1', mode=None)]
buf158.users = [NodeUser(node=ExternKernelSchedulerNode(name='buf159'), can_inplace=False, is_weak=False)]
buf158.node.kernel = extern_kernels.addmm


buf159: ExternKernelSchedulerNode(FallbackKernel)
buf159.writes = [StarDep(name='buf159', mode=None)]
buf159.unmet_dependencies = 
    [   StarDep(name='buf156', mode=None),
        StarDep(name='buf157', mode=None),
        StarDep(name='buf158', mode=None)]
buf159.met_dependencies = []
buf159.users = [NodeUser(node=ExternKernelSchedulerNode(name='buf160'), can_inplace=False, is_weak=False), NodeUser(node=ExternKernelSchedulerNode(name='buf161'), can_inplace=False, is_weak=False), NodeUser(node=ExternKernelSchedulerNode(name='buf162'), can_inplace=False, is_weak=False), NodeUser(node=ExternKernelSchedulerNode(name='buf163'), can_inplace=False, is_weak=False)]
buf159.node.kernel = None


buf160: ExternKernelSchedulerNode(MultiOutput)
buf160.writes = [StarDep(name='buf160', mode=None)]
buf160.unmet_dependencies = [StarDep(name='buf159', mode=None)]
buf160.met_dependencies = []
buf160.users = [NodeUser(node=ExternKernelSchedulerNode(name='buf164'), can_inplace=False, is_weak=False)]
buf160.node.kernel = None


buf164: ExternKernelSchedulerNode(ExternKernelOut)
buf164.writes = [StarDep(name='buf164', mode=None)]
buf164.unmet_dependencies = [StarDep(name='buf160', mode=None)]
buf164.met_dependencies = [StarDep(name='arg124_1', mode=None)]
buf164.users = [NodeUser(node=SchedulerNode(name='buf165'), can_inplace=True, is_weak=False)]
buf164.node.kernel = extern_kernels.mm


buf165: SchedulerNode(ComputedBuffer)
buf165.writes = [MemoryDep('buf165', c0, {c0: 151296}, None)]
buf165.unmet_dependencies = 
    [   MemoryDep('buf144', c0, {c0: 151296}, None),
        MemoryDep('buf151', c0, {c0: 151296}, None),
        MemoryDep('buf164', c0, {c0: 151296}, None)]
buf165.met_dependencies = 
    [   MemoryDep('arg115_1', c1, {c0: 197, c1: 768}, None),
        MemoryDep('arg125_1', c1, {c0: 197, c1: 768}, None)]
buf165.users = [NodeUser(node=SchedulerNode(name='buf166'), can_inplace=False, is_weak=False), NodeUser(node=SchedulerNode(name='buf167'), can_inplace=False, is_weak=False), NodeUser(node=SchedulerNode(name='buf168'), can_inplace=False, is_weak=False), NodeUser(node=SchedulerNode(name='buf169'), can_inplace=True, is_weak=False), NodeUser(node=SchedulerNode(name='buf173'), can_inplace=False, is_weak=False), NodeUser(node=SchedulerNode(name='buf174'), can_inplace=False, is_weak=False), NodeUser(node=SchedulerNode(name='buf175'), can_inplace=False, is_weak=False), NodeUser(node=SchedulerNode(name='buf176'), can_inplace=True, is_weak=False), NodeUser(node=SchedulerNode(name='buf186'), can_inplace=True, is_weak=False)]
buf165.group.device = cuda:0
buf165.group.iteration = (151296, 1)
buf165.sizes = ([197, 768], [])
arg115_1_layout = FixedLayout('cuda', torch.float32, size=[768], stride=[1])
arg125_1_layout = FixedLayout('cuda', torch.float32, size=[768], stride=[1])
buf151_layout = FixedLayout('cuda', torch.float32, size=[197, 768], stride=[768, 1])
buf164_layout = FixedLayout('cuda', torch.float32, size=[197, 768], stride=[768, 1])
buf144_layout = FixedLayout('cuda', torch.float32, size=[1, 197, 768], stride=[151296, 768, 1])
buf165_layout = FixedLayout('cuda', torch.float32, size=[1, 197, 768], stride=[151296, 768, 1])
class buf165_loop_body:
    var_ranges = {z0: 197, z1: 768}
    index0 = 768*z0 + z1
    index1 = z1
    def body(self, ops):
        get_index = self.get_index('index0')
        load = ops.load('buf164', get_index)
        get_index_1 = self.get_index('index1')
        load_1 = ops.load('arg125_1', get_index_1)
        add = ops.add(load, load_1)
        get_index_2 = self.get_index('index0')
        load_2 = ops.load('buf151', get_index_2)
        get_index_3 = self.get_index('index1')
        load_3 = ops.load('arg115_1', get_index_3)
        add_1 = ops.add(load_2, load_3)
        get_index_4 = self.get_index('index0')
        load_4 = ops.load('buf144', get_index_4)
        add_2 = ops.add(add_1, load_4)
        add_3 = ops.add(add, add_2)
        get_index_5 = self.get_index('index0')
        store = ops.store('buf165', get_index_5, add_3, None)
        return store
buf165 Triton code:
    import triton
    import triton.language as tl
    from triton.compiler.compiler import AttrsDescriptor

    from torch._inductor.runtime import triton_helpers, triton_heuristics
    from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
    from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, instance_descriptor, DeviceProperties

    @triton_heuristics.pointwise(
        size_hints=[262144], 
        filename=__file__,
        triton_meta={'signature': {0: '*fp32', 1: '*fp32', 2: '*fp32', 3: '*fp32', 4: '*fp32', 5: 'i32'}, 'device': DeviceProperties(type='cuda', index=0, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, multi_processor_count=132), 'constants': {}, 'configs': [AttrsDescriptor(divisible_by_16=(0, 1, 2, 3, 4, 5), equal_to_1=())]},
        inductor_meta={'autotune_hints': set(), 'kernel_name': 'Placeholder.DESCRIPTIVE_NAME', 'mutated_arg_names': ['in_out_ptr0'], 'no_x_dim': False, 'num_load': 5, 'num_reduction': 0, 'backend_hash': '3944FAAD9D18E987535CF029C4441E186640DD453AE5AE6831EDF736C09E0A47', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': False, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False},
        min_elem_per_thread=0
    )
    @triton.jit
    def triton_(in_out_ptr0, in_ptr0, in_ptr1, in_ptr2, in_ptr3, xnumel, XBLOCK : tl.constexpr):
        xnumel = 151296
        xoffset = tl.program_id(0) * XBLOCK
        xindex = xoffset + tl.arange(0, XBLOCK)[:]
        xmask = xindex < xnumel
        x2 = xindex
        x0 = xindex % 768
        tmp0 = tl.load(in_out_ptr0 + (x2), xmask)
        tmp1 = tl.load(in_ptr0 + (x0), xmask, eviction_policy='evict_last')
        tmp3 = tl.load(in_ptr1 + (x2), xmask)
        tmp4 = tl.load(in_ptr2 + (x0), xmask, eviction_policy='evict_last')
        tmp6 = tl.load(in_ptr3 + (x2), xmask)
        tmp2 = tmp0 + tmp1
        tmp5 = tmp3 + tmp4
        tmp7 = tmp5 + tmp6
        tmp8 = tmp2 + tmp7
        tl.store(in_out_ptr0 + (x2), tmp8, xmask)


buf166: SchedulerNode(ComputedBuffer)
buf166.writes = [MemoryDep('buf166', c0, {c0: 197}, None)]
buf166.unmet_dependencies = [MemoryDep('buf165', c0, {c0: 151296}, None)]
buf166.met_dependencies = []
buf166.users = [NodeUser(node=SchedulerNode(name='buf169'), can_inplace=False, is_weak=False)]
buf166.group.device = cuda:0
buf166.group.iteration = (197, 768)
buf166.sizes = ([197], [768])
buf165_layout = FixedLayout('cuda', torch.float32, size=[1, 197, 768], stride=[151296, 768, 1])
buf166_layout = FixedLayout('cuda', torch.float32, size=[1, 197, 1], stride=[197, 1, 197])
class buf166_loop_body:
    var_ranges = {z0: 197, z1: 768}
    index0 = 768*z0 + z1
    index1 = z0
    def body(self, ops):
        get_index = self.get_index('index0')
        load = ops.load('buf165', get_index)
        reduction = ops.reduction(torch.float32, torch.float32, 'welford_reduce', load)
        getitem = reduction[0]
        getitem_1 = reduction[1]
        getitem_2 = reduction[2]
        get_index_1 = self.get_index('index1')
        store_reduction = ops.store_reduction('buf166', get_index_1, getitem)
        return store_reduction
buf166 Triton code:
    import triton
    import triton.language as tl
    from triton.compiler.compiler import AttrsDescriptor

    from torch._inductor.runtime import triton_helpers, triton_heuristics
    from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
    from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, instance_descriptor, DeviceProperties

    @triton_heuristics.persistent_reduction(
        size_hints=[256, 1024],
        reduction_hint=ReductionHint.INNER,
        filename=__file__,
        triton_meta={'signature': {0: '*fp32', 1: '*fp32', 2: 'i32', 3: 'i32'}, 'device': DeviceProperties(type='cuda', index=0, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, multi_processor_count=132), 'constants': {}, 'configs': [AttrsDescriptor(divisible_by_16=(0, 1, 3), equal_to_1=())]},
        inductor_meta={'autotune_hints': set(), 'kernel_name': 'Placeholder.DESCRIPTIVE_NAME', 'mutated_arg_names': [], 'no_x_dim': True, 'num_load': 1, 'num_reduction': 3, 'backend_hash': '3944FAAD9D18E987535CF029C4441E186640DD453AE5AE6831EDF736C09E0A47', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': False, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False}
    )
    @triton.jit
    def triton_(in_ptr0, out_ptr0, xnumel, rnumel):
        xnumel = 197
        XBLOCK: tl.constexpr = 1
        rnumel = 768
        RBLOCK: tl.constexpr = 1024
        xoffset = tl.program_id(0) * XBLOCK
        xindex = tl.full([1], xoffset, tl.int32)
        xmask = xindex < xnumel
        rindex = tl.arange(0, RBLOCK)[:]
        roffset = 0
        rmask = rindex < rnumel
        r1 = rindex
        x0 = xindex
        tmp0 = tl.load(in_ptr0 + (r1 + (768*x0)), rmask & xmask, other=0.0)
        tmp1 = tl.broadcast_to(tmp0, [RBLOCK])
        tmp3 = tl.where(rmask & xmask, tmp1, 0)
        tmp4 = tl.broadcast_to(tmp1, [RBLOCK])
        tmp6 = tl.where(rmask & xmask, tmp4, 0)
        tmp7 = triton_helpers.promote_to_tensor(tl.sum(tmp6, 0))
        tmp8 = tl.full([1], 768, tl.int32)
        tmp9 = tmp8.to(tl.float32)
        tmp10 = tmp7 / tmp9
        tmp11 = tmp1 - tmp10
        tmp12 = tmp11 * tmp11
        tmp13 = tl.broadcast_to(tmp12, [RBLOCK])
        tmp15 = tl.where(rmask & xmask, tmp13, 0)
        tmp16 = triton_helpers.promote_to_tensor(tl.sum(tmp15, 0))
        tl.store(out_ptr0 + (x0), tmp10, xmask)


buf167: SchedulerNode(ComputedBuffer)
buf167.writes = [MemoryDep('buf167', c0, {c0: 197}, None)]
buf167.unmet_dependencies = [MemoryDep('buf165', c0, {c0: 151296}, None)]
buf167.met_dependencies = []
buf167.users = [NodeUser(node=SchedulerNode(name='buf169'), can_inplace=False, is_weak=False)]
buf167.group.device = cuda:0
buf167.group.iteration = (197, 768)
buf167.sizes = ([197], [768])
buf165_layout = FixedLayout('cuda', torch.float32, size=[1, 197, 768], stride=[151296, 768, 1])
buf167_layout = FixedLayout('cuda', torch.float32, size=[1, 197, 1], stride=[197, 1, 197])
class buf167_loop_body:
    var_ranges = {z0: 197, z1: 768}
    index0 = 768*z0 + z1
    index1 = z0
    def body(self, ops):
        get_index = self.get_index('index0')
        load = ops.load('buf165', get_index)
        reduction = ops.reduction(torch.float32, torch.float32, 'welford_reduce', load)
        getitem = reduction[0]
        getitem_1 = reduction[1]
        getitem_2 = reduction[2]
        get_index_1 = self.get_index('index1')
        store_reduction = ops.store_reduction('buf167', get_index_1, getitem_1)
        return store_reduction
buf167 Triton code:
    import triton
    import triton.language as tl
    from triton.compiler.compiler import AttrsDescriptor

    from torch._inductor.runtime import triton_helpers, triton_heuristics
    from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
    from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, instance_descriptor, DeviceProperties

    @triton_heuristics.persistent_reduction(
        size_hints=[256, 1024],
        reduction_hint=ReductionHint.INNER,
        filename=__file__,
        triton_meta={'signature': {0: '*fp32', 1: '*fp32', 2: 'i32', 3: 'i32'}, 'device': DeviceProperties(type='cuda', index=0, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, multi_processor_count=132), 'constants': {}, 'configs': [AttrsDescriptor(divisible_by_16=(0, 1, 3), equal_to_1=())]},
        inductor_meta={'autotune_hints': set(), 'kernel_name': 'Placeholder.DESCRIPTIVE_NAME', 'mutated_arg_names': [], 'no_x_dim': True, 'num_load': 1, 'num_reduction': 3, 'backend_hash': '3944FAAD9D18E987535CF029C4441E186640DD453AE5AE6831EDF736C09E0A47', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': False, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False}
    )
    @triton.jit
    def triton_(in_ptr0, out_ptr0, xnumel, rnumel):
        xnumel = 197
        XBLOCK: tl.constexpr = 1
        rnumel = 768
        RBLOCK: tl.constexpr = 1024
        xoffset = tl.program_id(0) * XBLOCK
        xindex = tl.full([1], xoffset, tl.int32)
        xmask = xindex < xnumel
        rindex = tl.arange(0, RBLOCK)[:]
        roffset = 0
        rmask = rindex < rnumel
        r1 = rindex
        x0 = xindex
        tmp0 = tl.load(in_ptr0 + (r1 + (768*x0)), rmask & xmask, other=0.0)
        tmp1 = tl.broadcast_to(tmp0, [RBLOCK])
        tmp3 = tl.where(rmask & xmask, tmp1, 0)
        tmp4 = tl.broadcast_to(tmp1, [RBLOCK])
        tmp6 = tl.where(rmask & xmask, tmp4, 0)
        tmp7 = triton_helpers.promote_to_tensor(tl.sum(tmp6, 0))
        tmp8 = tl.full([1], 768, tl.int32)
        tmp9 = tmp8.to(tl.float32)
        tmp10 = tmp7 / tmp9
        tmp11 = tmp1 - tmp10
        tmp12 = tmp11 * tmp11
        tmp13 = tl.broadcast_to(tmp12, [RBLOCK])
        tmp15 = tl.where(rmask & xmask, tmp13, 0)
        tmp16 = triton_helpers.promote_to_tensor(tl.sum(tmp15, 0))
        tl.store(out_ptr0 + (x0), tmp16, xmask)


buf169: SchedulerNode(ComputedBuffer)
buf169.writes = [MemoryDep('buf169', c0, {c0: 151296}, None)]
buf169.unmet_dependencies = 
    [   MemoryDep('buf165', c0, {c0: 151296}, None),
        MemoryDep('buf166', c0, {c0: 197}, None),
        MemoryDep('buf167', c0, {c0: 197}, None)]
buf169.met_dependencies = 
    [   MemoryDep('arg126_1', c1, {c0: 197, c1: 768}, None),
        MemoryDep('arg127_1', c1, {c0: 197, c1: 768}, None)]
buf169.users = [NodeUser(node=ExternKernelSchedulerNode(name='buf170'), can_inplace=False, is_weak=False)]
buf169.group.device = cuda:0
buf169.group.iteration = (151296, 1)
buf169.sizes = ([197, 768], [])
arg127_1_layout = FixedLayout('cuda', torch.float32, size=[768], stride=[1])
arg126_1_layout = FixedLayout('cuda', torch.float32, size=[768], stride=[1])
buf166_layout = FixedLayout('cuda', torch.float32, size=[1, 197, 1], stride=[197, 1, 197])
buf165_layout = FixedLayout('cuda', torch.float32, size=[1, 197, 768], stride=[151296, 768, 1])
buf167_layout = FixedLayout('cuda', torch.float32, size=[1, 197, 1], stride=[197, 1, 197])
buf169_layout = FixedLayout('cuda', torch.float32, size=[1, 197, 768], stride=[151296, 768, 1])
class buf169_loop_body:
    var_ranges = {z0: 197, z1: 768}
    index0 = 768*z0 + z1
    index1 = z0
    index2 = z1
    def body(self, ops):
        get_index = self.get_index('index0')
        load = ops.load('buf165', get_index)
        get_index_1 = self.get_index('index1')
        load_1 = ops.load('buf166', get_index_1)
        sub = ops.sub(load, load_1)
        get_index_2 = self.get_index('index1')
        load_2 = ops.load('buf167', get_index_2)
        constant = ops.constant(768.0, torch.float32)
        truediv = ops.truediv(load_2, constant)
        constant_1 = ops.constant(1e-12, torch.float32)
        add = ops.add(truediv, constant_1)
        rsqrt = ops.rsqrt(add)
        mul = ops.mul(sub, rsqrt)
        get_index_3 = self.get_index('index2')
        load_3 = ops.load('arg126_1', get_index_3)
        mul_1 = ops.mul(mul, load_3)
        get_index_4 = self.get_index('index2')
        load_4 = ops.load('arg127_1', get_index_4)
        add_1 = ops.add(mul_1, load_4)
        get_index_5 = self.get_index('index0')
        store = ops.store('buf169', get_index_5, add_1, None)
        return store
buf169 Triton code:
    import triton
    import triton.language as tl
    from triton.compiler.compiler import AttrsDescriptor

    from torch._inductor.runtime import triton_helpers, triton_heuristics
    from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
    from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, instance_descriptor, DeviceProperties

    @triton_heuristics.pointwise(
        size_hints=[262144], 
        filename=__file__,
        triton_meta={'signature': {0: '*fp32', 1: '*fp32', 2: '*fp32', 3: '*fp32', 4: '*fp32', 5: '*fp32', 6: 'i32'}, 'device': DeviceProperties(type='cuda', index=0, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, multi_processor_count=132), 'constants': {}, 'configs': [AttrsDescriptor(divisible_by_16=(0, 1, 2, 3, 4, 5, 6), equal_to_1=())]},
        inductor_meta={'autotune_hints': set(), 'kernel_name': 'Placeholder.DESCRIPTIVE_NAME', 'mutated_arg_names': [], 'no_x_dim': False, 'num_load': 5, 'num_reduction': 0, 'backend_hash': '3944FAAD9D18E987535CF029C4441E186640DD453AE5AE6831EDF736C09E0A47', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': False, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False},
        min_elem_per_thread=0
    )
    @triton.jit
    def triton_(in_ptr0, in_ptr1, in_ptr2, in_ptr3, in_ptr4, out_ptr0, xnumel, XBLOCK : tl.constexpr):
        xnumel = 151296
        xoffset = tl.program_id(0) * XBLOCK
        xindex = xoffset + tl.arange(0, XBLOCK)[:]
        xmask = xindex < xnumel
        x2 = xindex
        x1 = (xindex // 768)
        x0 = xindex % 768
        tmp0 = tl.load(in_ptr0 + (x2), xmask)
        tmp1 = tl.load(in_ptr1 + (x1), xmask, eviction_policy='evict_last')
        tmp3 = tl.load(in_ptr2 + (x1), xmask, eviction_policy='evict_last')
        tmp10 = tl.load(in_ptr3 + (x0), xmask, eviction_policy='evict_last')
        tmp12 = tl.load(in_ptr4 + (x0), xmask, eviction_policy='evict_last')
        tmp2 = tmp0 - tmp1
        tmp4 = 768.0
        tmp5 = tmp3 / tmp4
        tmp6 = 1e-12
        tmp7 = tmp5 + tmp6
        tmp8 = libdevice.rsqrt(tmp7)
        tmp9 = tmp2 * tmp8
        tmp11 = tmp9 * tmp10
        tmp13 = tmp11 + tmp12
        tl.store(out_ptr0 + (x2), tmp13, xmask)


buf170: ExternKernelSchedulerNode(ExternKernelOut)
buf170.writes = [StarDep(name='buf170', mode=None)]
buf170.unmet_dependencies = [StarDep(name='buf169', mode=None)]
buf170.met_dependencies = [StarDep(name='arg128_1', mode=None)]
buf170.users = [NodeUser(node=SchedulerNode(name='buf171'), can_inplace=True, is_weak=False)]
buf170.node.kernel = extern_kernels.mm


buf171: SchedulerNode(ComputedBuffer)
buf171.writes = [MemoryDep('buf171', c0, {c0: 605184}, None)]
buf171.unmet_dependencies = [MemoryDep('buf170', c0, {c0: 605184}, None)]
buf171.met_dependencies = [MemoryDep('arg129_1', c1, {c0: 197, c1: 3072}, None)]
buf171.users = [NodeUser(node=ExternKernelSchedulerNode(name='buf172'), can_inplace=False, is_weak=False)]
buf171.group.device = cuda:0
buf171.group.iteration = (605184, 1)
buf171.sizes = ([197, 3072], [])
arg129_1_layout = FixedLayout('cuda', torch.float32, size=[3072], stride=[1])
buf170_layout = FixedLayout('cuda', torch.float32, size=[197, 3072], stride=[3072, 1])
buf171_layout = FixedLayout('cuda', torch.float32, size=[1, 197, 3072], stride=[605184, 3072, 1])
class buf171_loop_body:
    var_ranges = {z0: 197, z1: 3072}
    index0 = 3072*z0 + z1
    index1 = z1
    def body(self, ops):
        get_index = self.get_index('index0')
        load = ops.load('buf170', get_index)
        get_index_1 = self.get_index('index1')
        load_1 = ops.load('arg129_1', get_index_1)
        add = ops.add(load, load_1)
        constant = ops.constant(0.5, torch.float32)
        mul = ops.mul(add, constant)
        get_index_2 = self.get_index('index0')
        load_2 = ops.load('buf170', get_index_2)
        get_index_3 = self.get_index('index1')
        load_3 = ops.load('arg129_1', get_index_3)
        add_1 = ops.add(load_2, load_3)
        constant_1 = ops.constant(0.7071067811865476, torch.float32)
        mul_1 = ops.mul(add_1, constant_1)
        erf = ops.erf(mul_1)
        constant_2 = ops.constant(1.0, torch.float32)
        add_2 = ops.add(erf, constant_2)
        mul_2 = ops.mul(mul, add_2)
        get_index_4 = self.get_index('index0')
        store = ops.store('buf171', get_index_4, mul_2, None)
        return store
buf171 Triton code:
    import triton
    import triton.language as tl
    from triton.compiler.compiler import AttrsDescriptor

    from torch._inductor.runtime import triton_helpers, triton_heuristics
    from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
    from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, instance_descriptor, DeviceProperties

    @triton_heuristics.pointwise(
        size_hints=[1048576], 
        filename=__file__,
        triton_meta={'signature': {0: '*fp32', 1: '*fp32', 2: 'i32'}, 'device': DeviceProperties(type='cuda', index=0, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, multi_processor_count=132), 'constants': {}, 'configs': [AttrsDescriptor(divisible_by_16=(0, 1, 2), equal_to_1=())]},
        inductor_meta={'autotune_hints': set(), 'kernel_name': 'Placeholder.DESCRIPTIVE_NAME', 'mutated_arg_names': ['in_out_ptr0'], 'no_x_dim': False, 'num_load': 2, 'num_reduction': 0, 'backend_hash': '3944FAAD9D18E987535CF029C4441E186640DD453AE5AE6831EDF736C09E0A47', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': False, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False},
        min_elem_per_thread=0
    )
    @triton.jit
    def triton_(in_out_ptr0, in_ptr0, xnumel, XBLOCK : tl.constexpr):
        xnumel = 605184
        xoffset = tl.program_id(0) * XBLOCK
        xindex = xoffset + tl.arange(0, XBLOCK)[:]
        xmask = xindex < xnumel
        x2 = xindex
        x0 = xindex % 3072
        tmp0 = tl.load(in_out_ptr0 + (x2), xmask)
        tmp1 = tl.load(in_ptr0 + (x0), xmask, eviction_policy='evict_last')
        tmp2 = tmp0 + tmp1
        tmp3 = 0.5
        tmp4 = tmp2 * tmp3
        tmp5 = 0.7071067811865476
        tmp6 = tmp2 * tmp5
        tmp7 = libdevice.erf(tmp6)
        tmp8 = 1.0
        tmp9 = tmp7 + tmp8
        tmp10 = tmp4 * tmp9
        tl.store(in_out_ptr0 + (x2), tmp10, xmask)


buf172: ExternKernelSchedulerNode(ExternKernelOut)
buf172.writes = [StarDep(name='buf172', mode=None)]
buf172.unmet_dependencies = [StarDep(name='buf171', mode=None)]
buf172.met_dependencies = [StarDep(name='arg130_1', mode=None)]
buf172.users = [NodeUser(node=SchedulerNode(name='buf173'), can_inplace=False, is_weak=False), NodeUser(node=SchedulerNode(name='buf174'), can_inplace=False, is_weak=False), NodeUser(node=SchedulerNode(name='buf175'), can_inplace=False, is_weak=False), NodeUser(node=SchedulerNode(name='buf176'), can_inplace=True, is_weak=False), NodeUser(node=SchedulerNode(name='buf186'), can_inplace=True, is_weak=False)]
buf172.node.kernel = extern_kernels.mm


buf173: SchedulerNode(ComputedBuffer)
buf173.writes = [MemoryDep('buf173', c0, {c0: 197}, None)]
buf173.unmet_dependencies = 
    [   MemoryDep('buf165', c0, {c0: 151296}, None),
        MemoryDep('buf172', c0, {c0: 151296}, None)]
buf173.met_dependencies = [MemoryDep('arg131_1', c1, {c0: 197, c1: 768}, None)]
buf173.users = [NodeUser(node=SchedulerNode(name='buf176'), can_inplace=False, is_weak=False)]
buf173.group.device = cuda:0
buf173.group.iteration = (197, 768)
buf173.sizes = ([197], [768])
buf165_layout = FixedLayout('cuda', torch.float32, size=[1, 197, 768], stride=[151296, 768, 1])
buf172_layout = FixedLayout('cuda', torch.float32, size=[197, 768], stride=[768, 1])
arg131_1_layout = FixedLayout('cuda', torch.float32, size=[768], stride=[1])
buf173_layout = FixedLayout('cuda', torch.float32, size=[1, 197, 1], stride=[197, 1, 197])
class buf173_loop_body:
    var_ranges = {z0: 197, z1: 768}
    index0 = 768*z0 + z1
    index1 = z1
    index2 = z0
    def body(self, ops):
        get_index = self.get_index('index0')
        load = ops.load('buf172', get_index)
        get_index_1 = self.get_index('index1')
        load_1 = ops.load('arg131_1', get_index_1)
        add = ops.add(load, load_1)
        get_index_2 = self.get_index('index0')
        load_2 = ops.load('buf165', get_index_2)
        add_1 = ops.add(add, load_2)
        reduction = ops.reduction(torch.float32, torch.float32, 'welford_reduce', add_1)
        getitem = reduction[0]
        getitem_1 = reduction[1]
        getitem_2 = reduction[2]
        get_index_3 = self.get_index('index2')
        store_reduction = ops.store_reduction('buf173', get_index_3, getitem)
        return store_reduction
buf173 Triton code:
    import triton
    import triton.language as tl
    from triton.compiler.compiler import AttrsDescriptor

    from torch._inductor.runtime import triton_helpers, triton_heuristics
    from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
    from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, instance_descriptor, DeviceProperties

    @triton_heuristics.persistent_reduction(
        size_hints=[256, 1024],
        reduction_hint=ReductionHint.INNER,
        filename=__file__,
        triton_meta={'signature': {0: '*fp32', 1: '*fp32', 2: '*fp32', 3: '*fp32', 4: 'i32', 5: 'i32'}, 'device': DeviceProperties(type='cuda', index=0, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, multi_processor_count=132), 'constants': {}, 'configs': [AttrsDescriptor(divisible_by_16=(0, 1, 2, 3, 5), equal_to_1=())]},
        inductor_meta={'autotune_hints': set(), 'kernel_name': 'Placeholder.DESCRIPTIVE_NAME', 'mutated_arg_names': [], 'no_x_dim': True, 'num_load': 3, 'num_reduction': 3, 'backend_hash': '3944FAAD9D18E987535CF029C4441E186640DD453AE5AE6831EDF736C09E0A47', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': False, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False}
    )
    @triton.jit
    def triton_(in_ptr0, in_ptr1, in_ptr2, out_ptr0, xnumel, rnumel):
        xnumel = 197
        XBLOCK: tl.constexpr = 1
        rnumel = 768
        RBLOCK: tl.constexpr = 1024
        xoffset = tl.program_id(0) * XBLOCK
        xindex = tl.full([1], xoffset, tl.int32)
        xmask = xindex < xnumel
        rindex = tl.arange(0, RBLOCK)[:]
        roffset = 0
        rmask = rindex < rnumel
        r1 = rindex
        x0 = xindex
        tmp0 = tl.load(in_ptr0 + (r1 + (768*x0)), rmask & xmask, other=0.0)
        tmp1 = tl.load(in_ptr1 + (r1), rmask, eviction_policy='evict_last', other=0.0)
        tmp3 = tl.load(in_ptr2 + (r1 + (768*x0)), rmask & xmask, other=0.0)
        tmp2 = tmp0 + tmp1
        tmp4 = tmp2 + tmp3
        tmp5 = tl.broadcast_to(tmp4, [RBLOCK])
        tmp7 = tl.where(rmask & xmask, tmp5, 0)
        tmp8 = tl.broadcast_to(tmp5, [RBLOCK])
        tmp10 = tl.where(rmask & xmask, tmp8, 0)
        tmp11 = triton_helpers.promote_to_tensor(tl.sum(tmp10, 0))
        tmp12 = tl.full([1], 768, tl.int32)
        tmp13 = tmp12.to(tl.float32)
        tmp14 = tmp11 / tmp13
        tmp15 = tmp5 - tmp14
        tmp16 = tmp15 * tmp15
        tmp17 = tl.broadcast_to(tmp16, [RBLOCK])
        tmp19 = tl.where(rmask & xmask, tmp17, 0)
        tmp20 = triton_helpers.promote_to_tensor(tl.sum(tmp19, 0))
        tl.store(out_ptr0 + (x0), tmp14, xmask)


buf174: SchedulerNode(ComputedBuffer)
buf174.writes = [MemoryDep('buf174', c0, {c0: 197}, None)]
buf174.unmet_dependencies = 
    [   MemoryDep('buf165', c0, {c0: 151296}, None),
        MemoryDep('buf172', c0, {c0: 151296}, None)]
buf174.met_dependencies = [MemoryDep('arg131_1', c1, {c0: 197, c1: 768}, None)]
buf174.users = [NodeUser(node=SchedulerNode(name='buf176'), can_inplace=False, is_weak=False)]
buf174.group.device = cuda:0
buf174.group.iteration = (197, 768)
buf174.sizes = ([197], [768])
buf165_layout = FixedLayout('cuda', torch.float32, size=[1, 197, 768], stride=[151296, 768, 1])
buf172_layout = FixedLayout('cuda', torch.float32, size=[197, 768], stride=[768, 1])
arg131_1_layout = FixedLayout('cuda', torch.float32, size=[768], stride=[1])
buf174_layout = FixedLayout('cuda', torch.float32, size=[1, 197, 1], stride=[197, 1, 197])
class buf174_loop_body:
    var_ranges = {z0: 197, z1: 768}
    index0 = 768*z0 + z1
    index1 = z1
    index2 = z0
    def body(self, ops):
        get_index = self.get_index('index0')
        load = ops.load('buf172', get_index)
        get_index_1 = self.get_index('index1')
        load_1 = ops.load('arg131_1', get_index_1)
        add = ops.add(load, load_1)
        get_index_2 = self.get_index('index0')
        load_2 = ops.load('buf165', get_index_2)
        add_1 = ops.add(add, load_2)
        reduction = ops.reduction(torch.float32, torch.float32, 'welford_reduce', add_1)
        getitem = reduction[0]
        getitem_1 = reduction[1]
        getitem_2 = reduction[2]
        get_index_3 = self.get_index('index2')
        store_reduction = ops.store_reduction('buf174', get_index_3, getitem_1)
        return store_reduction
buf174 Triton code:
    import triton
    import triton.language as tl
    from triton.compiler.compiler import AttrsDescriptor

    from torch._inductor.runtime import triton_helpers, triton_heuristics
    from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
    from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, instance_descriptor, DeviceProperties

    @triton_heuristics.persistent_reduction(
        size_hints=[256, 1024],
        reduction_hint=ReductionHint.INNER,
        filename=__file__,
        triton_meta={'signature': {0: '*fp32', 1: '*fp32', 2: '*fp32', 3: '*fp32', 4: 'i32', 5: 'i32'}, 'device': DeviceProperties(type='cuda', index=0, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, multi_processor_count=132), 'constants': {}, 'configs': [AttrsDescriptor(divisible_by_16=(0, 1, 2, 3, 5), equal_to_1=())]},
        inductor_meta={'autotune_hints': set(), 'kernel_name': 'Placeholder.DESCRIPTIVE_NAME', 'mutated_arg_names': [], 'no_x_dim': True, 'num_load': 3, 'num_reduction': 3, 'backend_hash': '3944FAAD9D18E987535CF029C4441E186640DD453AE5AE6831EDF736C09E0A47', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': False, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False}
    )
    @triton.jit
    def triton_(in_ptr0, in_ptr1, in_ptr2, out_ptr0, xnumel, rnumel):
        xnumel = 197
        XBLOCK: tl.constexpr = 1
        rnumel = 768
        RBLOCK: tl.constexpr = 1024
        xoffset = tl.program_id(0) * XBLOCK
        xindex = tl.full([1], xoffset, tl.int32)
        xmask = xindex < xnumel
        rindex = tl.arange(0, RBLOCK)[:]
        roffset = 0
        rmask = rindex < rnumel
        r1 = rindex
        x0 = xindex
        tmp0 = tl.load(in_ptr0 + (r1 + (768*x0)), rmask & xmask, other=0.0)
        tmp1 = tl.load(in_ptr1 + (r1), rmask, eviction_policy='evict_last', other=0.0)
        tmp3 = tl.load(in_ptr2 + (r1 + (768*x0)), rmask & xmask, other=0.0)
        tmp2 = tmp0 + tmp1
        tmp4 = tmp2 + tmp3
        tmp5 = tl.broadcast_to(tmp4, [RBLOCK])
        tmp7 = tl.where(rmask & xmask, tmp5, 0)
        tmp8 = tl.broadcast_to(tmp5, [RBLOCK])
        tmp10 = tl.where(rmask & xmask, tmp8, 0)
        tmp11 = triton_helpers.promote_to_tensor(tl.sum(tmp10, 0))
        tmp12 = tl.full([1], 768, tl.int32)
        tmp13 = tmp12.to(tl.float32)
        tmp14 = tmp11 / tmp13
        tmp15 = tmp5 - tmp14
        tmp16 = tmp15 * tmp15
        tmp17 = tl.broadcast_to(tmp16, [RBLOCK])
        tmp19 = tl.where(rmask & xmask, tmp17, 0)
        tmp20 = triton_helpers.promote_to_tensor(tl.sum(tmp19, 0))
        tl.store(out_ptr0 + (x0), tmp20, xmask)


buf176: SchedulerNode(ComputedBuffer)
buf176.writes = [MemoryDep('buf176', c0, {c0: 151296}, None)]
buf176.unmet_dependencies = 
    [   MemoryDep('buf165', c0, {c0: 151296}, None),
        MemoryDep('buf172', c0, {c0: 151296}, None),
        MemoryDep('buf173', c0, {c0: 197}, None),
        MemoryDep('buf174', c0, {c0: 197}, None)]
buf176.met_dependencies = 
    [   MemoryDep('arg131_1', c1, {c0: 197, c1: 768}, None),
        MemoryDep('arg132_1', c1, {c0: 197, c1: 768}, None),
        MemoryDep('arg133_1', c1, {c0: 197, c1: 768}, None)]
buf176.users = [NodeUser(node=ExternKernelSchedulerNode(name='buf177'), can_inplace=False, is_weak=False), NodeUser(node=ExternKernelSchedulerNode(name='buf178'), can_inplace=False, is_weak=False), NodeUser(node=ExternKernelSchedulerNode(name='buf179'), can_inplace=False, is_weak=False)]
buf176.group.device = cuda:0
buf176.group.iteration = (151296, 1)
buf176.sizes = ([197, 768], [])
buf165_layout = FixedLayout('cuda', torch.float32, size=[1, 197, 768], stride=[151296, 768, 1])
arg133_1_layout = FixedLayout('cuda', torch.float32, size=[768], stride=[1])
buf172_layout = FixedLayout('cuda', torch.float32, size=[197, 768], stride=[768, 1])
buf173_layout = FixedLayout('cuda', torch.float32, size=[1, 197, 1], stride=[197, 1, 197])
buf174_layout = FixedLayout('cuda', torch.float32, size=[1, 197, 1], stride=[197, 1, 197])
arg131_1_layout = FixedLayout('cuda', torch.float32, size=[768], stride=[1])
arg132_1_layout = FixedLayout('cuda', torch.float32, size=[768], stride=[1])
buf176_layout = FixedLayout('cuda', torch.float32, size=[1, 197, 768], stride=[151296, 768, 1])
class buf176_loop_body:
    var_ranges = {z0: 197, z1: 768}
    index0 = 768*z0 + z1
    index1 = z1
    index2 = z0
    def body(self, ops):
        get_index = self.get_index('index0')
        load = ops.load('buf172', get_index)
        get_index_1 = self.get_index('index1')
        load_1 = ops.load('arg131_1', get_index_1)
        add = ops.add(load, load_1)
        get_index_2 = self.get_index('index0')
        load_2 = ops.load('buf165', get_index_2)
        add_1 = ops.add(add, load_2)
        get_index_3 = self.get_index('index2')
        load_3 = ops.load('buf173', get_index_3)
        sub = ops.sub(add_1, load_3)
        get_index_4 = self.get_index('index2')
        load_4 = ops.load('buf174', get_index_4)
        constant = ops.constant(768.0, torch.float32)
        truediv = ops.truediv(load_4, constant)
        constant_1 = ops.constant(1e-12, torch.float32)
        add_2 = ops.add(truediv, constant_1)
        rsqrt = ops.rsqrt(add_2)
        mul = ops.mul(sub, rsqrt)
        get_index_5 = self.get_index('index1')
        load_5 = ops.load('arg132_1', get_index_5)
        mul_1 = ops.mul(mul, load_5)
        get_index_6 = self.get_index('index1')
        load_6 = ops.load('arg133_1', get_index_6)
        add_3 = ops.add(mul_1, load_6)
        get_index_7 = self.get_index('index0')
        store = ops.store('buf176', get_index_7, add_3, None)
        return store
buf176 Triton code:
    import triton
    import triton.language as tl
    from triton.compiler.compiler import AttrsDescriptor

    from torch._inductor.runtime import triton_helpers, triton_heuristics
    from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
    from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, instance_descriptor, DeviceProperties

    @triton_heuristics.pointwise(
        size_hints=[262144], 
        filename=__file__,
        triton_meta={'signature': {0: '*fp32', 1: '*fp32', 2: '*fp32', 3: '*fp32', 4: '*fp32', 5: '*fp32', 6: '*fp32', 7: '*fp32', 8: 'i32'}, 'device': DeviceProperties(type='cuda', index=0, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, multi_processor_count=132), 'constants': {}, 'configs': [AttrsDescriptor(divisible_by_16=(0, 1, 2, 3, 4, 5, 6, 7, 8), equal_to_1=())]},
        inductor_meta={'autotune_hints': set(), 'kernel_name': 'Placeholder.DESCRIPTIVE_NAME', 'mutated_arg_names': [], 'no_x_dim': False, 'num_load': 7, 'num_reduction': 0, 'backend_hash': '3944FAAD9D18E987535CF029C4441E186640DD453AE5AE6831EDF736C09E0A47', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': False, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False},
        min_elem_per_thread=0
    )
    @triton.jit
    def triton_(in_ptr0, in_ptr1, in_ptr2, in_ptr3, in_ptr4, in_ptr5, in_ptr6, out_ptr0, xnumel, XBLOCK : tl.constexpr):
        xnumel = 151296
        xoffset = tl.program_id(0) * XBLOCK
        xindex = xoffset + tl.arange(0, XBLOCK)[:]
        xmask = xindex < xnumel
        x2 = xindex
        x0 = xindex % 768
        x1 = (xindex // 768)
        tmp0 = tl.load(in_ptr0 + (x2), xmask)
        tmp1 = tl.load(in_ptr1 + (x0), xmask, eviction_policy='evict_last')
        tmp3 = tl.load(in_ptr2 + (x2), xmask)
        tmp5 = tl.load(in_ptr3 + (x1), xmask, eviction_policy='evict_last')
        tmp7 = tl.load(in_ptr4 + (x1), xmask, eviction_policy='evict_last')
        tmp14 = tl.load(in_ptr5 + (x0), xmask, eviction_policy='evict_last')
        tmp16 = tl.load(in_ptr6 + (x0), xmask, eviction_policy='evict_last')
        tmp2 = tmp0 + tmp1
        tmp4 = tmp2 + tmp3
        tmp6 = tmp4 - tmp5
        tmp8 = 768.0
        tmp9 = tmp7 / tmp8
        tmp10 = 1e-12
        tmp11 = tmp9 + tmp10
        tmp12 = libdevice.rsqrt(tmp11)
        tmp13 = tmp6 * tmp12
        tmp15 = tmp13 * tmp14
        tmp17 = tmp15 + tmp16
        tl.store(out_ptr0 + (x2), tmp17, xmask)


buf177: ExternKernelSchedulerNode(ExternKernelOut)
buf177.writes = [StarDep(name='buf177', mode=None)]
buf177.unmet_dependencies = [StarDep(name='buf176', mode=None)]
buf177.met_dependencies = [StarDep(name='arg134_1', mode=None), StarDep(name='arg135_1', mode=None)]
buf177.users = [NodeUser(node=ExternKernelSchedulerNode(name='buf180'), can_inplace=False, is_weak=False)]
buf177.node.kernel = extern_kernels.addmm


buf178: ExternKernelSchedulerNode(ExternKernelOut)
buf178.writes = [StarDep(name='buf178', mode=None)]
buf178.unmet_dependencies = [StarDep(name='buf176', mode=None)]
buf178.met_dependencies = [StarDep(name='arg136_1', mode=None), StarDep(name='arg137_1', mode=None)]
buf178.users = [NodeUser(node=ExternKernelSchedulerNode(name='buf180'), can_inplace=False, is_weak=False)]
buf178.node.kernel = extern_kernels.addmm


buf179: ExternKernelSchedulerNode(ExternKernelOut)
buf179.writes = [StarDep(name='buf179', mode=None)]
buf179.unmet_dependencies = [StarDep(name='buf176', mode=None)]
buf179.met_dependencies = [StarDep(name='arg138_1', mode=None), StarDep(name='arg139_1', mode=None)]
buf179.users = [NodeUser(node=ExternKernelSchedulerNode(name='buf180'), can_inplace=False, is_weak=False)]
buf179.node.kernel = extern_kernels.addmm


buf180: ExternKernelSchedulerNode(FallbackKernel)
buf180.writes = [StarDep(name='buf180', mode=None)]
buf180.unmet_dependencies = 
    [   StarDep(name='buf177', mode=None),
        StarDep(name='buf178', mode=None),
        StarDep(name='buf179', mode=None)]
buf180.met_dependencies = []
buf180.users = [NodeUser(node=ExternKernelSchedulerNode(name='buf181'), can_inplace=False, is_weak=False), NodeUser(node=ExternKernelSchedulerNode(name='buf182'), can_inplace=False, is_weak=False), NodeUser(node=ExternKernelSchedulerNode(name='buf183'), can_inplace=False, is_weak=False), NodeUser(node=ExternKernelSchedulerNode(name='buf184'), can_inplace=False, is_weak=False)]
buf180.node.kernel = None


buf181: ExternKernelSchedulerNode(MultiOutput)
buf181.writes = [StarDep(name='buf181', mode=None)]
buf181.unmet_dependencies = [StarDep(name='buf180', mode=None)]
buf181.met_dependencies = []
buf181.users = [NodeUser(node=ExternKernelSchedulerNode(name='buf185'), can_inplace=False, is_weak=False)]
buf181.node.kernel = None


buf185: ExternKernelSchedulerNode(ExternKernelOut)
buf185.writes = [StarDep(name='buf185', mode=None)]
buf185.unmet_dependencies = [StarDep(name='buf181', mode=None)]
buf185.met_dependencies = [StarDep(name='arg140_1', mode=None)]
buf185.users = [NodeUser(node=SchedulerNode(name='buf186'), can_inplace=True, is_weak=False)]
buf185.node.kernel = extern_kernels.mm


buf186: SchedulerNode(ComputedBuffer)
buf186.writes = [MemoryDep('buf186', c0, {c0: 151296}, None)]
buf186.unmet_dependencies = 
    [   MemoryDep('buf165', c0, {c0: 151296}, None),
        MemoryDep('buf172', c0, {c0: 151296}, None),
        MemoryDep('buf185', c0, {c0: 151296}, None)]
buf186.met_dependencies = 
    [   MemoryDep('arg131_1', c1, {c0: 197, c1: 768}, None),
        MemoryDep('arg141_1', c1, {c0: 197, c1: 768}, None)]
buf186.users = [NodeUser(node=SchedulerNode(name='buf187'), can_inplace=False, is_weak=False), NodeUser(node=SchedulerNode(name='buf188'), can_inplace=False, is_weak=False), NodeUser(node=SchedulerNode(name='buf189'), can_inplace=False, is_weak=False), NodeUser(node=SchedulerNode(name='buf190'), can_inplace=True, is_weak=False), NodeUser(node=SchedulerNode(name='buf194'), can_inplace=False, is_weak=False), NodeUser(node=SchedulerNode(name='buf195'), can_inplace=False, is_weak=False), NodeUser(node=SchedulerNode(name='buf196'), can_inplace=False, is_weak=False), NodeUser(node=SchedulerNode(name='buf197'), can_inplace=True, is_weak=False), NodeUser(node=SchedulerNode(name='buf207'), can_inplace=True, is_weak=False)]
buf186.group.device = cuda:0
buf186.group.iteration = (151296, 1)
buf186.sizes = ([197, 768], [])
arg141_1_layout = FixedLayout('cuda', torch.float32, size=[768], stride=[1])
arg131_1_layout = FixedLayout('cuda', torch.float32, size=[768], stride=[1])
buf172_layout = FixedLayout('cuda', torch.float32, size=[197, 768], stride=[768, 1])
buf185_layout = FixedLayout('cuda', torch.float32, size=[197, 768], stride=[768, 1])
buf165_layout = FixedLayout('cuda', torch.float32, size=[1, 197, 768], stride=[151296, 768, 1])
buf186_layout = FixedLayout('cuda', torch.float32, size=[1, 197, 768], stride=[151296, 768, 1])
class buf186_loop_body:
    var_ranges = {z0: 197, z1: 768}
    index0 = 768*z0 + z1
    index1 = z1
    def body(self, ops):
        get_index = self.get_index('index0')
        load = ops.load('buf185', get_index)
        get_index_1 = self.get_index('index1')
        load_1 = ops.load('arg141_1', get_index_1)
        add = ops.add(load, load_1)
        get_index_2 = self.get_index('index0')
        load_2 = ops.load('buf172', get_index_2)
        get_index_3 = self.get_index('index1')
        load_3 = ops.load('arg131_1', get_index_3)
        add_1 = ops.add(load_2, load_3)
        get_index_4 = self.get_index('index0')
        load_4 = ops.load('buf165', get_index_4)
        add_2 = ops.add(add_1, load_4)
        add_3 = ops.add(add, add_2)
        get_index_5 = self.get_index('index0')
        store = ops.store('buf186', get_index_5, add_3, None)
        return store
buf186 Triton code:
    import triton
    import triton.language as tl
    from triton.compiler.compiler import AttrsDescriptor

    from torch._inductor.runtime import triton_helpers, triton_heuristics
    from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
    from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, instance_descriptor, DeviceProperties

    @triton_heuristics.pointwise(
        size_hints=[262144], 
        filename=__file__,
        triton_meta={'signature': {0: '*fp32', 1: '*fp32', 2: '*fp32', 3: '*fp32', 4: '*fp32', 5: 'i32'}, 'device': DeviceProperties(type='cuda', index=0, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, multi_processor_count=132), 'constants': {}, 'configs': [AttrsDescriptor(divisible_by_16=(0, 1, 2, 3, 4, 5), equal_to_1=())]},
        inductor_meta={'autotune_hints': set(), 'kernel_name': 'Placeholder.DESCRIPTIVE_NAME', 'mutated_arg_names': ['in_out_ptr0'], 'no_x_dim': False, 'num_load': 5, 'num_reduction': 0, 'backend_hash': '3944FAAD9D18E987535CF029C4441E186640DD453AE5AE6831EDF736C09E0A47', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': False, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False},
        min_elem_per_thread=0
    )
    @triton.jit
    def triton_(in_out_ptr0, in_ptr0, in_ptr1, in_ptr2, in_ptr3, xnumel, XBLOCK : tl.constexpr):
        xnumel = 151296
        xoffset = tl.program_id(0) * XBLOCK
        xindex = xoffset + tl.arange(0, XBLOCK)[:]
        xmask = xindex < xnumel
        x2 = xindex
        x0 = xindex % 768
        tmp0 = tl.load(in_out_ptr0 + (x2), xmask)
        tmp1 = tl.load(in_ptr0 + (x0), xmask, eviction_policy='evict_last')
        tmp3 = tl.load(in_ptr1 + (x2), xmask)
        tmp4 = tl.load(in_ptr2 + (x0), xmask, eviction_policy='evict_last')
        tmp6 = tl.load(in_ptr3 + (x2), xmask)
        tmp2 = tmp0 + tmp1
        tmp5 = tmp3 + tmp4
        tmp7 = tmp5 + tmp6
        tmp8 = tmp2 + tmp7
        tl.store(in_out_ptr0 + (x2), tmp8, xmask)


buf187: SchedulerNode(ComputedBuffer)
buf187.writes = [MemoryDep('buf187', c0, {c0: 197}, None)]
buf187.unmet_dependencies = [MemoryDep('buf186', c0, {c0: 151296}, None)]
buf187.met_dependencies = []
buf187.users = [NodeUser(node=SchedulerNode(name='buf190'), can_inplace=False, is_weak=False)]
buf187.group.device = cuda:0
buf187.group.iteration = (197, 768)
buf187.sizes = ([197], [768])
buf186_layout = FixedLayout('cuda', torch.float32, size=[1, 197, 768], stride=[151296, 768, 1])
buf187_layout = FixedLayout('cuda', torch.float32, size=[1, 197, 1], stride=[197, 1, 197])
class buf187_loop_body:
    var_ranges = {z0: 197, z1: 768}
    index0 = 768*z0 + z1
    index1 = z0
    def body(self, ops):
        get_index = self.get_index('index0')
        load = ops.load('buf186', get_index)
        reduction = ops.reduction(torch.float32, torch.float32, 'welford_reduce', load)
        getitem = reduction[0]
        getitem_1 = reduction[1]
        getitem_2 = reduction[2]
        get_index_1 = self.get_index('index1')
        store_reduction = ops.store_reduction('buf187', get_index_1, getitem)
        return store_reduction
buf187 Triton code:
    import triton
    import triton.language as tl
    from triton.compiler.compiler import AttrsDescriptor

    from torch._inductor.runtime import triton_helpers, triton_heuristics
    from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
    from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, instance_descriptor, DeviceProperties

    @triton_heuristics.persistent_reduction(
        size_hints=[256, 1024],
        reduction_hint=ReductionHint.INNER,
        filename=__file__,
        triton_meta={'signature': {0: '*fp32', 1: '*fp32', 2: 'i32', 3: 'i32'}, 'device': DeviceProperties(type='cuda', index=0, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, multi_processor_count=132), 'constants': {}, 'configs': [AttrsDescriptor(divisible_by_16=(0, 1, 3), equal_to_1=())]},
        inductor_meta={'autotune_hints': set(), 'kernel_name': 'Placeholder.DESCRIPTIVE_NAME', 'mutated_arg_names': [], 'no_x_dim': True, 'num_load': 1, 'num_reduction': 3, 'backend_hash': '3944FAAD9D18E987535CF029C4441E186640DD453AE5AE6831EDF736C09E0A47', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': False, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False}
    )
    @triton.jit
    def triton_(in_ptr0, out_ptr0, xnumel, rnumel):
        xnumel = 197
        XBLOCK: tl.constexpr = 1
        rnumel = 768
        RBLOCK: tl.constexpr = 1024
        xoffset = tl.program_id(0) * XBLOCK
        xindex = tl.full([1], xoffset, tl.int32)
        xmask = xindex < xnumel
        rindex = tl.arange(0, RBLOCK)[:]
        roffset = 0
        rmask = rindex < rnumel
        r1 = rindex
        x0 = xindex
        tmp0 = tl.load(in_ptr0 + (r1 + (768*x0)), rmask & xmask, other=0.0)
        tmp1 = tl.broadcast_to(tmp0, [RBLOCK])
        tmp3 = tl.where(rmask & xmask, tmp1, 0)
        tmp4 = tl.broadcast_to(tmp1, [RBLOCK])
        tmp6 = tl.where(rmask & xmask, tmp4, 0)
        tmp7 = triton_helpers.promote_to_tensor(tl.sum(tmp6, 0))
        tmp8 = tl.full([1], 768, tl.int32)
        tmp9 = tmp8.to(tl.float32)
        tmp10 = tmp7 / tmp9
        tmp11 = tmp1 - tmp10
        tmp12 = tmp11 * tmp11
        tmp13 = tl.broadcast_to(tmp12, [RBLOCK])
        tmp15 = tl.where(rmask & xmask, tmp13, 0)
        tmp16 = triton_helpers.promote_to_tensor(tl.sum(tmp15, 0))
        tl.store(out_ptr0 + (x0), tmp10, xmask)


buf188: SchedulerNode(ComputedBuffer)
buf188.writes = [MemoryDep('buf188', c0, {c0: 197}, None)]
buf188.unmet_dependencies = [MemoryDep('buf186', c0, {c0: 151296}, None)]
buf188.met_dependencies = []
buf188.users = [NodeUser(node=SchedulerNode(name='buf190'), can_inplace=False, is_weak=False)]
buf188.group.device = cuda:0
buf188.group.iteration = (197, 768)
buf188.sizes = ([197], [768])
buf186_layout = FixedLayout('cuda', torch.float32, size=[1, 197, 768], stride=[151296, 768, 1])
buf188_layout = FixedLayout('cuda', torch.float32, size=[1, 197, 1], stride=[197, 1, 197])
class buf188_loop_body:
    var_ranges = {z0: 197, z1: 768}
    index0 = 768*z0 + z1
    index1 = z0
    def body(self, ops):
        get_index = self.get_index('index0')
        load = ops.load('buf186', get_index)
        reduction = ops.reduction(torch.float32, torch.float32, 'welford_reduce', load)
        getitem = reduction[0]
        getitem_1 = reduction[1]
        getitem_2 = reduction[2]
        get_index_1 = self.get_index('index1')
        store_reduction = ops.store_reduction('buf188', get_index_1, getitem_1)
        return store_reduction
buf188 Triton code:
    import triton
    import triton.language as tl
    from triton.compiler.compiler import AttrsDescriptor

    from torch._inductor.runtime import triton_helpers, triton_heuristics
    from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
    from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, instance_descriptor, DeviceProperties

    @triton_heuristics.persistent_reduction(
        size_hints=[256, 1024],
        reduction_hint=ReductionHint.INNER,
        filename=__file__,
        triton_meta={'signature': {0: '*fp32', 1: '*fp32', 2: 'i32', 3: 'i32'}, 'device': DeviceProperties(type='cuda', index=0, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, multi_processor_count=132), 'constants': {}, 'configs': [AttrsDescriptor(divisible_by_16=(0, 1, 3), equal_to_1=())]},
        inductor_meta={'autotune_hints': set(), 'kernel_name': 'Placeholder.DESCRIPTIVE_NAME', 'mutated_arg_names': [], 'no_x_dim': True, 'num_load': 1, 'num_reduction': 3, 'backend_hash': '3944FAAD9D18E987535CF029C4441E186640DD453AE5AE6831EDF736C09E0A47', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': False, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False}
    )
    @triton.jit
    def triton_(in_ptr0, out_ptr0, xnumel, rnumel):
        xnumel = 197
        XBLOCK: tl.constexpr = 1
        rnumel = 768
        RBLOCK: tl.constexpr = 1024
        xoffset = tl.program_id(0) * XBLOCK
        xindex = tl.full([1], xoffset, tl.int32)
        xmask = xindex < xnumel
        rindex = tl.arange(0, RBLOCK)[:]
        roffset = 0
        rmask = rindex < rnumel
        r1 = rindex
        x0 = xindex
        tmp0 = tl.load(in_ptr0 + (r1 + (768*x0)), rmask & xmask, other=0.0)
        tmp1 = tl.broadcast_to(tmp0, [RBLOCK])
        tmp3 = tl.where(rmask & xmask, tmp1, 0)
        tmp4 = tl.broadcast_to(tmp1, [RBLOCK])
        tmp6 = tl.where(rmask & xmask, tmp4, 0)
        tmp7 = triton_helpers.promote_to_tensor(tl.sum(tmp6, 0))
        tmp8 = tl.full([1], 768, tl.int32)
        tmp9 = tmp8.to(tl.float32)
        tmp10 = tmp7 / tmp9
        tmp11 = tmp1 - tmp10
        tmp12 = tmp11 * tmp11
        tmp13 = tl.broadcast_to(tmp12, [RBLOCK])
        tmp15 = tl.where(rmask & xmask, tmp13, 0)
        tmp16 = triton_helpers.promote_to_tensor(tl.sum(tmp15, 0))
        tl.store(out_ptr0 + (x0), tmp16, xmask)


buf190: SchedulerNode(ComputedBuffer)
buf190.writes = [MemoryDep('buf190', c0, {c0: 151296}, None)]
buf190.unmet_dependencies = 
    [   MemoryDep('buf186', c0, {c0: 151296}, None),
        MemoryDep('buf187', c0, {c0: 197}, None),
        MemoryDep('buf188', c0, {c0: 197}, None)]
buf190.met_dependencies = 
    [   MemoryDep('arg142_1', c1, {c0: 197, c1: 768}, None),
        MemoryDep('arg143_1', c1, {c0: 197, c1: 768}, None)]
buf190.users = [NodeUser(node=ExternKernelSchedulerNode(name='buf191'), can_inplace=False, is_weak=False)]
buf190.group.device = cuda:0
buf190.group.iteration = (151296, 1)
buf190.sizes = ([197, 768], [])
buf187_layout = FixedLayout('cuda', torch.float32, size=[1, 197, 1], stride=[197, 1, 197])
buf186_layout = FixedLayout('cuda', torch.float32, size=[1, 197, 768], stride=[151296, 768, 1])
buf188_layout = FixedLayout('cuda', torch.float32, size=[1, 197, 1], stride=[197, 1, 197])
arg143_1_layout = FixedLayout('cuda', torch.float32, size=[768], stride=[1])
arg142_1_layout = FixedLayout('cuda', torch.float32, size=[768], stride=[1])
buf190_layout = FixedLayout('cuda', torch.float32, size=[1, 197, 768], stride=[151296, 768, 1])
class buf190_loop_body:
    var_ranges = {z0: 197, z1: 768}
    index0 = 768*z0 + z1
    index1 = z0
    index2 = z1
    def body(self, ops):
        get_index = self.get_index('index0')
        load = ops.load('buf186', get_index)
        get_index_1 = self.get_index('index1')
        load_1 = ops.load('buf187', get_index_1)
        sub = ops.sub(load, load_1)
        get_index_2 = self.get_index('index1')
        load_2 = ops.load('buf188', get_index_2)
        constant = ops.constant(768.0, torch.float32)
        truediv = ops.truediv(load_2, constant)
        constant_1 = ops.constant(1e-12, torch.float32)
        add = ops.add(truediv, constant_1)
        rsqrt = ops.rsqrt(add)
        mul = ops.mul(sub, rsqrt)
        get_index_3 = self.get_index('index2')
        load_3 = ops.load('arg142_1', get_index_3)
        mul_1 = ops.mul(mul, load_3)
        get_index_4 = self.get_index('index2')
        load_4 = ops.load('arg143_1', get_index_4)
        add_1 = ops.add(mul_1, load_4)
        get_index_5 = self.get_index('index0')
        store = ops.store('buf190', get_index_5, add_1, None)
        return store
buf190 Triton code:
    import triton
    import triton.language as tl
    from triton.compiler.compiler import AttrsDescriptor

    from torch._inductor.runtime import triton_helpers, triton_heuristics
    from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
    from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, instance_descriptor, DeviceProperties

    @triton_heuristics.pointwise(
        size_hints=[262144], 
        filename=__file__,
        triton_meta={'signature': {0: '*fp32', 1: '*fp32', 2: '*fp32', 3: '*fp32', 4: '*fp32', 5: '*fp32', 6: 'i32'}, 'device': DeviceProperties(type='cuda', index=0, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, multi_processor_count=132), 'constants': {}, 'configs': [AttrsDescriptor(divisible_by_16=(0, 1, 2, 3, 4, 5, 6), equal_to_1=())]},
        inductor_meta={'autotune_hints': set(), 'kernel_name': 'Placeholder.DESCRIPTIVE_NAME', 'mutated_arg_names': [], 'no_x_dim': False, 'num_load': 5, 'num_reduction': 0, 'backend_hash': '3944FAAD9D18E987535CF029C4441E186640DD453AE5AE6831EDF736C09E0A47', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': False, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False},
        min_elem_per_thread=0
    )
    @triton.jit
    def triton_(in_ptr0, in_ptr1, in_ptr2, in_ptr3, in_ptr4, out_ptr0, xnumel, XBLOCK : tl.constexpr):
        xnumel = 151296
        xoffset = tl.program_id(0) * XBLOCK
        xindex = xoffset + tl.arange(0, XBLOCK)[:]
        xmask = xindex < xnumel
        x2 = xindex
        x1 = (xindex // 768)
        x0 = xindex % 768
        tmp0 = tl.load(in_ptr0 + (x2), xmask)
        tmp1 = tl.load(in_ptr1 + (x1), xmask, eviction_policy='evict_last')
        tmp3 = tl.load(in_ptr2 + (x1), xmask, eviction_policy='evict_last')
        tmp10 = tl.load(in_ptr3 + (x0), xmask, eviction_policy='evict_last')
        tmp12 = tl.load(in_ptr4 + (x0), xmask, eviction_policy='evict_last')
        tmp2 = tmp0 - tmp1
        tmp4 = 768.0
        tmp5 = tmp3 / tmp4
        tmp6 = 1e-12
        tmp7 = tmp5 + tmp6
        tmp8 = libdevice.rsqrt(tmp7)
        tmp9 = tmp2 * tmp8
        tmp11 = tmp9 * tmp10
        tmp13 = tmp11 + tmp12
        tl.store(out_ptr0 + (x2), tmp13, xmask)


buf191: ExternKernelSchedulerNode(ExternKernelOut)
buf191.writes = [StarDep(name='buf191', mode=None)]
buf191.unmet_dependencies = [StarDep(name='buf190', mode=None)]
buf191.met_dependencies = [StarDep(name='arg144_1', mode=None)]
buf191.users = [NodeUser(node=SchedulerNode(name='buf192'), can_inplace=True, is_weak=False)]
buf191.node.kernel = extern_kernels.mm


buf192: SchedulerNode(ComputedBuffer)
buf192.writes = [MemoryDep('buf192', c0, {c0: 605184}, None)]
buf192.unmet_dependencies = [MemoryDep('buf191', c0, {c0: 605184}, None)]
buf192.met_dependencies = [MemoryDep('arg145_1', c1, {c0: 197, c1: 3072}, None)]
buf192.users = [NodeUser(node=ExternKernelSchedulerNode(name='buf193'), can_inplace=False, is_weak=False)]
buf192.group.device = cuda:0
buf192.group.iteration = (605184, 1)
buf192.sizes = ([197, 3072], [])
buf191_layout = FixedLayout('cuda', torch.float32, size=[197, 3072], stride=[3072, 1])
arg145_1_layout = FixedLayout('cuda', torch.float32, size=[3072], stride=[1])
buf192_layout = FixedLayout('cuda', torch.float32, size=[1, 197, 3072], stride=[605184, 3072, 1])
class buf192_loop_body:
    var_ranges = {z0: 197, z1: 3072}
    index0 = 3072*z0 + z1
    index1 = z1
    def body(self, ops):
        get_index = self.get_index('index0')
        load = ops.load('buf191', get_index)
        get_index_1 = self.get_index('index1')
        load_1 = ops.load('arg145_1', get_index_1)
        add = ops.add(load, load_1)
        constant = ops.constant(0.5, torch.float32)
        mul = ops.mul(add, constant)
        get_index_2 = self.get_index('index0')
        load_2 = ops.load('buf191', get_index_2)
        get_index_3 = self.get_index('index1')
        load_3 = ops.load('arg145_1', get_index_3)
        add_1 = ops.add(load_2, load_3)
        constant_1 = ops.constant(0.7071067811865476, torch.float32)
        mul_1 = ops.mul(add_1, constant_1)
        erf = ops.erf(mul_1)
        constant_2 = ops.constant(1.0, torch.float32)
        add_2 = ops.add(erf, constant_2)
        mul_2 = ops.mul(mul, add_2)
        get_index_4 = self.get_index('index0')
        store = ops.store('buf192', get_index_4, mul_2, None)
        return store
buf192 Triton code:
    import triton
    import triton.language as tl
    from triton.compiler.compiler import AttrsDescriptor

    from torch._inductor.runtime import triton_helpers, triton_heuristics
    from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
    from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, instance_descriptor, DeviceProperties

    @triton_heuristics.pointwise(
        size_hints=[1048576], 
        filename=__file__,
        triton_meta={'signature': {0: '*fp32', 1: '*fp32', 2: 'i32'}, 'device': DeviceProperties(type='cuda', index=0, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, multi_processor_count=132), 'constants': {}, 'configs': [AttrsDescriptor(divisible_by_16=(0, 1, 2), equal_to_1=())]},
        inductor_meta={'autotune_hints': set(), 'kernel_name': 'Placeholder.DESCRIPTIVE_NAME', 'mutated_arg_names': ['in_out_ptr0'], 'no_x_dim': False, 'num_load': 2, 'num_reduction': 0, 'backend_hash': '3944FAAD9D18E987535CF029C4441E186640DD453AE5AE6831EDF736C09E0A47', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': False, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False},
        min_elem_per_thread=0
    )
    @triton.jit
    def triton_(in_out_ptr0, in_ptr0, xnumel, XBLOCK : tl.constexpr):
        xnumel = 605184
        xoffset = tl.program_id(0) * XBLOCK
        xindex = xoffset + tl.arange(0, XBLOCK)[:]
        xmask = xindex < xnumel
        x2 = xindex
        x0 = xindex % 3072
        tmp0 = tl.load(in_out_ptr0 + (x2), xmask)
        tmp1 = tl.load(in_ptr0 + (x0), xmask, eviction_policy='evict_last')
        tmp2 = tmp0 + tmp1
        tmp3 = 0.5
        tmp4 = tmp2 * tmp3
        tmp5 = 0.7071067811865476
        tmp6 = tmp2 * tmp5
        tmp7 = libdevice.erf(tmp6)
        tmp8 = 1.0
        tmp9 = tmp7 + tmp8
        tmp10 = tmp4 * tmp9
        tl.store(in_out_ptr0 + (x2), tmp10, xmask)


buf193: ExternKernelSchedulerNode(ExternKernelOut)
buf193.writes = [StarDep(name='buf193', mode=None)]
buf193.unmet_dependencies = [StarDep(name='buf192', mode=None)]
buf193.met_dependencies = [StarDep(name='arg146_1', mode=None)]
buf193.users = [NodeUser(node=SchedulerNode(name='buf194'), can_inplace=False, is_weak=False), NodeUser(node=SchedulerNode(name='buf195'), can_inplace=False, is_weak=False), NodeUser(node=SchedulerNode(name='buf196'), can_inplace=False, is_weak=False), NodeUser(node=SchedulerNode(name='buf197'), can_inplace=True, is_weak=False), NodeUser(node=SchedulerNode(name='buf207'), can_inplace=True, is_weak=False)]
buf193.node.kernel = extern_kernels.mm


buf194: SchedulerNode(ComputedBuffer)
buf194.writes = [MemoryDep('buf194', c0, {c0: 197}, None)]
buf194.unmet_dependencies = 
    [   MemoryDep('buf186', c0, {c0: 151296}, None),
        MemoryDep('buf193', c0, {c0: 151296}, None)]
buf194.met_dependencies = [MemoryDep('arg147_1', c1, {c0: 197, c1: 768}, None)]
buf194.users = [NodeUser(node=SchedulerNode(name='buf197'), can_inplace=False, is_weak=False)]
buf194.group.device = cuda:0
buf194.group.iteration = (197, 768)
buf194.sizes = ([197], [768])
arg147_1_layout = FixedLayout('cuda', torch.float32, size=[768], stride=[1])
buf186_layout = FixedLayout('cuda', torch.float32, size=[1, 197, 768], stride=[151296, 768, 1])
buf193_layout = FixedLayout('cuda', torch.float32, size=[197, 768], stride=[768, 1])
buf194_layout = FixedLayout('cuda', torch.float32, size=[1, 197, 1], stride=[197, 1, 197])
class buf194_loop_body:
    var_ranges = {z0: 197, z1: 768}
    index0 = 768*z0 + z1
    index1 = z1
    index2 = z0
    def body(self, ops):
        get_index = self.get_index('index0')
        load = ops.load('buf193', get_index)
        get_index_1 = self.get_index('index1')
        load_1 = ops.load('arg147_1', get_index_1)
        add = ops.add(load, load_1)
        get_index_2 = self.get_index('index0')
        load_2 = ops.load('buf186', get_index_2)
        add_1 = ops.add(add, load_2)
        reduction = ops.reduction(torch.float32, torch.float32, 'welford_reduce', add_1)
        getitem = reduction[0]
        getitem_1 = reduction[1]
        getitem_2 = reduction[2]
        get_index_3 = self.get_index('index2')
        store_reduction = ops.store_reduction('buf194', get_index_3, getitem)
        return store_reduction
buf194 Triton code:
    import triton
    import triton.language as tl
    from triton.compiler.compiler import AttrsDescriptor

    from torch._inductor.runtime import triton_helpers, triton_heuristics
    from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
    from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, instance_descriptor, DeviceProperties

    @triton_heuristics.persistent_reduction(
        size_hints=[256, 1024],
        reduction_hint=ReductionHint.INNER,
        filename=__file__,
        triton_meta={'signature': {0: '*fp32', 1: '*fp32', 2: '*fp32', 3: '*fp32', 4: 'i32', 5: 'i32'}, 'device': DeviceProperties(type='cuda', index=0, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, multi_processor_count=132), 'constants': {}, 'configs': [AttrsDescriptor(divisible_by_16=(0, 1, 2, 3, 5), equal_to_1=())]},
        inductor_meta={'autotune_hints': set(), 'kernel_name': 'Placeholder.DESCRIPTIVE_NAME', 'mutated_arg_names': [], 'no_x_dim': True, 'num_load': 3, 'num_reduction': 3, 'backend_hash': '3944FAAD9D18E987535CF029C4441E186640DD453AE5AE6831EDF736C09E0A47', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': False, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False}
    )
    @triton.jit
    def triton_(in_ptr0, in_ptr1, in_ptr2, out_ptr0, xnumel, rnumel):
        xnumel = 197
        XBLOCK: tl.constexpr = 1
        rnumel = 768
        RBLOCK: tl.constexpr = 1024
        xoffset = tl.program_id(0) * XBLOCK
        xindex = tl.full([1], xoffset, tl.int32)
        xmask = xindex < xnumel
        rindex = tl.arange(0, RBLOCK)[:]
        roffset = 0
        rmask = rindex < rnumel
        r1 = rindex
        x0 = xindex
        tmp0 = tl.load(in_ptr0 + (r1 + (768*x0)), rmask & xmask, other=0.0)
        tmp1 = tl.load(in_ptr1 + (r1), rmask, eviction_policy='evict_last', other=0.0)
        tmp3 = tl.load(in_ptr2 + (r1 + (768*x0)), rmask & xmask, other=0.0)
        tmp2 = tmp0 + tmp1
        tmp4 = tmp2 + tmp3
        tmp5 = tl.broadcast_to(tmp4, [RBLOCK])
        tmp7 = tl.where(rmask & xmask, tmp5, 0)
        tmp8 = tl.broadcast_to(tmp5, [RBLOCK])
        tmp10 = tl.where(rmask & xmask, tmp8, 0)
        tmp11 = triton_helpers.promote_to_tensor(tl.sum(tmp10, 0))
        tmp12 = tl.full([1], 768, tl.int32)
        tmp13 = tmp12.to(tl.float32)
        tmp14 = tmp11 / tmp13
        tmp15 = tmp5 - tmp14
        tmp16 = tmp15 * tmp15
        tmp17 = tl.broadcast_to(tmp16, [RBLOCK])
        tmp19 = tl.where(rmask & xmask, tmp17, 0)
        tmp20 = triton_helpers.promote_to_tensor(tl.sum(tmp19, 0))
        tl.store(out_ptr0 + (x0), tmp14, xmask)


buf195: SchedulerNode(ComputedBuffer)
buf195.writes = [MemoryDep('buf195', c0, {c0: 197}, None)]
buf195.unmet_dependencies = 
    [   MemoryDep('buf186', c0, {c0: 151296}, None),
        MemoryDep('buf193', c0, {c0: 151296}, None)]
buf195.met_dependencies = [MemoryDep('arg147_1', c1, {c0: 197, c1: 768}, None)]
buf195.users = [NodeUser(node=SchedulerNode(name='buf197'), can_inplace=False, is_weak=False)]
buf195.group.device = cuda:0
buf195.group.iteration = (197, 768)
buf195.sizes = ([197], [768])
arg147_1_layout = FixedLayout('cuda', torch.float32, size=[768], stride=[1])
buf186_layout = FixedLayout('cuda', torch.float32, size=[1, 197, 768], stride=[151296, 768, 1])
buf193_layout = FixedLayout('cuda', torch.float32, size=[197, 768], stride=[768, 1])
buf195_layout = FixedLayout('cuda', torch.float32, size=[1, 197, 1], stride=[197, 1, 197])
class buf195_loop_body:
    var_ranges = {z0: 197, z1: 768}
    index0 = 768*z0 + z1
    index1 = z1
    index2 = z0
    def body(self, ops):
        get_index = self.get_index('index0')
        load = ops.load('buf193', get_index)
        get_index_1 = self.get_index('index1')
        load_1 = ops.load('arg147_1', get_index_1)
        add = ops.add(load, load_1)
        get_index_2 = self.get_index('index0')
        load_2 = ops.load('buf186', get_index_2)
        add_1 = ops.add(add, load_2)
        reduction = ops.reduction(torch.float32, torch.float32, 'welford_reduce', add_1)
        getitem = reduction[0]
        getitem_1 = reduction[1]
        getitem_2 = reduction[2]
        get_index_3 = self.get_index('index2')
        store_reduction = ops.store_reduction('buf195', get_index_3, getitem_1)
        return store_reduction
buf195 Triton code:
    import triton
    import triton.language as tl
    from triton.compiler.compiler import AttrsDescriptor

    from torch._inductor.runtime import triton_helpers, triton_heuristics
    from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
    from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, instance_descriptor, DeviceProperties

    @triton_heuristics.persistent_reduction(
        size_hints=[256, 1024],
        reduction_hint=ReductionHint.INNER,
        filename=__file__,
        triton_meta={'signature': {0: '*fp32', 1: '*fp32', 2: '*fp32', 3: '*fp32', 4: 'i32', 5: 'i32'}, 'device': DeviceProperties(type='cuda', index=0, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, multi_processor_count=132), 'constants': {}, 'configs': [AttrsDescriptor(divisible_by_16=(0, 1, 2, 3, 5), equal_to_1=())]},
        inductor_meta={'autotune_hints': set(), 'kernel_name': 'Placeholder.DESCRIPTIVE_NAME', 'mutated_arg_names': [], 'no_x_dim': True, 'num_load': 3, 'num_reduction': 3, 'backend_hash': '3944FAAD9D18E987535CF029C4441E186640DD453AE5AE6831EDF736C09E0A47', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': False, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False}
    )
    @triton.jit
    def triton_(in_ptr0, in_ptr1, in_ptr2, out_ptr0, xnumel, rnumel):
        xnumel = 197
        XBLOCK: tl.constexpr = 1
        rnumel = 768
        RBLOCK: tl.constexpr = 1024
        xoffset = tl.program_id(0) * XBLOCK
        xindex = tl.full([1], xoffset, tl.int32)
        xmask = xindex < xnumel
        rindex = tl.arange(0, RBLOCK)[:]
        roffset = 0
        rmask = rindex < rnumel
        r1 = rindex
        x0 = xindex
        tmp0 = tl.load(in_ptr0 + (r1 + (768*x0)), rmask & xmask, other=0.0)
        tmp1 = tl.load(in_ptr1 + (r1), rmask, eviction_policy='evict_last', other=0.0)
        tmp3 = tl.load(in_ptr2 + (r1 + (768*x0)), rmask & xmask, other=0.0)
        tmp2 = tmp0 + tmp1
        tmp4 = tmp2 + tmp3
        tmp5 = tl.broadcast_to(tmp4, [RBLOCK])
        tmp7 = tl.where(rmask & xmask, tmp5, 0)
        tmp8 = tl.broadcast_to(tmp5, [RBLOCK])
        tmp10 = tl.where(rmask & xmask, tmp8, 0)
        tmp11 = triton_helpers.promote_to_tensor(tl.sum(tmp10, 0))
        tmp12 = tl.full([1], 768, tl.int32)
        tmp13 = tmp12.to(tl.float32)
        tmp14 = tmp11 / tmp13
        tmp15 = tmp5 - tmp14
        tmp16 = tmp15 * tmp15
        tmp17 = tl.broadcast_to(tmp16, [RBLOCK])
        tmp19 = tl.where(rmask & xmask, tmp17, 0)
        tmp20 = triton_helpers.promote_to_tensor(tl.sum(tmp19, 0))
        tl.store(out_ptr0 + (x0), tmp20, xmask)


buf197: SchedulerNode(ComputedBuffer)
buf197.writes = [MemoryDep('buf197', c0, {c0: 151296}, None)]
buf197.unmet_dependencies = 
    [   MemoryDep('buf186', c0, {c0: 151296}, None),
        MemoryDep('buf193', c0, {c0: 151296}, None),
        MemoryDep('buf194', c0, {c0: 197}, None),
        MemoryDep('buf195', c0, {c0: 197}, None)]
buf197.met_dependencies = 
    [   MemoryDep('arg147_1', c1, {c0: 197, c1: 768}, None),
        MemoryDep('arg148_1', c1, {c0: 197, c1: 768}, None),
        MemoryDep('arg149_1', c1, {c0: 197, c1: 768}, None)]
buf197.users = [NodeUser(node=ExternKernelSchedulerNode(name='buf198'), can_inplace=False, is_weak=False), NodeUser(node=ExternKernelSchedulerNode(name='buf199'), can_inplace=False, is_weak=False), NodeUser(node=ExternKernelSchedulerNode(name='buf200'), can_inplace=False, is_weak=False)]
buf197.group.device = cuda:0
buf197.group.iteration = (151296, 1)
buf197.sizes = ([197, 768], [])
arg149_1_layout = FixedLayout('cuda', torch.float32, size=[768], stride=[1])
arg147_1_layout = FixedLayout('cuda', torch.float32, size=[768], stride=[1])
buf194_layout = FixedLayout('cuda', torch.float32, size=[1, 197, 1], stride=[197, 1, 197])
buf195_layout = FixedLayout('cuda', torch.float32, size=[1, 197, 1], stride=[197, 1, 197])
buf193_layout = FixedLayout('cuda', torch.float32, size=[197, 768], stride=[768, 1])
buf186_layout = FixedLayout('cuda', torch.float32, size=[1, 197, 768], stride=[151296, 768, 1])
arg148_1_layout = FixedLayout('cuda', torch.float32, size=[768], stride=[1])
buf197_layout = FixedLayout('cuda', torch.float32, size=[1, 197, 768], stride=[151296, 768, 1])
class buf197_loop_body:
    var_ranges = {z0: 197, z1: 768}
    index0 = 768*z0 + z1
    index1 = z1
    index2 = z0
    def body(self, ops):
        get_index = self.get_index('index0')
        load = ops.load('buf193', get_index)
        get_index_1 = self.get_index('index1')
        load_1 = ops.load('arg147_1', get_index_1)
        add = ops.add(load, load_1)
        get_index_2 = self.get_index('index0')
        load_2 = ops.load('buf186', get_index_2)
        add_1 = ops.add(add, load_2)
        get_index_3 = self.get_index('index2')
        load_3 = ops.load('buf194', get_index_3)
        sub = ops.sub(add_1, load_3)
        get_index_4 = self.get_index('index2')
        load_4 = ops.load('buf195', get_index_4)
        constant = ops.constant(768.0, torch.float32)
        truediv = ops.truediv(load_4, constant)
        constant_1 = ops.constant(1e-12, torch.float32)
        add_2 = ops.add(truediv, constant_1)
        rsqrt = ops.rsqrt(add_2)
        mul = ops.mul(sub, rsqrt)
        get_index_5 = self.get_index('index1')
        load_5 = ops.load('arg148_1', get_index_5)
        mul_1 = ops.mul(mul, load_5)
        get_index_6 = self.get_index('index1')
        load_6 = ops.load('arg149_1', get_index_6)
        add_3 = ops.add(mul_1, load_6)
        get_index_7 = self.get_index('index0')
        store = ops.store('buf197', get_index_7, add_3, None)
        return store
buf197 Triton code:
    import triton
    import triton.language as tl
    from triton.compiler.compiler import AttrsDescriptor

    from torch._inductor.runtime import triton_helpers, triton_heuristics
    from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
    from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, instance_descriptor, DeviceProperties

    @triton_heuristics.pointwise(
        size_hints=[262144], 
        filename=__file__,
        triton_meta={'signature': {0: '*fp32', 1: '*fp32', 2: '*fp32', 3: '*fp32', 4: '*fp32', 5: '*fp32', 6: '*fp32', 7: '*fp32', 8: 'i32'}, 'device': DeviceProperties(type='cuda', index=0, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, multi_processor_count=132), 'constants': {}, 'configs': [AttrsDescriptor(divisible_by_16=(0, 1, 2, 3, 4, 5, 6, 7, 8), equal_to_1=())]},
        inductor_meta={'autotune_hints': set(), 'kernel_name': 'Placeholder.DESCRIPTIVE_NAME', 'mutated_arg_names': [], 'no_x_dim': False, 'num_load': 7, 'num_reduction': 0, 'backend_hash': '3944FAAD9D18E987535CF029C4441E186640DD453AE5AE6831EDF736C09E0A47', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': False, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False},
        min_elem_per_thread=0
    )
    @triton.jit
    def triton_(in_ptr0, in_ptr1, in_ptr2, in_ptr3, in_ptr4, in_ptr5, in_ptr6, out_ptr0, xnumel, XBLOCK : tl.constexpr):
        xnumel = 151296
        xoffset = tl.program_id(0) * XBLOCK
        xindex = xoffset + tl.arange(0, XBLOCK)[:]
        xmask = xindex < xnumel
        x2 = xindex
        x0 = xindex % 768
        x1 = (xindex // 768)
        tmp0 = tl.load(in_ptr0 + (x2), xmask)
        tmp1 = tl.load(in_ptr1 + (x0), xmask, eviction_policy='evict_last')
        tmp3 = tl.load(in_ptr2 + (x2), xmask)
        tmp5 = tl.load(in_ptr3 + (x1), xmask, eviction_policy='evict_last')
        tmp7 = tl.load(in_ptr4 + (x1), xmask, eviction_policy='evict_last')
        tmp14 = tl.load(in_ptr5 + (x0), xmask, eviction_policy='evict_last')
        tmp16 = tl.load(in_ptr6 + (x0), xmask, eviction_policy='evict_last')
        tmp2 = tmp0 + tmp1
        tmp4 = tmp2 + tmp3
        tmp6 = tmp4 - tmp5
        tmp8 = 768.0
        tmp9 = tmp7 / tmp8
        tmp10 = 1e-12
        tmp11 = tmp9 + tmp10
        tmp12 = libdevice.rsqrt(tmp11)
        tmp13 = tmp6 * tmp12
        tmp15 = tmp13 * tmp14
        tmp17 = tmp15 + tmp16
        tl.store(out_ptr0 + (x2), tmp17, xmask)


buf198: ExternKernelSchedulerNode(ExternKernelOut)
buf198.writes = [StarDep(name='buf198', mode=None)]
buf198.unmet_dependencies = [StarDep(name='buf197', mode=None)]
buf198.met_dependencies = [StarDep(name='arg150_1', mode=None), StarDep(name='arg151_1', mode=None)]
buf198.users = [NodeUser(node=ExternKernelSchedulerNode(name='buf201'), can_inplace=False, is_weak=False)]
buf198.node.kernel = extern_kernels.addmm


buf199: ExternKernelSchedulerNode(ExternKernelOut)
buf199.writes = [StarDep(name='buf199', mode=None)]
buf199.unmet_dependencies = [StarDep(name='buf197', mode=None)]
buf199.met_dependencies = [StarDep(name='arg152_1', mode=None), StarDep(name='arg153_1', mode=None)]
buf199.users = [NodeUser(node=ExternKernelSchedulerNode(name='buf201'), can_inplace=False, is_weak=False)]
buf199.node.kernel = extern_kernels.addmm


buf200: ExternKernelSchedulerNode(ExternKernelOut)
buf200.writes = [StarDep(name='buf200', mode=None)]
buf200.unmet_dependencies = [StarDep(name='buf197', mode=None)]
buf200.met_dependencies = [StarDep(name='arg154_1', mode=None), StarDep(name='arg155_1', mode=None)]
buf200.users = [NodeUser(node=ExternKernelSchedulerNode(name='buf201'), can_inplace=False, is_weak=False)]
buf200.node.kernel = extern_kernels.addmm


buf201: ExternKernelSchedulerNode(FallbackKernel)
buf201.writes = [StarDep(name='buf201', mode=None)]
buf201.unmet_dependencies = 
    [   StarDep(name='buf198', mode=None),
        StarDep(name='buf199', mode=None),
        StarDep(name='buf200', mode=None)]
buf201.met_dependencies = []
buf201.users = [NodeUser(node=ExternKernelSchedulerNode(name='buf202'), can_inplace=False, is_weak=False), NodeUser(node=ExternKernelSchedulerNode(name='buf203'), can_inplace=False, is_weak=False), NodeUser(node=ExternKernelSchedulerNode(name='buf204'), can_inplace=False, is_weak=False), NodeUser(node=ExternKernelSchedulerNode(name='buf205'), can_inplace=False, is_weak=False)]
buf201.node.kernel = None


buf202: ExternKernelSchedulerNode(MultiOutput)
buf202.writes = [StarDep(name='buf202', mode=None)]
buf202.unmet_dependencies = [StarDep(name='buf201', mode=None)]
buf202.met_dependencies = []
buf202.users = [NodeUser(node=ExternKernelSchedulerNode(name='buf206'), can_inplace=False, is_weak=False)]
buf202.node.kernel = None


buf206: ExternKernelSchedulerNode(ExternKernelOut)
buf206.writes = [StarDep(name='buf206', mode=None)]
buf206.unmet_dependencies = [StarDep(name='buf202', mode=None)]
buf206.met_dependencies = [StarDep(name='arg156_1', mode=None)]
buf206.users = [NodeUser(node=SchedulerNode(name='buf207'), can_inplace=True, is_weak=False)]
buf206.node.kernel = extern_kernels.mm


buf207: SchedulerNode(ComputedBuffer)
buf207.writes = [MemoryDep('buf207', c0, {c0: 151296}, None)]
buf207.unmet_dependencies = 
    [   MemoryDep('buf186', c0, {c0: 151296}, None),
        MemoryDep('buf193', c0, {c0: 151296}, None),
        MemoryDep('buf206', c0, {c0: 151296}, None)]
buf207.met_dependencies = 
    [   MemoryDep('arg147_1', c1, {c0: 197, c1: 768}, None),
        MemoryDep('arg157_1', c1, {c0: 197, c1: 768}, None)]
buf207.users = [NodeUser(node=SchedulerNode(name='buf208'), can_inplace=False, is_weak=False), NodeUser(node=SchedulerNode(name='buf209'), can_inplace=False, is_weak=False), NodeUser(node=SchedulerNode(name='buf210'), can_inplace=False, is_weak=False), NodeUser(node=SchedulerNode(name='buf211'), can_inplace=True, is_weak=False), NodeUser(node=SchedulerNode(name='buf215'), can_inplace=False, is_weak=False), NodeUser(node=SchedulerNode(name='buf216'), can_inplace=False, is_weak=False), NodeUser(node=SchedulerNode(name='buf217'), can_inplace=False, is_weak=False), NodeUser(node=SchedulerNode(name='buf218'), can_inplace=True, is_weak=False), NodeUser(node=SchedulerNode(name='buf228'), can_inplace=True, is_weak=False)]
buf207.group.device = cuda:0
buf207.group.iteration = (151296, 1)
buf207.sizes = ([197, 768], [])
arg157_1_layout = FixedLayout('cuda', torch.float32, size=[768], stride=[1])
arg147_1_layout = FixedLayout('cuda', torch.float32, size=[768], stride=[1])
buf206_layout = FixedLayout('cuda', torch.float32, size=[197, 768], stride=[768, 1])
buf186_layout = FixedLayout('cuda', torch.float32, size=[1, 197, 768], stride=[151296, 768, 1])
buf193_layout = FixedLayout('cuda', torch.float32, size=[197, 768], stride=[768, 1])
buf207_layout = FixedLayout('cuda', torch.float32, size=[1, 197, 768], stride=[151296, 768, 1])
class buf207_loop_body:
    var_ranges = {z0: 197, z1: 768}
    index0 = 768*z0 + z1
    index1 = z1
    def body(self, ops):
        get_index = self.get_index('index0')
        load = ops.load('buf206', get_index)
        get_index_1 = self.get_index('index1')
        load_1 = ops.load('arg157_1', get_index_1)
        add = ops.add(load, load_1)
        get_index_2 = self.get_index('index0')
        load_2 = ops.load('buf193', get_index_2)
        get_index_3 = self.get_index('index1')
        load_3 = ops.load('arg147_1', get_index_3)
        add_1 = ops.add(load_2, load_3)
        get_index_4 = self.get_index('index0')
        load_4 = ops.load('buf186', get_index_4)
        add_2 = ops.add(add_1, load_4)
        add_3 = ops.add(add, add_2)
        get_index_5 = self.get_index('index0')
        store = ops.store('buf207', get_index_5, add_3, None)
        return store
buf207 Triton code:
    import triton
    import triton.language as tl
    from triton.compiler.compiler import AttrsDescriptor

    from torch._inductor.runtime import triton_helpers, triton_heuristics
    from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
    from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, instance_descriptor, DeviceProperties

    @triton_heuristics.pointwise(
        size_hints=[262144], 
        filename=__file__,
        triton_meta={'signature': {0: '*fp32', 1: '*fp32', 2: '*fp32', 3: '*fp32', 4: '*fp32', 5: 'i32'}, 'device': DeviceProperties(type='cuda', index=0, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, multi_processor_count=132), 'constants': {}, 'configs': [AttrsDescriptor(divisible_by_16=(0, 1, 2, 3, 4, 5), equal_to_1=())]},
        inductor_meta={'autotune_hints': set(), 'kernel_name': 'Placeholder.DESCRIPTIVE_NAME', 'mutated_arg_names': ['in_out_ptr0'], 'no_x_dim': False, 'num_load': 5, 'num_reduction': 0, 'backend_hash': '3944FAAD9D18E987535CF029C4441E186640DD453AE5AE6831EDF736C09E0A47', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': False, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False},
        min_elem_per_thread=0
    )
    @triton.jit
    def triton_(in_out_ptr0, in_ptr0, in_ptr1, in_ptr2, in_ptr3, xnumel, XBLOCK : tl.constexpr):
        xnumel = 151296
        xoffset = tl.program_id(0) * XBLOCK
        xindex = xoffset + tl.arange(0, XBLOCK)[:]
        xmask = xindex < xnumel
        x2 = xindex
        x0 = xindex % 768
        tmp0 = tl.load(in_out_ptr0 + (x2), xmask)
        tmp1 = tl.load(in_ptr0 + (x0), xmask, eviction_policy='evict_last')
        tmp3 = tl.load(in_ptr1 + (x2), xmask)
        tmp4 = tl.load(in_ptr2 + (x0), xmask, eviction_policy='evict_last')
        tmp6 = tl.load(in_ptr3 + (x2), xmask)
        tmp2 = tmp0 + tmp1
        tmp5 = tmp3 + tmp4
        tmp7 = tmp5 + tmp6
        tmp8 = tmp2 + tmp7
        tl.store(in_out_ptr0 + (x2), tmp8, xmask)


buf208: SchedulerNode(ComputedBuffer)
buf208.writes = [MemoryDep('buf208', c0, {c0: 197}, None)]
buf208.unmet_dependencies = [MemoryDep('buf207', c0, {c0: 151296}, None)]
buf208.met_dependencies = []
buf208.users = [NodeUser(node=SchedulerNode(name='buf211'), can_inplace=False, is_weak=False)]
buf208.group.device = cuda:0
buf208.group.iteration = (197, 768)
buf208.sizes = ([197], [768])
buf207_layout = FixedLayout('cuda', torch.float32, size=[1, 197, 768], stride=[151296, 768, 1])
buf208_layout = FixedLayout('cuda', torch.float32, size=[1, 197, 1], stride=[197, 1, 197])
class buf208_loop_body:
    var_ranges = {z0: 197, z1: 768}
    index0 = 768*z0 + z1
    index1 = z0
    def body(self, ops):
        get_index = self.get_index('index0')
        load = ops.load('buf207', get_index)
        reduction = ops.reduction(torch.float32, torch.float32, 'welford_reduce', load)
        getitem = reduction[0]
        getitem_1 = reduction[1]
        getitem_2 = reduction[2]
        get_index_1 = self.get_index('index1')
        store_reduction = ops.store_reduction('buf208', get_index_1, getitem)
        return store_reduction
buf208 Triton code:
    import triton
    import triton.language as tl
    from triton.compiler.compiler import AttrsDescriptor

    from torch._inductor.runtime import triton_helpers, triton_heuristics
    from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
    from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, instance_descriptor, DeviceProperties

    @triton_heuristics.persistent_reduction(
        size_hints=[256, 1024],
        reduction_hint=ReductionHint.INNER,
        filename=__file__,
        triton_meta={'signature': {0: '*fp32', 1: '*fp32', 2: 'i32', 3: 'i32'}, 'device': DeviceProperties(type='cuda', index=0, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, multi_processor_count=132), 'constants': {}, 'configs': [AttrsDescriptor(divisible_by_16=(0, 1, 3), equal_to_1=())]},
        inductor_meta={'autotune_hints': set(), 'kernel_name': 'Placeholder.DESCRIPTIVE_NAME', 'mutated_arg_names': [], 'no_x_dim': True, 'num_load': 1, 'num_reduction': 3, 'backend_hash': '3944FAAD9D18E987535CF029C4441E186640DD453AE5AE6831EDF736C09E0A47', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': False, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False}
    )
    @triton.jit
    def triton_(in_ptr0, out_ptr0, xnumel, rnumel):
        xnumel = 197
        XBLOCK: tl.constexpr = 1
        rnumel = 768
        RBLOCK: tl.constexpr = 1024
        xoffset = tl.program_id(0) * XBLOCK
        xindex = tl.full([1], xoffset, tl.int32)
        xmask = xindex < xnumel
        rindex = tl.arange(0, RBLOCK)[:]
        roffset = 0
        rmask = rindex < rnumel
        r1 = rindex
        x0 = xindex
        tmp0 = tl.load(in_ptr0 + (r1 + (768*x0)), rmask & xmask, other=0.0)
        tmp1 = tl.broadcast_to(tmp0, [RBLOCK])
        tmp3 = tl.where(rmask & xmask, tmp1, 0)
        tmp4 = tl.broadcast_to(tmp1, [RBLOCK])
        tmp6 = tl.where(rmask & xmask, tmp4, 0)
        tmp7 = triton_helpers.promote_to_tensor(tl.sum(tmp6, 0))
        tmp8 = tl.full([1], 768, tl.int32)
        tmp9 = tmp8.to(tl.float32)
        tmp10 = tmp7 / tmp9
        tmp11 = tmp1 - tmp10
        tmp12 = tmp11 * tmp11
        tmp13 = tl.broadcast_to(tmp12, [RBLOCK])
        tmp15 = tl.where(rmask & xmask, tmp13, 0)
        tmp16 = triton_helpers.promote_to_tensor(tl.sum(tmp15, 0))
        tl.store(out_ptr0 + (x0), tmp10, xmask)


buf209: SchedulerNode(ComputedBuffer)
buf209.writes = [MemoryDep('buf209', c0, {c0: 197}, None)]
buf209.unmet_dependencies = [MemoryDep('buf207', c0, {c0: 151296}, None)]
buf209.met_dependencies = []
buf209.users = [NodeUser(node=SchedulerNode(name='buf211'), can_inplace=False, is_weak=False)]
buf209.group.device = cuda:0
buf209.group.iteration = (197, 768)
buf209.sizes = ([197], [768])
buf207_layout = FixedLayout('cuda', torch.float32, size=[1, 197, 768], stride=[151296, 768, 1])
buf209_layout = FixedLayout('cuda', torch.float32, size=[1, 197, 1], stride=[197, 1, 197])
class buf209_loop_body:
    var_ranges = {z0: 197, z1: 768}
    index0 = 768*z0 + z1
    index1 = z0
    def body(self, ops):
        get_index = self.get_index('index0')
        load = ops.load('buf207', get_index)
        reduction = ops.reduction(torch.float32, torch.float32, 'welford_reduce', load)
        getitem = reduction[0]
        getitem_1 = reduction[1]
        getitem_2 = reduction[2]
        get_index_1 = self.get_index('index1')
        store_reduction = ops.store_reduction('buf209', get_index_1, getitem_1)
        return store_reduction
buf209 Triton code:
    import triton
    import triton.language as tl
    from triton.compiler.compiler import AttrsDescriptor

    from torch._inductor.runtime import triton_helpers, triton_heuristics
    from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
    from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, instance_descriptor, DeviceProperties

    @triton_heuristics.persistent_reduction(
        size_hints=[256, 1024],
        reduction_hint=ReductionHint.INNER,
        filename=__file__,
        triton_meta={'signature': {0: '*fp32', 1: '*fp32', 2: 'i32', 3: 'i32'}, 'device': DeviceProperties(type='cuda', index=0, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, multi_processor_count=132), 'constants': {}, 'configs': [AttrsDescriptor(divisible_by_16=(0, 1, 3), equal_to_1=())]},
        inductor_meta={'autotune_hints': set(), 'kernel_name': 'Placeholder.DESCRIPTIVE_NAME', 'mutated_arg_names': [], 'no_x_dim': True, 'num_load': 1, 'num_reduction': 3, 'backend_hash': '3944FAAD9D18E987535CF029C4441E186640DD453AE5AE6831EDF736C09E0A47', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': False, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False}
    )
    @triton.jit
    def triton_(in_ptr0, out_ptr0, xnumel, rnumel):
        xnumel = 197
        XBLOCK: tl.constexpr = 1
        rnumel = 768
        RBLOCK: tl.constexpr = 1024
        xoffset = tl.program_id(0) * XBLOCK
        xindex = tl.full([1], xoffset, tl.int32)
        xmask = xindex < xnumel
        rindex = tl.arange(0, RBLOCK)[:]
        roffset = 0
        rmask = rindex < rnumel
        r1 = rindex
        x0 = xindex
        tmp0 = tl.load(in_ptr0 + (r1 + (768*x0)), rmask & xmask, other=0.0)
        tmp1 = tl.broadcast_to(tmp0, [RBLOCK])
        tmp3 = tl.where(rmask & xmask, tmp1, 0)
        tmp4 = tl.broadcast_to(tmp1, [RBLOCK])
        tmp6 = tl.where(rmask & xmask, tmp4, 0)
        tmp7 = triton_helpers.promote_to_tensor(tl.sum(tmp6, 0))
        tmp8 = tl.full([1], 768, tl.int32)
        tmp9 = tmp8.to(tl.float32)
        tmp10 = tmp7 / tmp9
        tmp11 = tmp1 - tmp10
        tmp12 = tmp11 * tmp11
        tmp13 = tl.broadcast_to(tmp12, [RBLOCK])
        tmp15 = tl.where(rmask & xmask, tmp13, 0)
        tmp16 = triton_helpers.promote_to_tensor(tl.sum(tmp15, 0))
        tl.store(out_ptr0 + (x0), tmp16, xmask)


buf211: SchedulerNode(ComputedBuffer)
buf211.writes = [MemoryDep('buf211', c0, {c0: 151296}, None)]
buf211.unmet_dependencies = 
    [   MemoryDep('buf207', c0, {c0: 151296}, None),
        MemoryDep('buf208', c0, {c0: 197}, None),
        MemoryDep('buf209', c0, {c0: 197}, None)]
buf211.met_dependencies = 
    [   MemoryDep('arg158_1', c1, {c0: 197, c1: 768}, None),
        MemoryDep('arg159_1', c1, {c0: 197, c1: 768}, None)]
buf211.users = [NodeUser(node=ExternKernelSchedulerNode(name='buf212'), can_inplace=False, is_weak=False)]
buf211.group.device = cuda:0
buf211.group.iteration = (151296, 1)
buf211.sizes = ([197, 768], [])
arg159_1_layout = FixedLayout('cuda', torch.float32, size=[768], stride=[1])
buf208_layout = FixedLayout('cuda', torch.float32, size=[1, 197, 1], stride=[197, 1, 197])
buf207_layout = FixedLayout('cuda', torch.float32, size=[1, 197, 768], stride=[151296, 768, 1])
arg158_1_layout = FixedLayout('cuda', torch.float32, size=[768], stride=[1])
buf209_layout = FixedLayout('cuda', torch.float32, size=[1, 197, 1], stride=[197, 1, 197])
buf211_layout = FixedLayout('cuda', torch.float32, size=[1, 197, 768], stride=[151296, 768, 1])
class buf211_loop_body:
    var_ranges = {z0: 197, z1: 768}
    index0 = 768*z0 + z1
    index1 = z0
    index2 = z1
    def body(self, ops):
        get_index = self.get_index('index0')
        load = ops.load('buf207', get_index)
        get_index_1 = self.get_index('index1')
        load_1 = ops.load('buf208', get_index_1)
        sub = ops.sub(load, load_1)
        get_index_2 = self.get_index('index1')
        load_2 = ops.load('buf209', get_index_2)
        constant = ops.constant(768.0, torch.float32)
        truediv = ops.truediv(load_2, constant)
        constant_1 = ops.constant(1e-12, torch.float32)
        add = ops.add(truediv, constant_1)
        rsqrt = ops.rsqrt(add)
        mul = ops.mul(sub, rsqrt)
        get_index_3 = self.get_index('index2')
        load_3 = ops.load('arg158_1', get_index_3)
        mul_1 = ops.mul(mul, load_3)
        get_index_4 = self.get_index('index2')
        load_4 = ops.load('arg159_1', get_index_4)
        add_1 = ops.add(mul_1, load_4)
        get_index_5 = self.get_index('index0')
        store = ops.store('buf211', get_index_5, add_1, None)
        return store
buf211 Triton code:
    import triton
    import triton.language as tl
    from triton.compiler.compiler import AttrsDescriptor

    from torch._inductor.runtime import triton_helpers, triton_heuristics
    from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
    from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, instance_descriptor, DeviceProperties

    @triton_heuristics.pointwise(
        size_hints=[262144], 
        filename=__file__,
        triton_meta={'signature': {0: '*fp32', 1: '*fp32', 2: '*fp32', 3: '*fp32', 4: '*fp32', 5: '*fp32', 6: 'i32'}, 'device': DeviceProperties(type='cuda', index=0, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, multi_processor_count=132), 'constants': {}, 'configs': [AttrsDescriptor(divisible_by_16=(0, 1, 2, 3, 4, 5, 6), equal_to_1=())]},
        inductor_meta={'autotune_hints': set(), 'kernel_name': 'Placeholder.DESCRIPTIVE_NAME', 'mutated_arg_names': [], 'no_x_dim': False, 'num_load': 5, 'num_reduction': 0, 'backend_hash': '3944FAAD9D18E987535CF029C4441E186640DD453AE5AE6831EDF736C09E0A47', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': False, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False},
        min_elem_per_thread=0
    )
    @triton.jit
    def triton_(in_ptr0, in_ptr1, in_ptr2, in_ptr3, in_ptr4, out_ptr0, xnumel, XBLOCK : tl.constexpr):
        xnumel = 151296
        xoffset = tl.program_id(0) * XBLOCK
        xindex = xoffset + tl.arange(0, XBLOCK)[:]
        xmask = xindex < xnumel
        x2 = xindex
        x1 = (xindex // 768)
        x0 = xindex % 768
        tmp0 = tl.load(in_ptr0 + (x2), xmask)
        tmp1 = tl.load(in_ptr1 + (x1), xmask, eviction_policy='evict_last')
        tmp3 = tl.load(in_ptr2 + (x1), xmask, eviction_policy='evict_last')
        tmp10 = tl.load(in_ptr3 + (x0), xmask, eviction_policy='evict_last')
        tmp12 = tl.load(in_ptr4 + (x0), xmask, eviction_policy='evict_last')
        tmp2 = tmp0 - tmp1
        tmp4 = 768.0
        tmp5 = tmp3 / tmp4
        tmp6 = 1e-12
        tmp7 = tmp5 + tmp6
        tmp8 = libdevice.rsqrt(tmp7)
        tmp9 = tmp2 * tmp8
        tmp11 = tmp9 * tmp10
        tmp13 = tmp11 + tmp12
        tl.store(out_ptr0 + (x2), tmp13, xmask)


buf212: ExternKernelSchedulerNode(ExternKernelOut)
buf212.writes = [StarDep(name='buf212', mode=None)]
buf212.unmet_dependencies = [StarDep(name='buf211', mode=None)]
buf212.met_dependencies = [StarDep(name='arg160_1', mode=None)]
buf212.users = [NodeUser(node=SchedulerNode(name='buf213'), can_inplace=True, is_weak=False)]
buf212.node.kernel = extern_kernels.mm


buf213: SchedulerNode(ComputedBuffer)
buf213.writes = [MemoryDep('buf213', c0, {c0: 605184}, None)]
buf213.unmet_dependencies = [MemoryDep('buf212', c0, {c0: 605184}, None)]
buf213.met_dependencies = [MemoryDep('arg161_1', c1, {c0: 197, c1: 3072}, None)]
buf213.users = [NodeUser(node=ExternKernelSchedulerNode(name='buf214'), can_inplace=False, is_weak=False)]
buf213.group.device = cuda:0
buf213.group.iteration = (605184, 1)
buf213.sizes = ([197, 3072], [])
buf212_layout = FixedLayout('cuda', torch.float32, size=[197, 3072], stride=[3072, 1])
arg161_1_layout = FixedLayout('cuda', torch.float32, size=[3072], stride=[1])
buf213_layout = FixedLayout('cuda', torch.float32, size=[1, 197, 3072], stride=[605184, 3072, 1])
class buf213_loop_body:
    var_ranges = {z0: 197, z1: 3072}
    index0 = 3072*z0 + z1
    index1 = z1
    def body(self, ops):
        get_index = self.get_index('index0')
        load = ops.load('buf212', get_index)
        get_index_1 = self.get_index('index1')
        load_1 = ops.load('arg161_1', get_index_1)
        add = ops.add(load, load_1)
        constant = ops.constant(0.5, torch.float32)
        mul = ops.mul(add, constant)
        get_index_2 = self.get_index('index0')
        load_2 = ops.load('buf212', get_index_2)
        get_index_3 = self.get_index('index1')
        load_3 = ops.load('arg161_1', get_index_3)
        add_1 = ops.add(load_2, load_3)
        constant_1 = ops.constant(0.7071067811865476, torch.float32)
        mul_1 = ops.mul(add_1, constant_1)
        erf = ops.erf(mul_1)
        constant_2 = ops.constant(1.0, torch.float32)
        add_2 = ops.add(erf, constant_2)
        mul_2 = ops.mul(mul, add_2)
        get_index_4 = self.get_index('index0')
        store = ops.store('buf213', get_index_4, mul_2, None)
        return store
buf213 Triton code:
    import triton
    import triton.language as tl
    from triton.compiler.compiler import AttrsDescriptor

    from torch._inductor.runtime import triton_helpers, triton_heuristics
    from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
    from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, instance_descriptor, DeviceProperties

    @triton_heuristics.pointwise(
        size_hints=[1048576], 
        filename=__file__,
        triton_meta={'signature': {0: '*fp32', 1: '*fp32', 2: 'i32'}, 'device': DeviceProperties(type='cuda', index=0, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, multi_processor_count=132), 'constants': {}, 'configs': [AttrsDescriptor(divisible_by_16=(0, 1, 2), equal_to_1=())]},
        inductor_meta={'autotune_hints': set(), 'kernel_name': 'Placeholder.DESCRIPTIVE_NAME', 'mutated_arg_names': ['in_out_ptr0'], 'no_x_dim': False, 'num_load': 2, 'num_reduction': 0, 'backend_hash': '3944FAAD9D18E987535CF029C4441E186640DD453AE5AE6831EDF736C09E0A47', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': False, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False},
        min_elem_per_thread=0
    )
    @triton.jit
    def triton_(in_out_ptr0, in_ptr0, xnumel, XBLOCK : tl.constexpr):
        xnumel = 605184
        xoffset = tl.program_id(0) * XBLOCK
        xindex = xoffset + tl.arange(0, XBLOCK)[:]
        xmask = xindex < xnumel
        x2 = xindex
        x0 = xindex % 3072
        tmp0 = tl.load(in_out_ptr0 + (x2), xmask)
        tmp1 = tl.load(in_ptr0 + (x0), xmask, eviction_policy='evict_last')
        tmp2 = tmp0 + tmp1
        tmp3 = 0.5
        tmp4 = tmp2 * tmp3
        tmp5 = 0.7071067811865476
        tmp6 = tmp2 * tmp5
        tmp7 = libdevice.erf(tmp6)
        tmp8 = 1.0
        tmp9 = tmp7 + tmp8
        tmp10 = tmp4 * tmp9
        tl.store(in_out_ptr0 + (x2), tmp10, xmask)


buf214: ExternKernelSchedulerNode(ExternKernelOut)
buf214.writes = [StarDep(name='buf214', mode=None)]
buf214.unmet_dependencies = [StarDep(name='buf213', mode=None)]
buf214.met_dependencies = [StarDep(name='arg162_1', mode=None)]
buf214.users = [NodeUser(node=SchedulerNode(name='buf215'), can_inplace=False, is_weak=False), NodeUser(node=SchedulerNode(name='buf216'), can_inplace=False, is_weak=False), NodeUser(node=SchedulerNode(name='buf217'), can_inplace=False, is_weak=False), NodeUser(node=SchedulerNode(name='buf218'), can_inplace=True, is_weak=False), NodeUser(node=SchedulerNode(name='buf228'), can_inplace=True, is_weak=False)]
buf214.node.kernel = extern_kernels.mm


buf215: SchedulerNode(ComputedBuffer)
buf215.writes = [MemoryDep('buf215', c0, {c0: 197}, None)]
buf215.unmet_dependencies = 
    [   MemoryDep('buf207', c0, {c0: 151296}, None),
        MemoryDep('buf214', c0, {c0: 151296}, None)]
buf215.met_dependencies = [MemoryDep('arg163_1', c1, {c0: 197, c1: 768}, None)]
buf215.users = [NodeUser(node=SchedulerNode(name='buf218'), can_inplace=False, is_weak=False)]
buf215.group.device = cuda:0
buf215.group.iteration = (197, 768)
buf215.sizes = ([197], [768])
buf207_layout = FixedLayout('cuda', torch.float32, size=[1, 197, 768], stride=[151296, 768, 1])
buf214_layout = FixedLayout('cuda', torch.float32, size=[197, 768], stride=[768, 1])
arg163_1_layout = FixedLayout('cuda', torch.float32, size=[768], stride=[1])
buf215_layout = FixedLayout('cuda', torch.float32, size=[1, 197, 1], stride=[197, 1, 197])
class buf215_loop_body:
    var_ranges = {z0: 197, z1: 768}
    index0 = 768*z0 + z1
    index1 = z1
    index2 = z0
    def body(self, ops):
        get_index = self.get_index('index0')
        load = ops.load('buf214', get_index)
        get_index_1 = self.get_index('index1')
        load_1 = ops.load('arg163_1', get_index_1)
        add = ops.add(load, load_1)
        get_index_2 = self.get_index('index0')
        load_2 = ops.load('buf207', get_index_2)
        add_1 = ops.add(add, load_2)
        reduction = ops.reduction(torch.float32, torch.float32, 'welford_reduce', add_1)
        getitem = reduction[0]
        getitem_1 = reduction[1]
        getitem_2 = reduction[2]
        get_index_3 = self.get_index('index2')
        store_reduction = ops.store_reduction('buf215', get_index_3, getitem)
        return store_reduction
buf215 Triton code:
    import triton
    import triton.language as tl
    from triton.compiler.compiler import AttrsDescriptor

    from torch._inductor.runtime import triton_helpers, triton_heuristics
    from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
    from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, instance_descriptor, DeviceProperties

    @triton_heuristics.persistent_reduction(
        size_hints=[256, 1024],
        reduction_hint=ReductionHint.INNER,
        filename=__file__,
        triton_meta={'signature': {0: '*fp32', 1: '*fp32', 2: '*fp32', 3: '*fp32', 4: 'i32', 5: 'i32'}, 'device': DeviceProperties(type='cuda', index=0, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, multi_processor_count=132), 'constants': {}, 'configs': [AttrsDescriptor(divisible_by_16=(0, 1, 2, 3, 5), equal_to_1=())]},
        inductor_meta={'autotune_hints': set(), 'kernel_name': 'Placeholder.DESCRIPTIVE_NAME', 'mutated_arg_names': [], 'no_x_dim': True, 'num_load': 3, 'num_reduction': 3, 'backend_hash': '3944FAAD9D18E987535CF029C4441E186640DD453AE5AE6831EDF736C09E0A47', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': False, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False}
    )
    @triton.jit
    def triton_(in_ptr0, in_ptr1, in_ptr2, out_ptr0, xnumel, rnumel):
        xnumel = 197
        XBLOCK: tl.constexpr = 1
        rnumel = 768
        RBLOCK: tl.constexpr = 1024
        xoffset = tl.program_id(0) * XBLOCK
        xindex = tl.full([1], xoffset, tl.int32)
        xmask = xindex < xnumel
        rindex = tl.arange(0, RBLOCK)[:]
        roffset = 0
        rmask = rindex < rnumel
        r1 = rindex
        x0 = xindex
        tmp0 = tl.load(in_ptr0 + (r1 + (768*x0)), rmask & xmask, other=0.0)
        tmp1 = tl.load(in_ptr1 + (r1), rmask, eviction_policy='evict_last', other=0.0)
        tmp3 = tl.load(in_ptr2 + (r1 + (768*x0)), rmask & xmask, other=0.0)
        tmp2 = tmp0 + tmp1
        tmp4 = tmp2 + tmp3
        tmp5 = tl.broadcast_to(tmp4, [RBLOCK])
        tmp7 = tl.where(rmask & xmask, tmp5, 0)
        tmp8 = tl.broadcast_to(tmp5, [RBLOCK])
        tmp10 = tl.where(rmask & xmask, tmp8, 0)
        tmp11 = triton_helpers.promote_to_tensor(tl.sum(tmp10, 0))
        tmp12 = tl.full([1], 768, tl.int32)
        tmp13 = tmp12.to(tl.float32)
        tmp14 = tmp11 / tmp13
        tmp15 = tmp5 - tmp14
        tmp16 = tmp15 * tmp15
        tmp17 = tl.broadcast_to(tmp16, [RBLOCK])
        tmp19 = tl.where(rmask & xmask, tmp17, 0)
        tmp20 = triton_helpers.promote_to_tensor(tl.sum(tmp19, 0))
        tl.store(out_ptr0 + (x0), tmp14, xmask)


buf216: SchedulerNode(ComputedBuffer)
buf216.writes = [MemoryDep('buf216', c0, {c0: 197}, None)]
buf216.unmet_dependencies = 
    [   MemoryDep('buf207', c0, {c0: 151296}, None),
        MemoryDep('buf214', c0, {c0: 151296}, None)]
buf216.met_dependencies = [MemoryDep('arg163_1', c1, {c0: 197, c1: 768}, None)]
buf216.users = [NodeUser(node=SchedulerNode(name='buf218'), can_inplace=False, is_weak=False)]
buf216.group.device = cuda:0
buf216.group.iteration = (197, 768)
buf216.sizes = ([197], [768])
buf207_layout = FixedLayout('cuda', torch.float32, size=[1, 197, 768], stride=[151296, 768, 1])
buf214_layout = FixedLayout('cuda', torch.float32, size=[197, 768], stride=[768, 1])
arg163_1_layout = FixedLayout('cuda', torch.float32, size=[768], stride=[1])
buf216_layout = FixedLayout('cuda', torch.float32, size=[1, 197, 1], stride=[197, 1, 197])
class buf216_loop_body:
    var_ranges = {z0: 197, z1: 768}
    index0 = 768*z0 + z1
    index1 = z1
    index2 = z0
    def body(self, ops):
        get_index = self.get_index('index0')
        load = ops.load('buf214', get_index)
        get_index_1 = self.get_index('index1')
        load_1 = ops.load('arg163_1', get_index_1)
        add = ops.add(load, load_1)
        get_index_2 = self.get_index('index0')
        load_2 = ops.load('buf207', get_index_2)
        add_1 = ops.add(add, load_2)
        reduction = ops.reduction(torch.float32, torch.float32, 'welford_reduce', add_1)
        getitem = reduction[0]
        getitem_1 = reduction[1]
        getitem_2 = reduction[2]
        get_index_3 = self.get_index('index2')
        store_reduction = ops.store_reduction('buf216', get_index_3, getitem_1)
        return store_reduction
buf216 Triton code:
    import triton
    import triton.language as tl
    from triton.compiler.compiler import AttrsDescriptor

    from torch._inductor.runtime import triton_helpers, triton_heuristics
    from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
    from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, instance_descriptor, DeviceProperties

    @triton_heuristics.persistent_reduction(
        size_hints=[256, 1024],
        reduction_hint=ReductionHint.INNER,
        filename=__file__,
        triton_meta={'signature': {0: '*fp32', 1: '*fp32', 2: '*fp32', 3: '*fp32', 4: 'i32', 5: 'i32'}, 'device': DeviceProperties(type='cuda', index=0, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, multi_processor_count=132), 'constants': {}, 'configs': [AttrsDescriptor(divisible_by_16=(0, 1, 2, 3, 5), equal_to_1=())]},
        inductor_meta={'autotune_hints': set(), 'kernel_name': 'Placeholder.DESCRIPTIVE_NAME', 'mutated_arg_names': [], 'no_x_dim': True, 'num_load': 3, 'num_reduction': 3, 'backend_hash': '3944FAAD9D18E987535CF029C4441E186640DD453AE5AE6831EDF736C09E0A47', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': False, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False}
    )
    @triton.jit
    def triton_(in_ptr0, in_ptr1, in_ptr2, out_ptr0, xnumel, rnumel):
        xnumel = 197
        XBLOCK: tl.constexpr = 1
        rnumel = 768
        RBLOCK: tl.constexpr = 1024
        xoffset = tl.program_id(0) * XBLOCK
        xindex = tl.full([1], xoffset, tl.int32)
        xmask = xindex < xnumel
        rindex = tl.arange(0, RBLOCK)[:]
        roffset = 0
        rmask = rindex < rnumel
        r1 = rindex
        x0 = xindex
        tmp0 = tl.load(in_ptr0 + (r1 + (768*x0)), rmask & xmask, other=0.0)
        tmp1 = tl.load(in_ptr1 + (r1), rmask, eviction_policy='evict_last', other=0.0)
        tmp3 = tl.load(in_ptr2 + (r1 + (768*x0)), rmask & xmask, other=0.0)
        tmp2 = tmp0 + tmp1
        tmp4 = tmp2 + tmp3
        tmp5 = tl.broadcast_to(tmp4, [RBLOCK])
        tmp7 = tl.where(rmask & xmask, tmp5, 0)
        tmp8 = tl.broadcast_to(tmp5, [RBLOCK])
        tmp10 = tl.where(rmask & xmask, tmp8, 0)
        tmp11 = triton_helpers.promote_to_tensor(tl.sum(tmp10, 0))
        tmp12 = tl.full([1], 768, tl.int32)
        tmp13 = tmp12.to(tl.float32)
        tmp14 = tmp11 / tmp13
        tmp15 = tmp5 - tmp14
        tmp16 = tmp15 * tmp15
        tmp17 = tl.broadcast_to(tmp16, [RBLOCK])
        tmp19 = tl.where(rmask & xmask, tmp17, 0)
        tmp20 = triton_helpers.promote_to_tensor(tl.sum(tmp19, 0))
        tl.store(out_ptr0 + (x0), tmp20, xmask)


buf218: SchedulerNode(ComputedBuffer)
buf218.writes = [MemoryDep('buf218', c0, {c0: 151296}, None)]
buf218.unmet_dependencies = 
    [   MemoryDep('buf207', c0, {c0: 151296}, None),
        MemoryDep('buf214', c0, {c0: 151296}, None),
        MemoryDep('buf215', c0, {c0: 197}, None),
        MemoryDep('buf216', c0, {c0: 197}, None)]
buf218.met_dependencies = 
    [   MemoryDep('arg163_1', c1, {c0: 197, c1: 768}, None),
        MemoryDep('arg164_1', c1, {c0: 197, c1: 768}, None),
        MemoryDep('arg165_1', c1, {c0: 197, c1: 768}, None)]
buf218.users = [NodeUser(node=ExternKernelSchedulerNode(name='buf219'), can_inplace=False, is_weak=False), NodeUser(node=ExternKernelSchedulerNode(name='buf220'), can_inplace=False, is_weak=False), NodeUser(node=ExternKernelSchedulerNode(name='buf221'), can_inplace=False, is_weak=False)]
buf218.group.device = cuda:0
buf218.group.iteration = (151296, 1)
buf218.sizes = ([197, 768], [])
buf207_layout = FixedLayout('cuda', torch.float32, size=[1, 197, 768], stride=[151296, 768, 1])
arg164_1_layout = FixedLayout('cuda', torch.float32, size=[768], stride=[1])
buf214_layout = FixedLayout('cuda', torch.float32, size=[197, 768], stride=[768, 1])
buf216_layout = FixedLayout('cuda', torch.float32, size=[1, 197, 1], stride=[197, 1, 197])
arg165_1_layout = FixedLayout('cuda', torch.float32, size=[768], stride=[1])
arg163_1_layout = FixedLayout('cuda', torch.float32, size=[768], stride=[1])
buf215_layout = FixedLayout('cuda', torch.float32, size=[1, 197, 1], stride=[197, 1, 197])
buf218_layout = FixedLayout('cuda', torch.float32, size=[1, 197, 768], stride=[151296, 768, 1])
class buf218_loop_body:
    var_ranges = {z0: 197, z1: 768}
    index0 = 768*z0 + z1
    index1 = z1
    index2 = z0
    def body(self, ops):
        get_index = self.get_index('index0')
        load = ops.load('buf214', get_index)
        get_index_1 = self.get_index('index1')
        load_1 = ops.load('arg163_1', get_index_1)
        add = ops.add(load, load_1)
        get_index_2 = self.get_index('index0')
        load_2 = ops.load('buf207', get_index_2)
        add_1 = ops.add(add, load_2)
        get_index_3 = self.get_index('index2')
        load_3 = ops.load('buf215', get_index_3)
        sub = ops.sub(add_1, load_3)
        get_index_4 = self.get_index('index2')
        load_4 = ops.load('buf216', get_index_4)
        constant = ops.constant(768.0, torch.float32)
        truediv = ops.truediv(load_4, constant)
        constant_1 = ops.constant(1e-12, torch.float32)
        add_2 = ops.add(truediv, constant_1)
        rsqrt = ops.rsqrt(add_2)
        mul = ops.mul(sub, rsqrt)
        get_index_5 = self.get_index('index1')
        load_5 = ops.load('arg164_1', get_index_5)
        mul_1 = ops.mul(mul, load_5)
        get_index_6 = self.get_index('index1')
        load_6 = ops.load('arg165_1', get_index_6)
        add_3 = ops.add(mul_1, load_6)
        get_index_7 = self.get_index('index0')
        store = ops.store('buf218', get_index_7, add_3, None)
        return store
buf218 Triton code:
    import triton
    import triton.language as tl
    from triton.compiler.compiler import AttrsDescriptor

    from torch._inductor.runtime import triton_helpers, triton_heuristics
    from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
    from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, instance_descriptor, DeviceProperties

    @triton_heuristics.pointwise(
        size_hints=[262144], 
        filename=__file__,
        triton_meta={'signature': {0: '*fp32', 1: '*fp32', 2: '*fp32', 3: '*fp32', 4: '*fp32', 5: '*fp32', 6: '*fp32', 7: '*fp32', 8: 'i32'}, 'device': DeviceProperties(type='cuda', index=0, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, multi_processor_count=132), 'constants': {}, 'configs': [AttrsDescriptor(divisible_by_16=(0, 1, 2, 3, 4, 5, 6, 7, 8), equal_to_1=())]},
        inductor_meta={'autotune_hints': set(), 'kernel_name': 'Placeholder.DESCRIPTIVE_NAME', 'mutated_arg_names': [], 'no_x_dim': False, 'num_load': 7, 'num_reduction': 0, 'backend_hash': '3944FAAD9D18E987535CF029C4441E186640DD453AE5AE6831EDF736C09E0A47', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': False, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False},
        min_elem_per_thread=0
    )
    @triton.jit
    def triton_(in_ptr0, in_ptr1, in_ptr2, in_ptr3, in_ptr4, in_ptr5, in_ptr6, out_ptr0, xnumel, XBLOCK : tl.constexpr):
        xnumel = 151296
        xoffset = tl.program_id(0) * XBLOCK
        xindex = xoffset + tl.arange(0, XBLOCK)[:]
        xmask = xindex < xnumel
        x2 = xindex
        x0 = xindex % 768
        x1 = (xindex // 768)
        tmp0 = tl.load(in_ptr0 + (x2), xmask)
        tmp1 = tl.load(in_ptr1 + (x0), xmask, eviction_policy='evict_last')
        tmp3 = tl.load(in_ptr2 + (x2), xmask)
        tmp5 = tl.load(in_ptr3 + (x1), xmask, eviction_policy='evict_last')
        tmp7 = tl.load(in_ptr4 + (x1), xmask, eviction_policy='evict_last')
        tmp14 = tl.load(in_ptr5 + (x0), xmask, eviction_policy='evict_last')
        tmp16 = tl.load(in_ptr6 + (x0), xmask, eviction_policy='evict_last')
        tmp2 = tmp0 + tmp1
        tmp4 = tmp2 + tmp3
        tmp6 = tmp4 - tmp5
        tmp8 = 768.0
        tmp9 = tmp7 / tmp8
        tmp10 = 1e-12
        tmp11 = tmp9 + tmp10
        tmp12 = libdevice.rsqrt(tmp11)
        tmp13 = tmp6 * tmp12
        tmp15 = tmp13 * tmp14
        tmp17 = tmp15 + tmp16
        tl.store(out_ptr0 + (x2), tmp17, xmask)


buf219: ExternKernelSchedulerNode(ExternKernelOut)
buf219.writes = [StarDep(name='buf219', mode=None)]
buf219.unmet_dependencies = [StarDep(name='buf218', mode=None)]
buf219.met_dependencies = [StarDep(name='arg166_1', mode=None), StarDep(name='arg167_1', mode=None)]
buf219.users = [NodeUser(node=ExternKernelSchedulerNode(name='buf222'), can_inplace=False, is_weak=False)]
buf219.node.kernel = extern_kernels.addmm


buf220: ExternKernelSchedulerNode(ExternKernelOut)
buf220.writes = [StarDep(name='buf220', mode=None)]
buf220.unmet_dependencies = [StarDep(name='buf218', mode=None)]
buf220.met_dependencies = [StarDep(name='arg168_1', mode=None), StarDep(name='arg169_1', mode=None)]
buf220.users = [NodeUser(node=ExternKernelSchedulerNode(name='buf222'), can_inplace=False, is_weak=False)]
buf220.node.kernel = extern_kernels.addmm


buf221: ExternKernelSchedulerNode(ExternKernelOut)
buf221.writes = [StarDep(name='buf221', mode=None)]
buf221.unmet_dependencies = [StarDep(name='buf218', mode=None)]
buf221.met_dependencies = [StarDep(name='arg170_1', mode=None), StarDep(name='arg171_1', mode=None)]
buf221.users = [NodeUser(node=ExternKernelSchedulerNode(name='buf222'), can_inplace=False, is_weak=False)]
buf221.node.kernel = extern_kernels.addmm


buf222: ExternKernelSchedulerNode(FallbackKernel)
buf222.writes = [StarDep(name='buf222', mode=None)]
buf222.unmet_dependencies = 
    [   StarDep(name='buf219', mode=None),
        StarDep(name='buf220', mode=None),
        StarDep(name='buf221', mode=None)]
buf222.met_dependencies = []
buf222.users = [NodeUser(node=ExternKernelSchedulerNode(name='buf223'), can_inplace=False, is_weak=False), NodeUser(node=ExternKernelSchedulerNode(name='buf224'), can_inplace=False, is_weak=False), NodeUser(node=ExternKernelSchedulerNode(name='buf225'), can_inplace=False, is_weak=False), NodeUser(node=ExternKernelSchedulerNode(name='buf226'), can_inplace=False, is_weak=False)]
buf222.node.kernel = None


buf223: ExternKernelSchedulerNode(MultiOutput)
buf223.writes = [StarDep(name='buf223', mode=None)]
buf223.unmet_dependencies = [StarDep(name='buf222', mode=None)]
buf223.met_dependencies = []
buf223.users = [NodeUser(node=ExternKernelSchedulerNode(name='buf227'), can_inplace=False, is_weak=False)]
buf223.node.kernel = None


buf227: ExternKernelSchedulerNode(ExternKernelOut)
buf227.writes = [StarDep(name='buf227', mode=None)]
buf227.unmet_dependencies = [StarDep(name='buf223', mode=None)]
buf227.met_dependencies = [StarDep(name='arg172_1', mode=None)]
buf227.users = [NodeUser(node=SchedulerNode(name='buf228'), can_inplace=True, is_weak=False)]
buf227.node.kernel = extern_kernels.mm


buf228: SchedulerNode(ComputedBuffer)
buf228.writes = [MemoryDep('buf228', c0, {c0: 151296}, None)]
buf228.unmet_dependencies = 
    [   MemoryDep('buf207', c0, {c0: 151296}, None),
        MemoryDep('buf214', c0, {c0: 151296}, None),
        MemoryDep('buf227', c0, {c0: 151296}, None)]
buf228.met_dependencies = 
    [   MemoryDep('arg163_1', c1, {c0: 197, c1: 768}, None),
        MemoryDep('arg173_1', c1, {c0: 197, c1: 768}, None)]
buf228.users = [NodeUser(node=SchedulerNode(name='buf229'), can_inplace=False, is_weak=False), NodeUser(node=SchedulerNode(name='buf230'), can_inplace=False, is_weak=False), NodeUser(node=SchedulerNode(name='buf231'), can_inplace=False, is_weak=False), NodeUser(node=SchedulerNode(name='buf232'), can_inplace=True, is_weak=False), NodeUser(node=SchedulerNode(name='buf236'), can_inplace=False, is_weak=False), NodeUser(node=SchedulerNode(name='buf237'), can_inplace=False, is_weak=False), NodeUser(node=SchedulerNode(name='buf238'), can_inplace=False, is_weak=False), NodeUser(node=SchedulerNode(name='buf239'), can_inplace=True, is_weak=False), NodeUser(node=SchedulerNode(name='buf249'), can_inplace=True, is_weak=False)]
buf228.group.device = cuda:0
buf228.group.iteration = (151296, 1)
buf228.sizes = ([197, 768], [])
buf207_layout = FixedLayout('cuda', torch.float32, size=[1, 197, 768], stride=[151296, 768, 1])
buf214_layout = FixedLayout('cuda', torch.float32, size=[197, 768], stride=[768, 1])
arg173_1_layout = FixedLayout('cuda', torch.float32, size=[768], stride=[1])
arg163_1_layout = FixedLayout('cuda', torch.float32, size=[768], stride=[1])
buf227_layout = FixedLayout('cuda', torch.float32, size=[197, 768], stride=[768, 1])
buf228_layout = FixedLayout('cuda', torch.float32, size=[1, 197, 768], stride=[151296, 768, 1])
class buf228_loop_body:
    var_ranges = {z0: 197, z1: 768}
    index0 = 768*z0 + z1
    index1 = z1
    def body(self, ops):
        get_index = self.get_index('index0')
        load = ops.load('buf227', get_index)
        get_index_1 = self.get_index('index1')
        load_1 = ops.load('arg173_1', get_index_1)
        add = ops.add(load, load_1)
        get_index_2 = self.get_index('index0')
        load_2 = ops.load('buf214', get_index_2)
        get_index_3 = self.get_index('index1')
        load_3 = ops.load('arg163_1', get_index_3)
        add_1 = ops.add(load_2, load_3)
        get_index_4 = self.get_index('index0')
        load_4 = ops.load('buf207', get_index_4)
        add_2 = ops.add(add_1, load_4)
        add_3 = ops.add(add, add_2)
        get_index_5 = self.get_index('index0')
        store = ops.store('buf228', get_index_5, add_3, None)
        return store
buf228 Triton code:
    import triton
    import triton.language as tl
    from triton.compiler.compiler import AttrsDescriptor

    from torch._inductor.runtime import triton_helpers, triton_heuristics
    from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
    from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, instance_descriptor, DeviceProperties

    @triton_heuristics.pointwise(
        size_hints=[262144], 
        filename=__file__,
        triton_meta={'signature': {0: '*fp32', 1: '*fp32', 2: '*fp32', 3: '*fp32', 4: '*fp32', 5: 'i32'}, 'device': DeviceProperties(type='cuda', index=0, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, multi_processor_count=132), 'constants': {}, 'configs': [AttrsDescriptor(divisible_by_16=(0, 1, 2, 3, 4, 5), equal_to_1=())]},
        inductor_meta={'autotune_hints': set(), 'kernel_name': 'Placeholder.DESCRIPTIVE_NAME', 'mutated_arg_names': ['in_out_ptr0'], 'no_x_dim': False, 'num_load': 5, 'num_reduction': 0, 'backend_hash': '3944FAAD9D18E987535CF029C4441E186640DD453AE5AE6831EDF736C09E0A47', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': False, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False},
        min_elem_per_thread=0
    )
    @triton.jit
    def triton_(in_out_ptr0, in_ptr0, in_ptr1, in_ptr2, in_ptr3, xnumel, XBLOCK : tl.constexpr):
        xnumel = 151296
        xoffset = tl.program_id(0) * XBLOCK
        xindex = xoffset + tl.arange(0, XBLOCK)[:]
        xmask = xindex < xnumel
        x2 = xindex
        x0 = xindex % 768
        tmp0 = tl.load(in_out_ptr0 + (x2), xmask)
        tmp1 = tl.load(in_ptr0 + (x0), xmask, eviction_policy='evict_last')
        tmp3 = tl.load(in_ptr1 + (x2), xmask)
        tmp4 = tl.load(in_ptr2 + (x0), xmask, eviction_policy='evict_last')
        tmp6 = tl.load(in_ptr3 + (x2), xmask)
        tmp2 = tmp0 + tmp1
        tmp5 = tmp3 + tmp4
        tmp7 = tmp5 + tmp6
        tmp8 = tmp2 + tmp7
        tl.store(in_out_ptr0 + (x2), tmp8, xmask)


buf229: SchedulerNode(ComputedBuffer)
buf229.writes = [MemoryDep('buf229', c0, {c0: 197}, None)]
buf229.unmet_dependencies = [MemoryDep('buf228', c0, {c0: 151296}, None)]
buf229.met_dependencies = []
buf229.users = [NodeUser(node=SchedulerNode(name='buf232'), can_inplace=False, is_weak=False)]
buf229.group.device = cuda:0
buf229.group.iteration = (197, 768)
buf229.sizes = ([197], [768])
buf228_layout = FixedLayout('cuda', torch.float32, size=[1, 197, 768], stride=[151296, 768, 1])
buf229_layout = FixedLayout('cuda', torch.float32, size=[1, 197, 1], stride=[197, 1, 197])
class buf229_loop_body:
    var_ranges = {z0: 197, z1: 768}
    index0 = 768*z0 + z1
    index1 = z0
    def body(self, ops):
        get_index = self.get_index('index0')
        load = ops.load('buf228', get_index)
        reduction = ops.reduction(torch.float32, torch.float32, 'welford_reduce', load)
        getitem = reduction[0]
        getitem_1 = reduction[1]
        getitem_2 = reduction[2]
        get_index_1 = self.get_index('index1')
        store_reduction = ops.store_reduction('buf229', get_index_1, getitem)
        return store_reduction
buf229 Triton code:
    import triton
    import triton.language as tl
    from triton.compiler.compiler import AttrsDescriptor

    from torch._inductor.runtime import triton_helpers, triton_heuristics
    from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
    from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, instance_descriptor, DeviceProperties

    @triton_heuristics.persistent_reduction(
        size_hints=[256, 1024],
        reduction_hint=ReductionHint.INNER,
        filename=__file__,
        triton_meta={'signature': {0: '*fp32', 1: '*fp32', 2: 'i32', 3: 'i32'}, 'device': DeviceProperties(type='cuda', index=0, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, multi_processor_count=132), 'constants': {}, 'configs': [AttrsDescriptor(divisible_by_16=(0, 1, 3), equal_to_1=())]},
        inductor_meta={'autotune_hints': set(), 'kernel_name': 'Placeholder.DESCRIPTIVE_NAME', 'mutated_arg_names': [], 'no_x_dim': True, 'num_load': 1, 'num_reduction': 3, 'backend_hash': '3944FAAD9D18E987535CF029C4441E186640DD453AE5AE6831EDF736C09E0A47', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': False, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False}
    )
    @triton.jit
    def triton_(in_ptr0, out_ptr0, xnumel, rnumel):
        xnumel = 197
        XBLOCK: tl.constexpr = 1
        rnumel = 768
        RBLOCK: tl.constexpr = 1024
        xoffset = tl.program_id(0) * XBLOCK
        xindex = tl.full([1], xoffset, tl.int32)
        xmask = xindex < xnumel
        rindex = tl.arange(0, RBLOCK)[:]
        roffset = 0
        rmask = rindex < rnumel
        r1 = rindex
        x0 = xindex
        tmp0 = tl.load(in_ptr0 + (r1 + (768*x0)), rmask & xmask, other=0.0)
        tmp1 = tl.broadcast_to(tmp0, [RBLOCK])
        tmp3 = tl.where(rmask & xmask, tmp1, 0)
        tmp4 = tl.broadcast_to(tmp1, [RBLOCK])
        tmp6 = tl.where(rmask & xmask, tmp4, 0)
        tmp7 = triton_helpers.promote_to_tensor(tl.sum(tmp6, 0))
        tmp8 = tl.full([1], 768, tl.int32)
        tmp9 = tmp8.to(tl.float32)
        tmp10 = tmp7 / tmp9
        tmp11 = tmp1 - tmp10
        tmp12 = tmp11 * tmp11
        tmp13 = tl.broadcast_to(tmp12, [RBLOCK])
        tmp15 = tl.where(rmask & xmask, tmp13, 0)
        tmp16 = triton_helpers.promote_to_tensor(tl.sum(tmp15, 0))
        tl.store(out_ptr0 + (x0), tmp10, xmask)


buf230: SchedulerNode(ComputedBuffer)
buf230.writes = [MemoryDep('buf230', c0, {c0: 197}, None)]
buf230.unmet_dependencies = [MemoryDep('buf228', c0, {c0: 151296}, None)]
buf230.met_dependencies = []
buf230.users = [NodeUser(node=SchedulerNode(name='buf232'), can_inplace=False, is_weak=False)]
buf230.group.device = cuda:0
buf230.group.iteration = (197, 768)
buf230.sizes = ([197], [768])
buf228_layout = FixedLayout('cuda', torch.float32, size=[1, 197, 768], stride=[151296, 768, 1])
buf230_layout = FixedLayout('cuda', torch.float32, size=[1, 197, 1], stride=[197, 1, 197])
class buf230_loop_body:
    var_ranges = {z0: 197, z1: 768}
    index0 = 768*z0 + z1
    index1 = z0
    def body(self, ops):
        get_index = self.get_index('index0')
        load = ops.load('buf228', get_index)
        reduction = ops.reduction(torch.float32, torch.float32, 'welford_reduce', load)
        getitem = reduction[0]
        getitem_1 = reduction[1]
        getitem_2 = reduction[2]
        get_index_1 = self.get_index('index1')
        store_reduction = ops.store_reduction('buf230', get_index_1, getitem_1)
        return store_reduction
buf230 Triton code:
    import triton
    import triton.language as tl
    from triton.compiler.compiler import AttrsDescriptor

    from torch._inductor.runtime import triton_helpers, triton_heuristics
    from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
    from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, instance_descriptor, DeviceProperties

    @triton_heuristics.persistent_reduction(
        size_hints=[256, 1024],
        reduction_hint=ReductionHint.INNER,
        filename=__file__,
        triton_meta={'signature': {0: '*fp32', 1: '*fp32', 2: 'i32', 3: 'i32'}, 'device': DeviceProperties(type='cuda', index=0, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, multi_processor_count=132), 'constants': {}, 'configs': [AttrsDescriptor(divisible_by_16=(0, 1, 3), equal_to_1=())]},
        inductor_meta={'autotune_hints': set(), 'kernel_name': 'Placeholder.DESCRIPTIVE_NAME', 'mutated_arg_names': [], 'no_x_dim': True, 'num_load': 1, 'num_reduction': 3, 'backend_hash': '3944FAAD9D18E987535CF029C4441E186640DD453AE5AE6831EDF736C09E0A47', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': False, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False}
    )
    @triton.jit
    def triton_(in_ptr0, out_ptr0, xnumel, rnumel):
        xnumel = 197
        XBLOCK: tl.constexpr = 1
        rnumel = 768
        RBLOCK: tl.constexpr = 1024
        xoffset = tl.program_id(0) * XBLOCK
        xindex = tl.full([1], xoffset, tl.int32)
        xmask = xindex < xnumel
        rindex = tl.arange(0, RBLOCK)[:]
        roffset = 0
        rmask = rindex < rnumel
        r1 = rindex
        x0 = xindex
        tmp0 = tl.load(in_ptr0 + (r1 + (768*x0)), rmask & xmask, other=0.0)
        tmp1 = tl.broadcast_to(tmp0, [RBLOCK])
        tmp3 = tl.where(rmask & xmask, tmp1, 0)
        tmp4 = tl.broadcast_to(tmp1, [RBLOCK])
        tmp6 = tl.where(rmask & xmask, tmp4, 0)
        tmp7 = triton_helpers.promote_to_tensor(tl.sum(tmp6, 0))
        tmp8 = tl.full([1], 768, tl.int32)
        tmp9 = tmp8.to(tl.float32)
        tmp10 = tmp7 / tmp9
        tmp11 = tmp1 - tmp10
        tmp12 = tmp11 * tmp11
        tmp13 = tl.broadcast_to(tmp12, [RBLOCK])
        tmp15 = tl.where(rmask & xmask, tmp13, 0)
        tmp16 = triton_helpers.promote_to_tensor(tl.sum(tmp15, 0))
        tl.store(out_ptr0 + (x0), tmp16, xmask)


buf232: SchedulerNode(ComputedBuffer)
buf232.writes = [MemoryDep('buf232', c0, {c0: 151296}, None)]
buf232.unmet_dependencies = 
    [   MemoryDep('buf228', c0, {c0: 151296}, None),
        MemoryDep('buf229', c0, {c0: 197}, None),
        MemoryDep('buf230', c0, {c0: 197}, None)]
buf232.met_dependencies = 
    [   MemoryDep('arg174_1', c1, {c0: 197, c1: 768}, None),
        MemoryDep('arg175_1', c1, {c0: 197, c1: 768}, None)]
buf232.users = [NodeUser(node=ExternKernelSchedulerNode(name='buf233'), can_inplace=False, is_weak=False)]
buf232.group.device = cuda:0
buf232.group.iteration = (151296, 1)
buf232.sizes = ([197, 768], [])
buf229_layout = FixedLayout('cuda', torch.float32, size=[1, 197, 1], stride=[197, 1, 197])
buf228_layout = FixedLayout('cuda', torch.float32, size=[1, 197, 768], stride=[151296, 768, 1])
arg174_1_layout = FixedLayout('cuda', torch.float32, size=[768], stride=[1])
arg175_1_layout = FixedLayout('cuda', torch.float32, size=[768], stride=[1])
buf230_layout = FixedLayout('cuda', torch.float32, size=[1, 197, 1], stride=[197, 1, 197])
buf232_layout = FixedLayout('cuda', torch.float32, size=[1, 197, 768], stride=[151296, 768, 1])
class buf232_loop_body:
    var_ranges = {z0: 197, z1: 768}
    index0 = 768*z0 + z1
    index1 = z0
    index2 = z1
    def body(self, ops):
        get_index = self.get_index('index0')
        load = ops.load('buf228', get_index)
        get_index_1 = self.get_index('index1')
        load_1 = ops.load('buf229', get_index_1)
        sub = ops.sub(load, load_1)
        get_index_2 = self.get_index('index1')
        load_2 = ops.load('buf230', get_index_2)
        constant = ops.constant(768.0, torch.float32)
        truediv = ops.truediv(load_2, constant)
        constant_1 = ops.constant(1e-12, torch.float32)
        add = ops.add(truediv, constant_1)
        rsqrt = ops.rsqrt(add)
        mul = ops.mul(sub, rsqrt)
        get_index_3 = self.get_index('index2')
        load_3 = ops.load('arg174_1', get_index_3)
        mul_1 = ops.mul(mul, load_3)
        get_index_4 = self.get_index('index2')
        load_4 = ops.load('arg175_1', get_index_4)
        add_1 = ops.add(mul_1, load_4)
        get_index_5 = self.get_index('index0')
        store = ops.store('buf232', get_index_5, add_1, None)
        return store
buf232 Triton code:
    import triton
    import triton.language as tl
    from triton.compiler.compiler import AttrsDescriptor

    from torch._inductor.runtime import triton_helpers, triton_heuristics
    from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
    from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, instance_descriptor, DeviceProperties

    @triton_heuristics.pointwise(
        size_hints=[262144], 
        filename=__file__,
        triton_meta={'signature': {0: '*fp32', 1: '*fp32', 2: '*fp32', 3: '*fp32', 4: '*fp32', 5: '*fp32', 6: 'i32'}, 'device': DeviceProperties(type='cuda', index=0, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, multi_processor_count=132), 'constants': {}, 'configs': [AttrsDescriptor(divisible_by_16=(0, 1, 2, 3, 4, 5, 6), equal_to_1=())]},
        inductor_meta={'autotune_hints': set(), 'kernel_name': 'Placeholder.DESCRIPTIVE_NAME', 'mutated_arg_names': [], 'no_x_dim': False, 'num_load': 5, 'num_reduction': 0, 'backend_hash': '3944FAAD9D18E987535CF029C4441E186640DD453AE5AE6831EDF736C09E0A47', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': False, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False},
        min_elem_per_thread=0
    )
    @triton.jit
    def triton_(in_ptr0, in_ptr1, in_ptr2, in_ptr3, in_ptr4, out_ptr0, xnumel, XBLOCK : tl.constexpr):
        xnumel = 151296
        xoffset = tl.program_id(0) * XBLOCK
        xindex = xoffset + tl.arange(0, XBLOCK)[:]
        xmask = xindex < xnumel
        x2 = xindex
        x1 = (xindex // 768)
        x0 = xindex % 768
        tmp0 = tl.load(in_ptr0 + (x2), xmask)
        tmp1 = tl.load(in_ptr1 + (x1), xmask, eviction_policy='evict_last')
        tmp3 = tl.load(in_ptr2 + (x1), xmask, eviction_policy='evict_last')
        tmp10 = tl.load(in_ptr3 + (x0), xmask, eviction_policy='evict_last')
        tmp12 = tl.load(in_ptr4 + (x0), xmask, eviction_policy='evict_last')
        tmp2 = tmp0 - tmp1
        tmp4 = 768.0
        tmp5 = tmp3 / tmp4
        tmp6 = 1e-12
        tmp7 = tmp5 + tmp6
        tmp8 = libdevice.rsqrt(tmp7)
        tmp9 = tmp2 * tmp8
        tmp11 = tmp9 * tmp10
        tmp13 = tmp11 + tmp12
        tl.store(out_ptr0 + (x2), tmp13, xmask)


buf233: ExternKernelSchedulerNode(ExternKernelOut)
buf233.writes = [StarDep(name='buf233', mode=None)]
buf233.unmet_dependencies = [StarDep(name='buf232', mode=None)]
buf233.met_dependencies = [StarDep(name='arg176_1', mode=None)]
buf233.users = [NodeUser(node=SchedulerNode(name='buf234'), can_inplace=True, is_weak=False)]
buf233.node.kernel = extern_kernels.mm


buf234: SchedulerNode(ComputedBuffer)
buf234.writes = [MemoryDep('buf234', c0, {c0: 605184}, None)]
buf234.unmet_dependencies = [MemoryDep('buf233', c0, {c0: 605184}, None)]
buf234.met_dependencies = [MemoryDep('arg177_1', c1, {c0: 197, c1: 3072}, None)]
buf234.users = [NodeUser(node=ExternKernelSchedulerNode(name='buf235'), can_inplace=False, is_weak=False)]
buf234.group.device = cuda:0
buf234.group.iteration = (605184, 1)
buf234.sizes = ([197, 3072], [])
arg177_1_layout = FixedLayout('cuda', torch.float32, size=[3072], stride=[1])
buf233_layout = FixedLayout('cuda', torch.float32, size=[197, 3072], stride=[3072, 1])
buf234_layout = FixedLayout('cuda', torch.float32, size=[1, 197, 3072], stride=[605184, 3072, 1])
class buf234_loop_body:
    var_ranges = {z0: 197, z1: 3072}
    index0 = 3072*z0 + z1
    index1 = z1
    def body(self, ops):
        get_index = self.get_index('index0')
        load = ops.load('buf233', get_index)
        get_index_1 = self.get_index('index1')
        load_1 = ops.load('arg177_1', get_index_1)
        add = ops.add(load, load_1)
        constant = ops.constant(0.5, torch.float32)
        mul = ops.mul(add, constant)
        get_index_2 = self.get_index('index0')
        load_2 = ops.load('buf233', get_index_2)
        get_index_3 = self.get_index('index1')
        load_3 = ops.load('arg177_1', get_index_3)
        add_1 = ops.add(load_2, load_3)
        constant_1 = ops.constant(0.7071067811865476, torch.float32)
        mul_1 = ops.mul(add_1, constant_1)
        erf = ops.erf(mul_1)
        constant_2 = ops.constant(1.0, torch.float32)
        add_2 = ops.add(erf, constant_2)
        mul_2 = ops.mul(mul, add_2)
        get_index_4 = self.get_index('index0')
        store = ops.store('buf234', get_index_4, mul_2, None)
        return store
buf234 Triton code:
    import triton
    import triton.language as tl
    from triton.compiler.compiler import AttrsDescriptor

    from torch._inductor.runtime import triton_helpers, triton_heuristics
    from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
    from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, instance_descriptor, DeviceProperties

    @triton_heuristics.pointwise(
        size_hints=[1048576], 
        filename=__file__,
        triton_meta={'signature': {0: '*fp32', 1: '*fp32', 2: 'i32'}, 'device': DeviceProperties(type='cuda', index=0, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, multi_processor_count=132), 'constants': {}, 'configs': [AttrsDescriptor(divisible_by_16=(0, 1, 2), equal_to_1=())]},
        inductor_meta={'autotune_hints': set(), 'kernel_name': 'Placeholder.DESCRIPTIVE_NAME', 'mutated_arg_names': ['in_out_ptr0'], 'no_x_dim': False, 'num_load': 2, 'num_reduction': 0, 'backend_hash': '3944FAAD9D18E987535CF029C4441E186640DD453AE5AE6831EDF736C09E0A47', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': False, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False},
        min_elem_per_thread=0
    )
    @triton.jit
    def triton_(in_out_ptr0, in_ptr0, xnumel, XBLOCK : tl.constexpr):
        xnumel = 605184
        xoffset = tl.program_id(0) * XBLOCK
        xindex = xoffset + tl.arange(0, XBLOCK)[:]
        xmask = xindex < xnumel
        x2 = xindex
        x0 = xindex % 3072
        tmp0 = tl.load(in_out_ptr0 + (x2), xmask)
        tmp1 = tl.load(in_ptr0 + (x0), xmask, eviction_policy='evict_last')
        tmp2 = tmp0 + tmp1
        tmp3 = 0.5
        tmp4 = tmp2 * tmp3
        tmp5 = 0.7071067811865476
        tmp6 = tmp2 * tmp5
        tmp7 = libdevice.erf(tmp6)
        tmp8 = 1.0
        tmp9 = tmp7 + tmp8
        tmp10 = tmp4 * tmp9
        tl.store(in_out_ptr0 + (x2), tmp10, xmask)


buf235: ExternKernelSchedulerNode(ExternKernelOut)
buf235.writes = [StarDep(name='buf235', mode=None)]
buf235.unmet_dependencies = [StarDep(name='buf234', mode=None)]
buf235.met_dependencies = [StarDep(name='arg178_1', mode=None)]
buf235.users = [NodeUser(node=SchedulerNode(name='buf236'), can_inplace=False, is_weak=False), NodeUser(node=SchedulerNode(name='buf237'), can_inplace=False, is_weak=False), NodeUser(node=SchedulerNode(name='buf238'), can_inplace=False, is_weak=False), NodeUser(node=SchedulerNode(name='buf239'), can_inplace=True, is_weak=False), NodeUser(node=SchedulerNode(name='buf249'), can_inplace=True, is_weak=False)]
buf235.node.kernel = extern_kernels.mm


buf236: SchedulerNode(ComputedBuffer)
buf236.writes = [MemoryDep('buf236', c0, {c0: 197}, None)]
buf236.unmet_dependencies = 
    [   MemoryDep('buf228', c0, {c0: 151296}, None),
        MemoryDep('buf235', c0, {c0: 151296}, None)]
buf236.met_dependencies = [MemoryDep('arg179_1', c1, {c0: 197, c1: 768}, None)]
buf236.users = [NodeUser(node=SchedulerNode(name='buf239'), can_inplace=False, is_weak=False)]
buf236.group.device = cuda:0
buf236.group.iteration = (197, 768)
buf236.sizes = ([197], [768])
buf228_layout = FixedLayout('cuda', torch.float32, size=[1, 197, 768], stride=[151296, 768, 1])
arg179_1_layout = FixedLayout('cuda', torch.float32, size=[768], stride=[1])
buf235_layout = FixedLayout('cuda', torch.float32, size=[197, 768], stride=[768, 1])
buf236_layout = FixedLayout('cuda', torch.float32, size=[1, 197, 1], stride=[197, 1, 197])
class buf236_loop_body:
    var_ranges = {z0: 197, z1: 768}
    index0 = 768*z0 + z1
    index1 = z1
    index2 = z0
    def body(self, ops):
        get_index = self.get_index('index0')
        load = ops.load('buf235', get_index)
        get_index_1 = self.get_index('index1')
        load_1 = ops.load('arg179_1', get_index_1)
        add = ops.add(load, load_1)
        get_index_2 = self.get_index('index0')
        load_2 = ops.load('buf228', get_index_2)
        add_1 = ops.add(add, load_2)
        reduction = ops.reduction(torch.float32, torch.float32, 'welford_reduce', add_1)
        getitem = reduction[0]
        getitem_1 = reduction[1]
        getitem_2 = reduction[2]
        get_index_3 = self.get_index('index2')
        store_reduction = ops.store_reduction('buf236', get_index_3, getitem)
        return store_reduction
buf236 Triton code:
    import triton
    import triton.language as tl
    from triton.compiler.compiler import AttrsDescriptor

    from torch._inductor.runtime import triton_helpers, triton_heuristics
    from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
    from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, instance_descriptor, DeviceProperties

    @triton_heuristics.persistent_reduction(
        size_hints=[256, 1024],
        reduction_hint=ReductionHint.INNER,
        filename=__file__,
        triton_meta={'signature': {0: '*fp32', 1: '*fp32', 2: '*fp32', 3: '*fp32', 4: 'i32', 5: 'i32'}, 'device': DeviceProperties(type='cuda', index=0, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, multi_processor_count=132), 'constants': {}, 'configs': [AttrsDescriptor(divisible_by_16=(0, 1, 2, 3, 5), equal_to_1=())]},
        inductor_meta={'autotune_hints': set(), 'kernel_name': 'Placeholder.DESCRIPTIVE_NAME', 'mutated_arg_names': [], 'no_x_dim': True, 'num_load': 3, 'num_reduction': 3, 'backend_hash': '3944FAAD9D18E987535CF029C4441E186640DD453AE5AE6831EDF736C09E0A47', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': False, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False}
    )
    @triton.jit
    def triton_(in_ptr0, in_ptr1, in_ptr2, out_ptr0, xnumel, rnumel):
        xnumel = 197
        XBLOCK: tl.constexpr = 1
        rnumel = 768
        RBLOCK: tl.constexpr = 1024
        xoffset = tl.program_id(0) * XBLOCK
        xindex = tl.full([1], xoffset, tl.int32)
        xmask = xindex < xnumel
        rindex = tl.arange(0, RBLOCK)[:]
        roffset = 0
        rmask = rindex < rnumel
        r1 = rindex
        x0 = xindex
        tmp0 = tl.load(in_ptr0 + (r1 + (768*x0)), rmask & xmask, other=0.0)
        tmp1 = tl.load(in_ptr1 + (r1), rmask, eviction_policy='evict_last', other=0.0)
        tmp3 = tl.load(in_ptr2 + (r1 + (768*x0)), rmask & xmask, other=0.0)
        tmp2 = tmp0 + tmp1
        tmp4 = tmp2 + tmp3
        tmp5 = tl.broadcast_to(tmp4, [RBLOCK])
        tmp7 = tl.where(rmask & xmask, tmp5, 0)
        tmp8 = tl.broadcast_to(tmp5, [RBLOCK])
        tmp10 = tl.where(rmask & xmask, tmp8, 0)
        tmp11 = triton_helpers.promote_to_tensor(tl.sum(tmp10, 0))
        tmp12 = tl.full([1], 768, tl.int32)
        tmp13 = tmp12.to(tl.float32)
        tmp14 = tmp11 / tmp13
        tmp15 = tmp5 - tmp14
        tmp16 = tmp15 * tmp15
        tmp17 = tl.broadcast_to(tmp16, [RBLOCK])
        tmp19 = tl.where(rmask & xmask, tmp17, 0)
        tmp20 = triton_helpers.promote_to_tensor(tl.sum(tmp19, 0))
        tl.store(out_ptr0 + (x0), tmp14, xmask)


buf237: SchedulerNode(ComputedBuffer)
buf237.writes = [MemoryDep('buf237', c0, {c0: 197}, None)]
buf237.unmet_dependencies = 
    [   MemoryDep('buf228', c0, {c0: 151296}, None),
        MemoryDep('buf235', c0, {c0: 151296}, None)]
buf237.met_dependencies = [MemoryDep('arg179_1', c1, {c0: 197, c1: 768}, None)]
buf237.users = [NodeUser(node=SchedulerNode(name='buf239'), can_inplace=False, is_weak=False)]
buf237.group.device = cuda:0
buf237.group.iteration = (197, 768)
buf237.sizes = ([197], [768])
buf228_layout = FixedLayout('cuda', torch.float32, size=[1, 197, 768], stride=[151296, 768, 1])
arg179_1_layout = FixedLayout('cuda', torch.float32, size=[768], stride=[1])
buf235_layout = FixedLayout('cuda', torch.float32, size=[197, 768], stride=[768, 1])
buf237_layout = FixedLayout('cuda', torch.float32, size=[1, 197, 1], stride=[197, 1, 197])
class buf237_loop_body:
    var_ranges = {z0: 197, z1: 768}
    index0 = 768*z0 + z1
    index1 = z1
    index2 = z0
    def body(self, ops):
        get_index = self.get_index('index0')
        load = ops.load('buf235', get_index)
        get_index_1 = self.get_index('index1')
        load_1 = ops.load('arg179_1', get_index_1)
        add = ops.add(load, load_1)
        get_index_2 = self.get_index('index0')
        load_2 = ops.load('buf228', get_index_2)
        add_1 = ops.add(add, load_2)
        reduction = ops.reduction(torch.float32, torch.float32, 'welford_reduce', add_1)
        getitem = reduction[0]
        getitem_1 = reduction[1]
        getitem_2 = reduction[2]
        get_index_3 = self.get_index('index2')
        store_reduction = ops.store_reduction('buf237', get_index_3, getitem_1)
        return store_reduction
buf237 Triton code:
    import triton
    import triton.language as tl
    from triton.compiler.compiler import AttrsDescriptor

    from torch._inductor.runtime import triton_helpers, triton_heuristics
    from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
    from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, instance_descriptor, DeviceProperties

    @triton_heuristics.persistent_reduction(
        size_hints=[256, 1024],
        reduction_hint=ReductionHint.INNER,
        filename=__file__,
        triton_meta={'signature': {0: '*fp32', 1: '*fp32', 2: '*fp32', 3: '*fp32', 4: 'i32', 5: 'i32'}, 'device': DeviceProperties(type='cuda', index=0, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, multi_processor_count=132), 'constants': {}, 'configs': [AttrsDescriptor(divisible_by_16=(0, 1, 2, 3, 5), equal_to_1=())]},
        inductor_meta={'autotune_hints': set(), 'kernel_name': 'Placeholder.DESCRIPTIVE_NAME', 'mutated_arg_names': [], 'no_x_dim': True, 'num_load': 3, 'num_reduction': 3, 'backend_hash': '3944FAAD9D18E987535CF029C4441E186640DD453AE5AE6831EDF736C09E0A47', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': False, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False}
    )
    @triton.jit
    def triton_(in_ptr0, in_ptr1, in_ptr2, out_ptr0, xnumel, rnumel):
        xnumel = 197
        XBLOCK: tl.constexpr = 1
        rnumel = 768
        RBLOCK: tl.constexpr = 1024
        xoffset = tl.program_id(0) * XBLOCK
        xindex = tl.full([1], xoffset, tl.int32)
        xmask = xindex < xnumel
        rindex = tl.arange(0, RBLOCK)[:]
        roffset = 0
        rmask = rindex < rnumel
        r1 = rindex
        x0 = xindex
        tmp0 = tl.load(in_ptr0 + (r1 + (768*x0)), rmask & xmask, other=0.0)
        tmp1 = tl.load(in_ptr1 + (r1), rmask, eviction_policy='evict_last', other=0.0)
        tmp3 = tl.load(in_ptr2 + (r1 + (768*x0)), rmask & xmask, other=0.0)
        tmp2 = tmp0 + tmp1
        tmp4 = tmp2 + tmp3
        tmp5 = tl.broadcast_to(tmp4, [RBLOCK])
        tmp7 = tl.where(rmask & xmask, tmp5, 0)
        tmp8 = tl.broadcast_to(tmp5, [RBLOCK])
        tmp10 = tl.where(rmask & xmask, tmp8, 0)
        tmp11 = triton_helpers.promote_to_tensor(tl.sum(tmp10, 0))
        tmp12 = tl.full([1], 768, tl.int32)
        tmp13 = tmp12.to(tl.float32)
        tmp14 = tmp11 / tmp13
        tmp15 = tmp5 - tmp14
        tmp16 = tmp15 * tmp15
        tmp17 = tl.broadcast_to(tmp16, [RBLOCK])
        tmp19 = tl.where(rmask & xmask, tmp17, 0)
        tmp20 = triton_helpers.promote_to_tensor(tl.sum(tmp19, 0))
        tl.store(out_ptr0 + (x0), tmp20, xmask)


buf239: SchedulerNode(ComputedBuffer)
buf239.writes = [MemoryDep('buf239', c0, {c0: 151296}, None)]
buf239.unmet_dependencies = 
    [   MemoryDep('buf228', c0, {c0: 151296}, None),
        MemoryDep('buf235', c0, {c0: 151296}, None),
        MemoryDep('buf236', c0, {c0: 197}, None),
        MemoryDep('buf237', c0, {c0: 197}, None)]
buf239.met_dependencies = 
    [   MemoryDep('arg179_1', c1, {c0: 197, c1: 768}, None),
        MemoryDep('arg180_1', c1, {c0: 197, c1: 768}, None),
        MemoryDep('arg181_1', c1, {c0: 197, c1: 768}, None)]
buf239.users = [NodeUser(node=ExternKernelSchedulerNode(name='buf240'), can_inplace=False, is_weak=False), NodeUser(node=ExternKernelSchedulerNode(name='buf241'), can_inplace=False, is_weak=False), NodeUser(node=ExternKernelSchedulerNode(name='buf242'), can_inplace=False, is_weak=False)]
buf239.group.device = cuda:0
buf239.group.iteration = (151296, 1)
buf239.sizes = ([197, 768], [])
buf228_layout = FixedLayout('cuda', torch.float32, size=[1, 197, 768], stride=[151296, 768, 1])
arg180_1_layout = FixedLayout('cuda', torch.float32, size=[768], stride=[1])
buf235_layout = FixedLayout('cuda', torch.float32, size=[197, 768], stride=[768, 1])
arg181_1_layout = FixedLayout('cuda', torch.float32, size=[768], stride=[1])
arg179_1_layout = FixedLayout('cuda', torch.float32, size=[768], stride=[1])
buf237_layout = FixedLayout('cuda', torch.float32, size=[1, 197, 1], stride=[197, 1, 197])
buf236_layout = FixedLayout('cuda', torch.float32, size=[1, 197, 1], stride=[197, 1, 197])
buf239_layout = FixedLayout('cuda', torch.float32, size=[1, 197, 768], stride=[151296, 768, 1])
class buf239_loop_body:
    var_ranges = {z0: 197, z1: 768}
    index0 = 768*z0 + z1
    index1 = z1
    index2 = z0
    def body(self, ops):
        get_index = self.get_index('index0')
        load = ops.load('buf235', get_index)
        get_index_1 = self.get_index('index1')
        load_1 = ops.load('arg179_1', get_index_1)
        add = ops.add(load, load_1)
        get_index_2 = self.get_index('index0')
        load_2 = ops.load('buf228', get_index_2)
        add_1 = ops.add(add, load_2)
        get_index_3 = self.get_index('index2')
        load_3 = ops.load('buf236', get_index_3)
        sub = ops.sub(add_1, load_3)
        get_index_4 = self.get_index('index2')
        load_4 = ops.load('buf237', get_index_4)
        constant = ops.constant(768.0, torch.float32)
        truediv = ops.truediv(load_4, constant)
        constant_1 = ops.constant(1e-12, torch.float32)
        add_2 = ops.add(truediv, constant_1)
        rsqrt = ops.rsqrt(add_2)
        mul = ops.mul(sub, rsqrt)
        get_index_5 = self.get_index('index1')
        load_5 = ops.load('arg180_1', get_index_5)
        mul_1 = ops.mul(mul, load_5)
        get_index_6 = self.get_index('index1')
        load_6 = ops.load('arg181_1', get_index_6)
        add_3 = ops.add(mul_1, load_6)
        get_index_7 = self.get_index('index0')
        store = ops.store('buf239', get_index_7, add_3, None)
        return store
buf239 Triton code:
    import triton
    import triton.language as tl
    from triton.compiler.compiler import AttrsDescriptor

    from torch._inductor.runtime import triton_helpers, triton_heuristics
    from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
    from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, instance_descriptor, DeviceProperties

    @triton_heuristics.pointwise(
        size_hints=[262144], 
        filename=__file__,
        triton_meta={'signature': {0: '*fp32', 1: '*fp32', 2: '*fp32', 3: '*fp32', 4: '*fp32', 5: '*fp32', 6: '*fp32', 7: '*fp32', 8: 'i32'}, 'device': DeviceProperties(type='cuda', index=0, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, multi_processor_count=132), 'constants': {}, 'configs': [AttrsDescriptor(divisible_by_16=(0, 1, 2, 3, 4, 5, 6, 7, 8), equal_to_1=())]},
        inductor_meta={'autotune_hints': set(), 'kernel_name': 'Placeholder.DESCRIPTIVE_NAME', 'mutated_arg_names': [], 'no_x_dim': False, 'num_load': 7, 'num_reduction': 0, 'backend_hash': '3944FAAD9D18E987535CF029C4441E186640DD453AE5AE6831EDF736C09E0A47', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': False, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False},
        min_elem_per_thread=0
    )
    @triton.jit
    def triton_(in_ptr0, in_ptr1, in_ptr2, in_ptr3, in_ptr4, in_ptr5, in_ptr6, out_ptr0, xnumel, XBLOCK : tl.constexpr):
        xnumel = 151296
        xoffset = tl.program_id(0) * XBLOCK
        xindex = xoffset + tl.arange(0, XBLOCK)[:]
        xmask = xindex < xnumel
        x2 = xindex
        x0 = xindex % 768
        x1 = (xindex // 768)
        tmp0 = tl.load(in_ptr0 + (x2), xmask)
        tmp1 = tl.load(in_ptr1 + (x0), xmask, eviction_policy='evict_last')
        tmp3 = tl.load(in_ptr2 + (x2), xmask)
        tmp5 = tl.load(in_ptr3 + (x1), xmask, eviction_policy='evict_last')
        tmp7 = tl.load(in_ptr4 + (x1), xmask, eviction_policy='evict_last')
        tmp14 = tl.load(in_ptr5 + (x0), xmask, eviction_policy='evict_last')
        tmp16 = tl.load(in_ptr6 + (x0), xmask, eviction_policy='evict_last')
        tmp2 = tmp0 + tmp1
        tmp4 = tmp2 + tmp3
        tmp6 = tmp4 - tmp5
        tmp8 = 768.0
        tmp9 = tmp7 / tmp8
        tmp10 = 1e-12
        tmp11 = tmp9 + tmp10
        tmp12 = libdevice.rsqrt(tmp11)
        tmp13 = tmp6 * tmp12
        tmp15 = tmp13 * tmp14
        tmp17 = tmp15 + tmp16
        tl.store(out_ptr0 + (x2), tmp17, xmask)


buf240: ExternKernelSchedulerNode(ExternKernelOut)
buf240.writes = [StarDep(name='buf240', mode=None)]
buf240.unmet_dependencies = [StarDep(name='buf239', mode=None)]
buf240.met_dependencies = [StarDep(name='arg182_1', mode=None), StarDep(name='arg183_1', mode=None)]
buf240.users = [NodeUser(node=ExternKernelSchedulerNode(name='buf243'), can_inplace=False, is_weak=False)]
buf240.node.kernel = extern_kernels.addmm


buf241: ExternKernelSchedulerNode(ExternKernelOut)
buf241.writes = [StarDep(name='buf241', mode=None)]
buf241.unmet_dependencies = [StarDep(name='buf239', mode=None)]
buf241.met_dependencies = [StarDep(name='arg184_1', mode=None), StarDep(name='arg185_1', mode=None)]
buf241.users = [NodeUser(node=ExternKernelSchedulerNode(name='buf243'), can_inplace=False, is_weak=False)]
buf241.node.kernel = extern_kernels.addmm


buf242: ExternKernelSchedulerNode(ExternKernelOut)
buf242.writes = [StarDep(name='buf242', mode=None)]
buf242.unmet_dependencies = [StarDep(name='buf239', mode=None)]
buf242.met_dependencies = [StarDep(name='arg186_1', mode=None), StarDep(name='arg187_1', mode=None)]
buf242.users = [NodeUser(node=ExternKernelSchedulerNode(name='buf243'), can_inplace=False, is_weak=False)]
buf242.node.kernel = extern_kernels.addmm


buf243: ExternKernelSchedulerNode(FallbackKernel)
buf243.writes = [StarDep(name='buf243', mode=None)]
buf243.unmet_dependencies = 
    [   StarDep(name='buf240', mode=None),
        StarDep(name='buf241', mode=None),
        StarDep(name='buf242', mode=None)]
buf243.met_dependencies = []
buf243.users = [NodeUser(node=ExternKernelSchedulerNode(name='buf244'), can_inplace=False, is_weak=False), NodeUser(node=ExternKernelSchedulerNode(name='buf245'), can_inplace=False, is_weak=False), NodeUser(node=ExternKernelSchedulerNode(name='buf246'), can_inplace=False, is_weak=False), NodeUser(node=ExternKernelSchedulerNode(name='buf247'), can_inplace=False, is_weak=False)]
buf243.node.kernel = None


buf244: ExternKernelSchedulerNode(MultiOutput)
buf244.writes = [StarDep(name='buf244', mode=None)]
buf244.unmet_dependencies = [StarDep(name='buf243', mode=None)]
buf244.met_dependencies = []
buf244.users = [NodeUser(node=ExternKernelSchedulerNode(name='buf248'), can_inplace=False, is_weak=False)]
buf244.node.kernel = None


buf248: ExternKernelSchedulerNode(ExternKernelOut)
buf248.writes = [StarDep(name='buf248', mode=None)]
buf248.unmet_dependencies = [StarDep(name='buf244', mode=None)]
buf248.met_dependencies = [StarDep(name='arg188_1', mode=None)]
buf248.users = [NodeUser(node=SchedulerNode(name='buf249'), can_inplace=True, is_weak=False)]
buf248.node.kernel = extern_kernels.mm


buf249: SchedulerNode(ComputedBuffer)
buf249.writes = [MemoryDep('buf249', c0, {c0: 151296}, None)]
buf249.unmet_dependencies = 
    [   MemoryDep('buf228', c0, {c0: 151296}, None),
        MemoryDep('buf235', c0, {c0: 151296}, None),
        MemoryDep('buf248', c0, {c0: 151296}, None)]
buf249.met_dependencies = 
    [   MemoryDep('arg179_1', c1, {c0: 197, c1: 768}, None),
        MemoryDep('arg189_1', c1, {c0: 197, c1: 768}, None)]
buf249.users = [NodeUser(node=SchedulerNode(name='buf250'), can_inplace=False, is_weak=False), NodeUser(node=SchedulerNode(name='buf251'), can_inplace=False, is_weak=False), NodeUser(node=SchedulerNode(name='buf252'), can_inplace=False, is_weak=False), NodeUser(node=SchedulerNode(name='buf253'), can_inplace=True, is_weak=False), NodeUser(node=SchedulerNode(name='buf257'), can_inplace=True, is_weak=False)]
buf249.group.device = cuda:0
buf249.group.iteration = (151296, 1)
buf249.sizes = ([197, 768], [])
buf228_layout = FixedLayout('cuda', torch.float32, size=[1, 197, 768], stride=[151296, 768, 1])
buf235_layout = FixedLayout('cuda', torch.float32, size=[197, 768], stride=[768, 1])
buf248_layout = FixedLayout('cuda', torch.float32, size=[197, 768], stride=[768, 1])
arg179_1_layout = FixedLayout('cuda', torch.float32, size=[768], stride=[1])
arg189_1_layout = FixedLayout('cuda', torch.float32, size=[768], stride=[1])
buf249_layout = FixedLayout('cuda', torch.float32, size=[1, 197, 768], stride=[151296, 768, 1])
class buf249_loop_body:
    var_ranges = {z0: 197, z1: 768}
    index0 = 768*z0 + z1
    index1 = z1
    def body(self, ops):
        get_index = self.get_index('index0')
        load = ops.load('buf248', get_index)
        get_index_1 = self.get_index('index1')
        load_1 = ops.load('arg189_1', get_index_1)
        add = ops.add(load, load_1)
        get_index_2 = self.get_index('index0')
        load_2 = ops.load('buf235', get_index_2)
        get_index_3 = self.get_index('index1')
        load_3 = ops.load('arg179_1', get_index_3)
        add_1 = ops.add(load_2, load_3)
        get_index_4 = self.get_index('index0')
        load_4 = ops.load('buf228', get_index_4)
        add_2 = ops.add(add_1, load_4)
        add_3 = ops.add(add, add_2)
        get_index_5 = self.get_index('index0')
        store = ops.store('buf249', get_index_5, add_3, None)
        return store
buf249 Triton code:
    import triton
    import triton.language as tl
    from triton.compiler.compiler import AttrsDescriptor

    from torch._inductor.runtime import triton_helpers, triton_heuristics
    from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
    from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, instance_descriptor, DeviceProperties

    @triton_heuristics.pointwise(
        size_hints=[262144], 
        filename=__file__,
        triton_meta={'signature': {0: '*fp32', 1: '*fp32', 2: '*fp32', 3: '*fp32', 4: '*fp32', 5: 'i32'}, 'device': DeviceProperties(type='cuda', index=0, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, multi_processor_count=132), 'constants': {}, 'configs': [AttrsDescriptor(divisible_by_16=(0, 1, 2, 3, 4, 5), equal_to_1=())]},
        inductor_meta={'autotune_hints': set(), 'kernel_name': 'Placeholder.DESCRIPTIVE_NAME', 'mutated_arg_names': ['in_out_ptr0'], 'no_x_dim': False, 'num_load': 5, 'num_reduction': 0, 'backend_hash': '3944FAAD9D18E987535CF029C4441E186640DD453AE5AE6831EDF736C09E0A47', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': False, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False},
        min_elem_per_thread=0
    )
    @triton.jit
    def triton_(in_out_ptr0, in_ptr0, in_ptr1, in_ptr2, in_ptr3, xnumel, XBLOCK : tl.constexpr):
        xnumel = 151296
        xoffset = tl.program_id(0) * XBLOCK
        xindex = xoffset + tl.arange(0, XBLOCK)[:]
        xmask = xindex < xnumel
        x2 = xindex
        x0 = xindex % 768
        tmp0 = tl.load(in_out_ptr0 + (x2), xmask)
        tmp1 = tl.load(in_ptr0 + (x0), xmask, eviction_policy='evict_last')
        tmp3 = tl.load(in_ptr1 + (x2), xmask)
        tmp4 = tl.load(in_ptr2 + (x0), xmask, eviction_policy='evict_last')
        tmp6 = tl.load(in_ptr3 + (x2), xmask)
        tmp2 = tmp0 + tmp1
        tmp5 = tmp3 + tmp4
        tmp7 = tmp5 + tmp6
        tmp8 = tmp2 + tmp7
        tl.store(in_out_ptr0 + (x2), tmp8, xmask)


buf250: SchedulerNode(ComputedBuffer)
buf250.writes = [MemoryDep('buf250', c0, {c0: 197}, None)]
buf250.unmet_dependencies = [MemoryDep('buf249', c0, {c0: 151296}, None)]
buf250.met_dependencies = []
buf250.users = [NodeUser(node=SchedulerNode(name='buf253'), can_inplace=False, is_weak=False)]
buf250.group.device = cuda:0
buf250.group.iteration = (197, 768)
buf250.sizes = ([197], [768])
buf249_layout = FixedLayout('cuda', torch.float32, size=[1, 197, 768], stride=[151296, 768, 1])
buf250_layout = FixedLayout('cuda', torch.float32, size=[1, 197, 1], stride=[197, 1, 197])
class buf250_loop_body:
    var_ranges = {z0: 197, z1: 768}
    index0 = 768*z0 + z1
    index1 = z0
    def body(self, ops):
        get_index = self.get_index('index0')
        load = ops.load('buf249', get_index)
        reduction = ops.reduction(torch.float32, torch.float32, 'welford_reduce', load)
        getitem = reduction[0]
        getitem_1 = reduction[1]
        getitem_2 = reduction[2]
        get_index_1 = self.get_index('index1')
        store_reduction = ops.store_reduction('buf250', get_index_1, getitem)
        return store_reduction
buf250 Triton code:
    import triton
    import triton.language as tl
    from triton.compiler.compiler import AttrsDescriptor

    from torch._inductor.runtime import triton_helpers, triton_heuristics
    from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
    from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, instance_descriptor, DeviceProperties

    @triton_heuristics.persistent_reduction(
        size_hints=[256, 1024],
        reduction_hint=ReductionHint.INNER,
        filename=__file__,
        triton_meta={'signature': {0: '*fp32', 1: '*fp32', 2: 'i32', 3: 'i32'}, 'device': DeviceProperties(type='cuda', index=0, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, multi_processor_count=132), 'constants': {}, 'configs': [AttrsDescriptor(divisible_by_16=(0, 1, 3), equal_to_1=())]},
        inductor_meta={'autotune_hints': set(), 'kernel_name': 'Placeholder.DESCRIPTIVE_NAME', 'mutated_arg_names': [], 'no_x_dim': True, 'num_load': 1, 'num_reduction': 3, 'backend_hash': '3944FAAD9D18E987535CF029C4441E186640DD453AE5AE6831EDF736C09E0A47', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': False, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False}
    )
    @triton.jit
    def triton_(in_ptr0, out_ptr0, xnumel, rnumel):
        xnumel = 197
        XBLOCK: tl.constexpr = 1
        rnumel = 768
        RBLOCK: tl.constexpr = 1024
        xoffset = tl.program_id(0) * XBLOCK
        xindex = tl.full([1], xoffset, tl.int32)
        xmask = xindex < xnumel
        rindex = tl.arange(0, RBLOCK)[:]
        roffset = 0
        rmask = rindex < rnumel
        r1 = rindex
        x0 = xindex
        tmp0 = tl.load(in_ptr0 + (r1 + (768*x0)), rmask & xmask, other=0.0)
        tmp1 = tl.broadcast_to(tmp0, [RBLOCK])
        tmp3 = tl.where(rmask & xmask, tmp1, 0)
        tmp4 = tl.broadcast_to(tmp1, [RBLOCK])
        tmp6 = tl.where(rmask & xmask, tmp4, 0)
        tmp7 = triton_helpers.promote_to_tensor(tl.sum(tmp6, 0))
        tmp8 = tl.full([1], 768, tl.int32)
        tmp9 = tmp8.to(tl.float32)
        tmp10 = tmp7 / tmp9
        tmp11 = tmp1 - tmp10
        tmp12 = tmp11 * tmp11
        tmp13 = tl.broadcast_to(tmp12, [RBLOCK])
        tmp15 = tl.where(rmask & xmask, tmp13, 0)
        tmp16 = triton_helpers.promote_to_tensor(tl.sum(tmp15, 0))
        tl.store(out_ptr0 + (x0), tmp10, xmask)


buf251: SchedulerNode(ComputedBuffer)
buf251.writes = [MemoryDep('buf251', c0, {c0: 197}, None)]
buf251.unmet_dependencies = [MemoryDep('buf249', c0, {c0: 151296}, None)]
buf251.met_dependencies = []
buf251.users = [NodeUser(node=SchedulerNode(name='buf253'), can_inplace=False, is_weak=False)]
buf251.group.device = cuda:0
buf251.group.iteration = (197, 768)
buf251.sizes = ([197], [768])
buf249_layout = FixedLayout('cuda', torch.float32, size=[1, 197, 768], stride=[151296, 768, 1])
buf251_layout = FixedLayout('cuda', torch.float32, size=[1, 197, 1], stride=[197, 1, 197])
class buf251_loop_body:
    var_ranges = {z0: 197, z1: 768}
    index0 = 768*z0 + z1
    index1 = z0
    def body(self, ops):
        get_index = self.get_index('index0')
        load = ops.load('buf249', get_index)
        reduction = ops.reduction(torch.float32, torch.float32, 'welford_reduce', load)
        getitem = reduction[0]
        getitem_1 = reduction[1]
        getitem_2 = reduction[2]
        get_index_1 = self.get_index('index1')
        store_reduction = ops.store_reduction('buf251', get_index_1, getitem_1)
        return store_reduction
buf251 Triton code:
    import triton
    import triton.language as tl
    from triton.compiler.compiler import AttrsDescriptor

    from torch._inductor.runtime import triton_helpers, triton_heuristics
    from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
    from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, instance_descriptor, DeviceProperties

    @triton_heuristics.persistent_reduction(
        size_hints=[256, 1024],
        reduction_hint=ReductionHint.INNER,
        filename=__file__,
        triton_meta={'signature': {0: '*fp32', 1: '*fp32', 2: 'i32', 3: 'i32'}, 'device': DeviceProperties(type='cuda', index=0, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, multi_processor_count=132), 'constants': {}, 'configs': [AttrsDescriptor(divisible_by_16=(0, 1, 3), equal_to_1=())]},
        inductor_meta={'autotune_hints': set(), 'kernel_name': 'Placeholder.DESCRIPTIVE_NAME', 'mutated_arg_names': [], 'no_x_dim': True, 'num_load': 1, 'num_reduction': 3, 'backend_hash': '3944FAAD9D18E987535CF029C4441E186640DD453AE5AE6831EDF736C09E0A47', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': False, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False}
    )
    @triton.jit
    def triton_(in_ptr0, out_ptr0, xnumel, rnumel):
        xnumel = 197
        XBLOCK: tl.constexpr = 1
        rnumel = 768
        RBLOCK: tl.constexpr = 1024
        xoffset = tl.program_id(0) * XBLOCK
        xindex = tl.full([1], xoffset, tl.int32)
        xmask = xindex < xnumel
        rindex = tl.arange(0, RBLOCK)[:]
        roffset = 0
        rmask = rindex < rnumel
        r1 = rindex
        x0 = xindex
        tmp0 = tl.load(in_ptr0 + (r1 + (768*x0)), rmask & xmask, other=0.0)
        tmp1 = tl.broadcast_to(tmp0, [RBLOCK])
        tmp3 = tl.where(rmask & xmask, tmp1, 0)
        tmp4 = tl.broadcast_to(tmp1, [RBLOCK])
        tmp6 = tl.where(rmask & xmask, tmp4, 0)
        tmp7 = triton_helpers.promote_to_tensor(tl.sum(tmp6, 0))
        tmp8 = tl.full([1], 768, tl.int32)
        tmp9 = tmp8.to(tl.float32)
        tmp10 = tmp7 / tmp9
        tmp11 = tmp1 - tmp10
        tmp12 = tmp11 * tmp11
        tmp13 = tl.broadcast_to(tmp12, [RBLOCK])
        tmp15 = tl.where(rmask & xmask, tmp13, 0)
        tmp16 = triton_helpers.promote_to_tensor(tl.sum(tmp15, 0))
        tl.store(out_ptr0 + (x0), tmp16, xmask)


buf253: SchedulerNode(ComputedBuffer)
buf253.writes = [MemoryDep('buf253', c0, {c0: 151296}, None)]
buf253.unmet_dependencies = 
    [   MemoryDep('buf249', c0, {c0: 151296}, None),
        MemoryDep('buf250', c0, {c0: 197}, None),
        MemoryDep('buf251', c0, {c0: 197}, None)]
buf253.met_dependencies = 
    [   MemoryDep('arg190_1', c1, {c0: 197, c1: 768}, None),
        MemoryDep('arg191_1', c1, {c0: 197, c1: 768}, None)]
buf253.users = [NodeUser(node=ExternKernelSchedulerNode(name='buf254'), can_inplace=False, is_weak=False)]
buf253.group.device = cuda:0
buf253.group.iteration = (151296, 1)
buf253.sizes = ([197, 768], [])
arg191_1_layout = FixedLayout('cuda', torch.float32, size=[768], stride=[1])
arg190_1_layout = FixedLayout('cuda', torch.float32, size=[768], stride=[1])
buf251_layout = FixedLayout('cuda', torch.float32, size=[1, 197, 1], stride=[197, 1, 197])
buf249_layout = FixedLayout('cuda', torch.float32, size=[1, 197, 768], stride=[151296, 768, 1])
buf250_layout = FixedLayout('cuda', torch.float32, size=[1, 197, 1], stride=[197, 1, 197])
buf253_layout = FixedLayout('cuda', torch.float32, size=[1, 197, 768], stride=[151296, 768, 1])
class buf253_loop_body:
    var_ranges = {z0: 197, z1: 768}
    index0 = 768*z0 + z1
    index1 = z0
    index2 = z1
    def body(self, ops):
        get_index = self.get_index('index0')
        load = ops.load('buf249', get_index)
        get_index_1 = self.get_index('index1')
        load_1 = ops.load('buf250', get_index_1)
        sub = ops.sub(load, load_1)
        get_index_2 = self.get_index('index1')
        load_2 = ops.load('buf251', get_index_2)
        constant = ops.constant(768.0, torch.float32)
        truediv = ops.truediv(load_2, constant)
        constant_1 = ops.constant(1e-12, torch.float32)
        add = ops.add(truediv, constant_1)
        rsqrt = ops.rsqrt(add)
        mul = ops.mul(sub, rsqrt)
        get_index_3 = self.get_index('index2')
        load_3 = ops.load('arg190_1', get_index_3)
        mul_1 = ops.mul(mul, load_3)
        get_index_4 = self.get_index('index2')
        load_4 = ops.load('arg191_1', get_index_4)
        add_1 = ops.add(mul_1, load_4)
        get_index_5 = self.get_index('index0')
        store = ops.store('buf253', get_index_5, add_1, None)
        return store
buf253 Triton code:
    import triton
    import triton.language as tl
    from triton.compiler.compiler import AttrsDescriptor

    from torch._inductor.runtime import triton_helpers, triton_heuristics
    from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
    from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, instance_descriptor, DeviceProperties

    @triton_heuristics.pointwise(
        size_hints=[262144], 
        filename=__file__,
        triton_meta={'signature': {0: '*fp32', 1: '*fp32', 2: '*fp32', 3: '*fp32', 4: '*fp32', 5: '*fp32', 6: 'i32'}, 'device': DeviceProperties(type='cuda', index=0, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, multi_processor_count=132), 'constants': {}, 'configs': [AttrsDescriptor(divisible_by_16=(0, 1, 2, 3, 4, 5, 6), equal_to_1=())]},
        inductor_meta={'autotune_hints': set(), 'kernel_name': 'Placeholder.DESCRIPTIVE_NAME', 'mutated_arg_names': [], 'no_x_dim': False, 'num_load': 5, 'num_reduction': 0, 'backend_hash': '3944FAAD9D18E987535CF029C4441E186640DD453AE5AE6831EDF736C09E0A47', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': False, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False},
        min_elem_per_thread=0
    )
    @triton.jit
    def triton_(in_ptr0, in_ptr1, in_ptr2, in_ptr3, in_ptr4, out_ptr0, xnumel, XBLOCK : tl.constexpr):
        xnumel = 151296
        xoffset = tl.program_id(0) * XBLOCK
        xindex = xoffset + tl.arange(0, XBLOCK)[:]
        xmask = xindex < xnumel
        x2 = xindex
        x1 = (xindex // 768)
        x0 = xindex % 768
        tmp0 = tl.load(in_ptr0 + (x2), xmask)
        tmp1 = tl.load(in_ptr1 + (x1), xmask, eviction_policy='evict_last')
        tmp3 = tl.load(in_ptr2 + (x1), xmask, eviction_policy='evict_last')
        tmp10 = tl.load(in_ptr3 + (x0), xmask, eviction_policy='evict_last')
        tmp12 = tl.load(in_ptr4 + (x0), xmask, eviction_policy='evict_last')
        tmp2 = tmp0 - tmp1
        tmp4 = 768.0
        tmp5 = tmp3 / tmp4
        tmp6 = 1e-12
        tmp7 = tmp5 + tmp6
        tmp8 = libdevice.rsqrt(tmp7)
        tmp9 = tmp2 * tmp8
        tmp11 = tmp9 * tmp10
        tmp13 = tmp11 + tmp12
        tl.store(out_ptr0 + (x2), tmp13, xmask)


buf254: ExternKernelSchedulerNode(ExternKernelOut)
buf254.writes = [StarDep(name='buf254', mode=None)]
buf254.unmet_dependencies = [StarDep(name='buf253', mode=None)]
buf254.met_dependencies = [StarDep(name='arg192_1', mode=None)]
buf254.users = [NodeUser(node=SchedulerNode(name='buf255'), can_inplace=True, is_weak=False)]
buf254.node.kernel = extern_kernels.mm


buf255: SchedulerNode(ComputedBuffer)
buf255.writes = [MemoryDep('buf255', c0, {c0: 605184}, None)]
buf255.unmet_dependencies = [MemoryDep('buf254', c0, {c0: 605184}, None)]
buf255.met_dependencies = [MemoryDep('arg193_1', c1, {c0: 197, c1: 3072}, None)]
buf255.users = [NodeUser(node=ExternKernelSchedulerNode(name='buf256'), can_inplace=False, is_weak=False)]
buf255.group.device = cuda:0
buf255.group.iteration = (605184, 1)
buf255.sizes = ([197, 3072], [])
arg193_1_layout = FixedLayout('cuda', torch.float32, size=[3072], stride=[1])
buf254_layout = FixedLayout('cuda', torch.float32, size=[197, 3072], stride=[3072, 1])
buf255_layout = FixedLayout('cuda', torch.float32, size=[1, 197, 3072], stride=[605184, 3072, 1])
class buf255_loop_body:
    var_ranges = {z0: 197, z1: 3072}
    index0 = 3072*z0 + z1
    index1 = z1
    def body(self, ops):
        get_index = self.get_index('index0')
        load = ops.load('buf254', get_index)
        get_index_1 = self.get_index('index1')
        load_1 = ops.load('arg193_1', get_index_1)
        add = ops.add(load, load_1)
        constant = ops.constant(0.5, torch.float32)
        mul = ops.mul(add, constant)
        get_index_2 = self.get_index('index0')
        load_2 = ops.load('buf254', get_index_2)
        get_index_3 = self.get_index('index1')
        load_3 = ops.load('arg193_1', get_index_3)
        add_1 = ops.add(load_2, load_3)
        constant_1 = ops.constant(0.7071067811865476, torch.float32)
        mul_1 = ops.mul(add_1, constant_1)
        erf = ops.erf(mul_1)
        constant_2 = ops.constant(1.0, torch.float32)
        add_2 = ops.add(erf, constant_2)
        mul_2 = ops.mul(mul, add_2)
        get_index_4 = self.get_index('index0')
        store = ops.store('buf255', get_index_4, mul_2, None)
        return store
buf255 Triton code:
    import triton
    import triton.language as tl
    from triton.compiler.compiler import AttrsDescriptor

    from torch._inductor.runtime import triton_helpers, triton_heuristics
    from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
    from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, instance_descriptor, DeviceProperties

    @triton_heuristics.pointwise(
        size_hints=[1048576], 
        filename=__file__,
        triton_meta={'signature': {0: '*fp32', 1: '*fp32', 2: 'i32'}, 'device': DeviceProperties(type='cuda', index=0, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, multi_processor_count=132), 'constants': {}, 'configs': [AttrsDescriptor(divisible_by_16=(0, 1, 2), equal_to_1=())]},
        inductor_meta={'autotune_hints': set(), 'kernel_name': 'Placeholder.DESCRIPTIVE_NAME', 'mutated_arg_names': ['in_out_ptr0'], 'no_x_dim': False, 'num_load': 2, 'num_reduction': 0, 'backend_hash': '3944FAAD9D18E987535CF029C4441E186640DD453AE5AE6831EDF736C09E0A47', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': False, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False},
        min_elem_per_thread=0
    )
    @triton.jit
    def triton_(in_out_ptr0, in_ptr0, xnumel, XBLOCK : tl.constexpr):
        xnumel = 605184
        xoffset = tl.program_id(0) * XBLOCK
        xindex = xoffset + tl.arange(0, XBLOCK)[:]
        xmask = xindex < xnumel
        x2 = xindex
        x0 = xindex % 3072
        tmp0 = tl.load(in_out_ptr0 + (x2), xmask)
        tmp1 = tl.load(in_ptr0 + (x0), xmask, eviction_policy='evict_last')
        tmp2 = tmp0 + tmp1
        tmp3 = 0.5
        tmp4 = tmp2 * tmp3
        tmp5 = 0.7071067811865476
        tmp6 = tmp2 * tmp5
        tmp7 = libdevice.erf(tmp6)
        tmp8 = 1.0
        tmp9 = tmp7 + tmp8
        tmp10 = tmp4 * tmp9
        tl.store(in_out_ptr0 + (x2), tmp10, xmask)


buf256: ExternKernelSchedulerNode(ExternKernelOut)
buf256.writes = [StarDep(name='buf256', mode=None)]
buf256.unmet_dependencies = [StarDep(name='buf255', mode=None)]
buf256.met_dependencies = [StarDep(name='arg194_1', mode=None)]
buf256.users = [NodeUser(node=SchedulerNode(name='buf257'), can_inplace=True, is_weak=False)]
buf256.node.kernel = extern_kernels.mm


buf257: SchedulerNode(ComputedBuffer)
buf257.writes = [MemoryDep('buf257', c0, {c0: 151296}, None)]
buf257.unmet_dependencies = 
    [   MemoryDep('buf249', c0, {c0: 151296}, None),
        MemoryDep('buf256', c0, {c0: 151296}, None)]
buf257.met_dependencies = [MemoryDep('arg195_1', c1, {c0: 197, c1: 768}, None)]
buf257.users = [NodeUser(node=SchedulerNode(name='buf258'), can_inplace=False, is_weak=False), NodeUser(node=SchedulerNode(name='buf259'), can_inplace=False, is_weak=False), NodeUser(node=SchedulerNode(name='buf260'), can_inplace=False, is_weak=False), NodeUser(node=SchedulerNode(name='buf261'), can_inplace=True, is_weak=False), NodeUser(node=OUTPUT, can_inplace=False, is_weak=False)]
buf257.group.device = cuda:0
buf257.group.iteration = (151296, 1)
buf257.sizes = ([197, 768], [])
arg195_1_layout = FixedLayout('cuda', torch.float32, size=[768], stride=[1])
buf256_layout = FixedLayout('cuda', torch.float32, size=[197, 768], stride=[768, 1])
buf249_layout = FixedLayout('cuda', torch.float32, size=[1, 197, 768], stride=[151296, 768, 1])
buf257_layout = FixedLayout('cuda', torch.float32, size=[1, 197, 768], stride=[151296, 768, 1])
class buf257_loop_body:
    var_ranges = {z0: 197, z1: 768}
    index0 = 768*z0 + z1
    index1 = z1
    def body(self, ops):
        get_index = self.get_index('index0')
        load = ops.load('buf256', get_index)
        get_index_1 = self.get_index('index1')
        load_1 = ops.load('arg195_1', get_index_1)
        add = ops.add(load, load_1)
        get_index_2 = self.get_index('index0')
        load_2 = ops.load('buf249', get_index_2)
        add_1 = ops.add(add, load_2)
        get_index_3 = self.get_index('index0')
        store = ops.store('buf257', get_index_3, add_1, None)
        return store
buf257 Triton code:
    import triton
    import triton.language as tl
    from triton.compiler.compiler import AttrsDescriptor

    from torch._inductor.runtime import triton_helpers, triton_heuristics
    from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
    from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, instance_descriptor, DeviceProperties

    @triton_heuristics.pointwise(
        size_hints=[262144], 
        filename=__file__,
        triton_meta={'signature': {0: '*fp32', 1: '*fp32', 2: '*fp32', 3: 'i32'}, 'device': DeviceProperties(type='cuda', index=0, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, multi_processor_count=132), 'constants': {}, 'configs': [AttrsDescriptor(divisible_by_16=(0, 1, 2, 3), equal_to_1=())]},
        inductor_meta={'autotune_hints': set(), 'kernel_name': 'Placeholder.DESCRIPTIVE_NAME', 'mutated_arg_names': ['in_out_ptr0'], 'no_x_dim': False, 'num_load': 3, 'num_reduction': 0, 'backend_hash': '3944FAAD9D18E987535CF029C4441E186640DD453AE5AE6831EDF736C09E0A47', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': False, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False},
        min_elem_per_thread=0
    )
    @triton.jit
    def triton_(in_out_ptr0, in_ptr0, in_ptr1, xnumel, XBLOCK : tl.constexpr):
        xnumel = 151296
        xoffset = tl.program_id(0) * XBLOCK
        xindex = xoffset + tl.arange(0, XBLOCK)[:]
        xmask = xindex < xnumel
        x2 = xindex
        x0 = xindex % 768
        tmp0 = tl.load(in_out_ptr0 + (x2), xmask)
        tmp1 = tl.load(in_ptr0 + (x0), xmask, eviction_policy='evict_last')
        tmp3 = tl.load(in_ptr1 + (x2), xmask)
        tmp2 = tmp0 + tmp1
        tmp4 = tmp2 + tmp3
        tl.store(in_out_ptr0 + (x2), tmp4, xmask)


buf258: SchedulerNode(ComputedBuffer)
buf258.writes = [MemoryDep('buf258', c0, {c0: 197}, None)]
buf258.unmet_dependencies = [MemoryDep('buf257', c0, {c0: 151296}, None)]
buf258.met_dependencies = []
buf258.users = [NodeUser(node=SchedulerNode(name='buf261'), can_inplace=False, is_weak=False)]
buf258.group.device = cuda:0
buf258.group.iteration = (197, 768)
buf258.sizes = ([197], [768])
buf257_layout = FixedLayout('cuda', torch.float32, size=[1, 197, 768], stride=[151296, 768, 1])
buf258_layout = FixedLayout('cuda', torch.float32, size=[1, 197, 1], stride=[197, 1, 197])
class buf258_loop_body:
    var_ranges = {z0: 197, z1: 768}
    index0 = 768*z0 + z1
    index1 = z0
    def body(self, ops):
        get_index = self.get_index('index0')
        load = ops.load('buf257', get_index)
        reduction = ops.reduction(torch.float32, torch.float32, 'welford_reduce', load)
        getitem = reduction[0]
        getitem_1 = reduction[1]
        getitem_2 = reduction[2]
        get_index_1 = self.get_index('index1')
        store_reduction = ops.store_reduction('buf258', get_index_1, getitem)
        return store_reduction
buf258 Triton code:
    import triton
    import triton.language as tl
    from triton.compiler.compiler import AttrsDescriptor

    from torch._inductor.runtime import triton_helpers, triton_heuristics
    from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
    from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, instance_descriptor, DeviceProperties

    @triton_heuristics.persistent_reduction(
        size_hints=[256, 1024],
        reduction_hint=ReductionHint.INNER,
        filename=__file__,
        triton_meta={'signature': {0: '*fp32', 1: '*fp32', 2: 'i32', 3: 'i32'}, 'device': DeviceProperties(type='cuda', index=0, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, multi_processor_count=132), 'constants': {}, 'configs': [AttrsDescriptor(divisible_by_16=(0, 1, 3), equal_to_1=())]},
        inductor_meta={'autotune_hints': set(), 'kernel_name': 'Placeholder.DESCRIPTIVE_NAME', 'mutated_arg_names': [], 'no_x_dim': True, 'num_load': 1, 'num_reduction': 3, 'backend_hash': '3944FAAD9D18E987535CF029C4441E186640DD453AE5AE6831EDF736C09E0A47', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': False, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False}
    )
    @triton.jit
    def triton_(in_ptr0, out_ptr0, xnumel, rnumel):
        xnumel = 197
        XBLOCK: tl.constexpr = 1
        rnumel = 768
        RBLOCK: tl.constexpr = 1024
        xoffset = tl.program_id(0) * XBLOCK
        xindex = tl.full([1], xoffset, tl.int32)
        xmask = xindex < xnumel
        rindex = tl.arange(0, RBLOCK)[:]
        roffset = 0
        rmask = rindex < rnumel
        r1 = rindex
        x0 = xindex
        tmp0 = tl.load(in_ptr0 + (r1 + (768*x0)), rmask & xmask, other=0.0)
        tmp1 = tl.broadcast_to(tmp0, [RBLOCK])
        tmp3 = tl.where(rmask & xmask, tmp1, 0)
        tmp4 = tl.broadcast_to(tmp1, [RBLOCK])
        tmp6 = tl.where(rmask & xmask, tmp4, 0)
        tmp7 = triton_helpers.promote_to_tensor(tl.sum(tmp6, 0))
        tmp8 = tl.full([1], 768, tl.int32)
        tmp9 = tmp8.to(tl.float32)
        tmp10 = tmp7 / tmp9
        tmp11 = tmp1 - tmp10
        tmp12 = tmp11 * tmp11
        tmp13 = tl.broadcast_to(tmp12, [RBLOCK])
        tmp15 = tl.where(rmask & xmask, tmp13, 0)
        tmp16 = triton_helpers.promote_to_tensor(tl.sum(tmp15, 0))
        tl.store(out_ptr0 + (x0), tmp10, xmask)


buf259: SchedulerNode(ComputedBuffer)
buf259.writes = [MemoryDep('buf259', c0, {c0: 197}, None)]
buf259.unmet_dependencies = [MemoryDep('buf257', c0, {c0: 151296}, None)]
buf259.met_dependencies = []
buf259.users = [NodeUser(node=SchedulerNode(name='buf261'), can_inplace=False, is_weak=False)]
buf259.group.device = cuda:0
buf259.group.iteration = (197, 768)
buf259.sizes = ([197], [768])
buf257_layout = FixedLayout('cuda', torch.float32, size=[1, 197, 768], stride=[151296, 768, 1])
buf259_layout = FixedLayout('cuda', torch.float32, size=[1, 197, 1], stride=[197, 1, 197])
class buf259_loop_body:
    var_ranges = {z0: 197, z1: 768}
    index0 = 768*z0 + z1
    index1 = z0
    def body(self, ops):
        get_index = self.get_index('index0')
        load = ops.load('buf257', get_index)
        reduction = ops.reduction(torch.float32, torch.float32, 'welford_reduce', load)
        getitem = reduction[0]
        getitem_1 = reduction[1]
        getitem_2 = reduction[2]
        get_index_1 = self.get_index('index1')
        store_reduction = ops.store_reduction('buf259', get_index_1, getitem_1)
        return store_reduction
buf259 Triton code:
    import triton
    import triton.language as tl
    from triton.compiler.compiler import AttrsDescriptor

    from torch._inductor.runtime import triton_helpers, triton_heuristics
    from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
    from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, instance_descriptor, DeviceProperties

    @triton_heuristics.persistent_reduction(
        size_hints=[256, 1024],
        reduction_hint=ReductionHint.INNER,
        filename=__file__,
        triton_meta={'signature': {0: '*fp32', 1: '*fp32', 2: 'i32', 3: 'i32'}, 'device': DeviceProperties(type='cuda', index=0, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, multi_processor_count=132), 'constants': {}, 'configs': [AttrsDescriptor(divisible_by_16=(0, 1, 3), equal_to_1=())]},
        inductor_meta={'autotune_hints': set(), 'kernel_name': 'Placeholder.DESCRIPTIVE_NAME', 'mutated_arg_names': [], 'no_x_dim': True, 'num_load': 1, 'num_reduction': 3, 'backend_hash': '3944FAAD9D18E987535CF029C4441E186640DD453AE5AE6831EDF736C09E0A47', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': False, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False}
    )
    @triton.jit
    def triton_(in_ptr0, out_ptr0, xnumel, rnumel):
        xnumel = 197
        XBLOCK: tl.constexpr = 1
        rnumel = 768
        RBLOCK: tl.constexpr = 1024
        xoffset = tl.program_id(0) * XBLOCK
        xindex = tl.full([1], xoffset, tl.int32)
        xmask = xindex < xnumel
        rindex = tl.arange(0, RBLOCK)[:]
        roffset = 0
        rmask = rindex < rnumel
        r1 = rindex
        x0 = xindex
        tmp0 = tl.load(in_ptr0 + (r1 + (768*x0)), rmask & xmask, other=0.0)
        tmp1 = tl.broadcast_to(tmp0, [RBLOCK])
        tmp3 = tl.where(rmask & xmask, tmp1, 0)
        tmp4 = tl.broadcast_to(tmp1, [RBLOCK])
        tmp6 = tl.where(rmask & xmask, tmp4, 0)
        tmp7 = triton_helpers.promote_to_tensor(tl.sum(tmp6, 0))
        tmp8 = tl.full([1], 768, tl.int32)
        tmp9 = tmp8.to(tl.float32)
        tmp10 = tmp7 / tmp9
        tmp11 = tmp1 - tmp10
        tmp12 = tmp11 * tmp11
        tmp13 = tl.broadcast_to(tmp12, [RBLOCK])
        tmp15 = tl.where(rmask & xmask, tmp13, 0)
        tmp16 = triton_helpers.promote_to_tensor(tl.sum(tmp15, 0))
        tl.store(out_ptr0 + (x0), tmp16, xmask)


buf261: SchedulerNode(ComputedBuffer)
buf261.writes = [MemoryDep('buf261', c0, {c0: 151296}, None)]
buf261.unmet_dependencies = 
    [   MemoryDep('buf257', c0, {c0: 151296}, None),
        MemoryDep('buf258', c0, {c0: 197}, None),
        MemoryDep('buf259', c0, {c0: 197}, None)]
buf261.met_dependencies = 
    [   MemoryDep('arg196_1', c1, {c0: 197, c1: 768}, None),
        MemoryDep('arg197_1', c1, {c0: 197, c1: 768}, None)]
buf261.users = [NodeUser(node=ExternKernelSchedulerNode(name='buf262'), can_inplace=False, is_weak=False)]
buf261.group.device = cuda:0
buf261.group.iteration = (151296, 1)
buf261.sizes = ([197, 768], [])
arg196_1_layout = FixedLayout('cuda', torch.float32, size=[768], stride=[1])
buf257_layout = FixedLayout('cuda', torch.float32, size=[1, 197, 768], stride=[151296, 768, 1])
buf258_layout = FixedLayout('cuda', torch.float32, size=[1, 197, 1], stride=[197, 1, 197])
arg197_1_layout = FixedLayout('cuda', torch.float32, size=[768], stride=[1])
buf259_layout = FixedLayout('cuda', torch.float32, size=[1, 197, 1], stride=[197, 1, 197])
buf261_layout = FixedLayout('cuda', torch.float32, size=[1, 197, 768], stride=[151296, 768, 1])
class buf261_loop_body:
    var_ranges = {z0: 197, z1: 768}
    index0 = 768*z0 + z1
    index1 = z0
    index2 = z1
    def body(self, ops):
        get_index = self.get_index('index0')
        load = ops.load('buf257', get_index)
        get_index_1 = self.get_index('index1')
        load_1 = ops.load('buf258', get_index_1)
        sub = ops.sub(load, load_1)
        get_index_2 = self.get_index('index1')
        load_2 = ops.load('buf259', get_index_2)
        constant = ops.constant(768.0, torch.float32)
        truediv = ops.truediv(load_2, constant)
        constant_1 = ops.constant(1e-12, torch.float32)
        add = ops.add(truediv, constant_1)
        rsqrt = ops.rsqrt(add)
        mul = ops.mul(sub, rsqrt)
        get_index_3 = self.get_index('index2')
        load_3 = ops.load('arg196_1', get_index_3)
        mul_1 = ops.mul(mul, load_3)
        get_index_4 = self.get_index('index2')
        load_4 = ops.load('arg197_1', get_index_4)
        add_1 = ops.add(mul_1, load_4)
        get_index_5 = self.get_index('index0')
        store = ops.store('buf261', get_index_5, add_1, None)
        return store
buf261 Triton code:
    import triton
    import triton.language as tl
    from triton.compiler.compiler import AttrsDescriptor

    from torch._inductor.runtime import triton_helpers, triton_heuristics
    from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
    from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, instance_descriptor, DeviceProperties

    @triton_heuristics.pointwise(
        size_hints=[262144], 
        filename=__file__,
        triton_meta={'signature': {0: '*fp32', 1: '*fp32', 2: '*fp32', 3: '*fp32', 4: '*fp32', 5: '*fp32', 6: 'i32'}, 'device': DeviceProperties(type='cuda', index=0, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, multi_processor_count=132), 'constants': {}, 'configs': [AttrsDescriptor(divisible_by_16=(0, 1, 2, 3, 4, 5, 6), equal_to_1=())]},
        inductor_meta={'autotune_hints': set(), 'kernel_name': 'Placeholder.DESCRIPTIVE_NAME', 'mutated_arg_names': [], 'no_x_dim': False, 'num_load': 5, 'num_reduction': 0, 'backend_hash': '3944FAAD9D18E987535CF029C4441E186640DD453AE5AE6831EDF736C09E0A47', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': False, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False},
        min_elem_per_thread=0
    )
    @triton.jit
    def triton_(in_ptr0, in_ptr1, in_ptr2, in_ptr3, in_ptr4, out_ptr0, xnumel, XBLOCK : tl.constexpr):
        xnumel = 151296
        xoffset = tl.program_id(0) * XBLOCK
        xindex = xoffset + tl.arange(0, XBLOCK)[:]
        xmask = xindex < xnumel
        x2 = xindex
        x1 = (xindex // 768)
        x0 = xindex % 768
        tmp0 = tl.load(in_ptr0 + (x2), xmask)
        tmp1 = tl.load(in_ptr1 + (x1), xmask, eviction_policy='evict_last')
        tmp3 = tl.load(in_ptr2 + (x1), xmask, eviction_policy='evict_last')
        tmp10 = tl.load(in_ptr3 + (x0), xmask, eviction_policy='evict_last')
        tmp12 = tl.load(in_ptr4 + (x0), xmask, eviction_policy='evict_last')
        tmp2 = tmp0 - tmp1
        tmp4 = 768.0
        tmp5 = tmp3 / tmp4
        tmp6 = 1e-12
        tmp7 = tmp5 + tmp6
        tmp8 = libdevice.rsqrt(tmp7)
        tmp9 = tmp2 * tmp8
        tmp11 = tmp9 * tmp10
        tmp13 = tmp11 + tmp12
        tl.store(out_ptr0 + (x2), tmp13, xmask)


buf262: ExternKernelSchedulerNode(ExternKernelOut)
buf262.writes = [StarDep(name='buf262', mode=None)]
buf262.unmet_dependencies = [StarDep(name='buf261', mode=None)]
buf262.met_dependencies = [StarDep(name='arg198_1', mode=None), StarDep(name='arg199_1', mode=None)]
buf262.users = [NodeUser(node=OUTPUT, can_inplace=False, is_weak=False)]
buf262.node.kernel = extern_kernels.addmm


