_wandb:
    value:
        cli_version: 0.18.7
        m:
            - "1": global_step
              "6":
                - 3
              "7": []
            - "1": peak_memory_alloc
              "5": 1
              "6":
                - 1
                - 3
              "7": []
            - "1": loss
              "5": 1
              "6":
                - 1
                - 3
              "7": []
            - "1": tokens_per_second_per_gpu
              "5": 1
              "6":
                - 1
                - 3
              "7": []
            - "1": peak_memory_active
              "5": 1
              "6":
                - 1
                - 3
              "7": []
            - "1": lr
              "5": 1
              "6":
                - 1
                - 3
              "7": []
            - "1": peak_memory_reserved
              "5": 1
              "6":
                - 1
                - 3
              "7": []
        python_version: 3.11.10
        t:
            "1":
                - 1
                - 49
                - 51
                - 55
                - 105
            "2":
                - 1
                - 49
                - 51
                - 55
                - 105
            "3":
                - 2
                - 3
                - 7
                - 23
                - 55
                - 66
            "4": 3.11.10
            "5": 0.18.7
            "8":
                - 5
            "9":
                "2": torchtune
            "12": 0.18.7
            "13": linux-x86_64
batch_size:
    value: 2
checkpointer:
    value:
        _component_: torchtune.training.FullModelHFCheckpointer
        checkpoint_dir: //tmp/Llama-3.1-70B
        checkpoint_files:
            - model-00001-of-00030.safetensors
            - model-00002-of-00030.safetensors
            - model-00003-of-00030.safetensors
            - model-00004-of-00030.safetensors
            - model-00005-of-00030.safetensors
            - model-00006-of-00030.safetensors
            - model-00007-of-00030.safetensors
            - model-00008-of-00030.safetensors
            - model-00009-of-00030.safetensors
            - model-00010-of-00030.safetensors
            - model-00011-of-00030.safetensors
            - model-00012-of-00030.safetensors
            - model-00013-of-00030.safetensors
            - model-00014-of-00030.safetensors
            - model-00015-of-00030.safetensors
            - model-00016-of-00030.safetensors
            - model-00017-of-00030.safetensors
            - model-00018-of-00030.safetensors
            - model-00019-of-00030.safetensors
            - model-00020-of-00030.safetensors
            - model-00021-of-00030.safetensors
            - model-00022-of-00030.safetensors
            - model-00023-of-00030.safetensors
            - model-00024-of-00030.safetensors
            - model-00025-of-00030.safetensors
            - model-00026-of-00030.safetensors
            - model-00027-of-00030.safetensors
            - model-00028-of-00030.safetensors
            - model-00029-of-00030.safetensors
            - model-00030-of-00030.safetensors
        model_type: LLAMA3
        output_dir: /tmp/Meta-Llama-3-70b
        recipe_checkpoint: null
compile:
    value: false
custom_sharded_layers:
    value:
        - tok_embeddings
        - output
dataset:
    value:
        _component_: torchtune.datasets.text_completion_dataset
        column: input
        data_files: datasets/triton_functions.json
        source: json
        split: train
device:
    value: cuda
dtype:
    value: bf16
enable_activation_checkpointing:
    value: true
enable_activation_offloading:
    value: false
epochs:
    value: 2
fsdp_cpu_offload:
    value: true
gradient_accumulation_steps:
    value: 1
log_every_n_steps:
    value: 1
log_peak_memory_stats:
    value: true
loss:
    value:
        _component_: torchtune.modules.loss.CEWithChunkedOutputLoss
max_steps_per_epoch:
    value: null
metric_logger:
    value:
        _component_: torchtune.training.metric_logging.WandBLogger
        project: torchtune-llama3.1-70b-finetune
model:
    value:
        _component_: torchtune.models.llama3.llama3_70b
optimizer:
    value:
        _component_: torch.optim.AdamW
        fused: true
        lr: 2e-05
optimizer_in_bwd:
    value: false
output_dir:
    value: /tmp/full-llama3.1-finetune
profiler:
    value:
        _component_: torchtune.training.setup_torch_profiler
        active_steps: 2
        cpu: true
        cuda: true
        enabled: false
        num_cycles: 1
        output_dir: /tmp/full-llama3.1-finetune/profiling_outputs
        profile_memory: false
        record_shapes: true
        wait_steps: 5
        warmup_steps: 3
        with_flops: false
        with_stack: false
resume_from_checkpoint:
    value: false
seed:
    value: null
shuffle:
    value: true
tokenizer:
    value:
        _component_: torchtune.models.llama3.llama3_tokenizer
        max_seq_len: null
        path: //tmp/Llama-3.1-70B/original/tokenizer.model
