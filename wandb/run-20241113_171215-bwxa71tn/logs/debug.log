2024-11-13 17:12:15,611 INFO    MainThread:3952175 [wandb_setup.py:_flush():79] Current SDK version is 0.18.7
2024-11-13 17:12:15,611 INFO    MainThread:3952175 [wandb_setup.py:_flush():79] Configure stats pid to 3952175
2024-11-13 17:12:15,611 INFO    MainThread:3952175 [wandb_setup.py:_flush():79] Loading settings from /home/sahanp/.config/wandb/settings
2024-11-13 17:12:15,611 INFO    MainThread:3952175 [wandb_setup.py:_flush():79] Loading settings from /home/sahanp/triton-data/wandb/settings
2024-11-13 17:12:15,611 INFO    MainThread:3952175 [wandb_setup.py:_flush():79] Loading settings from environment variables: {}
2024-11-13 17:12:15,611 INFO    MainThread:3952175 [wandb_setup.py:_flush():79] Applying setup settings: {'mode': None, '_disable_service': None}
2024-11-13 17:12:15,611 WARNING MainThread:3952175 [wandb_setup.py:_flush():79] Could not save program above cwd: /home/sahanp/.conda/envs/parity-bench/lib/python3.11/site-packages/recipes/full_finetune_distributed.py
2024-11-13 17:12:15,611 INFO    MainThread:3952175 [wandb_setup.py:_flush():79] Inferring run settings from compute environment: {'program_relpath': None, 'program_abspath': '/home/sahanp/.conda/envs/parity-bench/lib/python3.11/site-packages/recipes/full_finetune_distributed.py', 'program': '/home/sahanp/.conda/envs/parity-bench/lib/python3.11/site-packages/recipes/full_finetune_distributed.py'}
2024-11-13 17:12:15,611 INFO    MainThread:3952175 [wandb_setup.py:_flush():79] Applying login settings: {}
2024-11-13 17:12:15,612 INFO    MainThread:3952175 [wandb_init.py:_log_setup():533] Logging user logs to /home/sahanp/triton-data/wandb/run-20241113_171215-bwxa71tn/logs/debug.log
2024-11-13 17:12:15,612 INFO    MainThread:3952175 [wandb_init.py:_log_setup():534] Logging internal logs to /home/sahanp/triton-data/wandb/run-20241113_171215-bwxa71tn/logs/debug-internal.log
2024-11-13 17:12:15,612 INFO    MainThread:3952175 [wandb_init.py:init():619] calling init triggers
2024-11-13 17:12:15,612 INFO    MainThread:3952175 [wandb_init.py:init():626] wandb.init called with sweep_config: {}
config: {}
2024-11-13 17:12:15,612 INFO    MainThread:3952175 [wandb_init.py:init():669] starting backend
2024-11-13 17:12:15,612 INFO    MainThread:3952175 [wandb_init.py:init():673] sending inform_init request
2024-11-13 17:12:15,612 INFO    MainThread:3952175 [backend.py:_multiprocessing_setup():104] multiprocessing start_methods=fork,spawn,forkserver, using: spawn
2024-11-13 17:12:15,613 INFO    MainThread:3952175 [wandb_init.py:init():686] backend started and connected
2024-11-13 17:12:15,614 INFO    MainThread:3952175 [wandb_init.py:init():781] updated telemetry
2024-11-13 17:12:15,627 INFO    MainThread:3952175 [wandb_init.py:init():814] communicating run to backend with 90.0 second timeout
2024-11-13 17:12:16,536 INFO    MainThread:3952175 [wandb_init.py:init():867] starting run threads in backend
2024-11-13 17:12:16,611 INFO    MainThread:3952175 [wandb_run.py:_console_start():2456] atexit reg
2024-11-13 17:12:16,611 INFO    MainThread:3952175 [wandb_run.py:_redirect():2305] redirect: wrap_raw
2024-11-13 17:12:16,611 INFO    MainThread:3952175 [wandb_run.py:_redirect():2370] Wrapping output streams.
2024-11-13 17:12:16,611 INFO    MainThread:3952175 [wandb_run.py:_redirect():2395] Redirects installed.
2024-11-13 17:12:16,612 INFO    MainThread:3952175 [wandb_init.py:init():911] run started, returning control to user process
2024-11-13 17:12:16,614 INFO    MainThread:3952175 [wandb_run.py:_config_callback():1387] config_cb None None {'seed': None, 'shuffle': True, 'tokenizer': {'_component_': 'torchtune.models.llama3.llama3_tokenizer', 'path': '//tmp/Llama-3.1-70B/original/tokenizer.model', 'max_seq_len': None}, 'dataset': {'_component_': 'torchtune.datasets.text_completion_dataset', 'data_files': 'datasets/triton_functions.json', 'source': 'json', 'column': 'input', 'split': 'train'}, 'model': {'_component_': 'torchtune.models.llama3.llama3_70b'}, 'checkpointer': {'_component_': 'torchtune.training.FullModelHFCheckpointer', 'checkpoint_dir': '//tmp/Llama-3.1-70B', 'checkpoint_files': ['model-00001-of-00030.safetensors', 'model-00002-of-00030.safetensors', 'model-00003-of-00030.safetensors', 'model-00004-of-00030.safetensors', 'model-00005-of-00030.safetensors', 'model-00006-of-00030.safetensors', 'model-00007-of-00030.safetensors', 'model-00008-of-00030.safetensors', 'model-00009-of-00030.safetensors', 'model-00010-of-00030.safetensors', 'model-00011-of-00030.safetensors', 'model-00012-of-00030.safetensors', 'model-00013-of-00030.safetensors', 'model-00014-of-00030.safetensors', 'model-00015-of-00030.safetensors', 'model-00016-of-00030.safetensors', 'model-00017-of-00030.safetensors', 'model-00018-of-00030.safetensors', 'model-00019-of-00030.safetensors', 'model-00020-of-00030.safetensors', 'model-00021-of-00030.safetensors', 'model-00022-of-00030.safetensors', 'model-00023-of-00030.safetensors', 'model-00024-of-00030.safetensors', 'model-00025-of-00030.safetensors', 'model-00026-of-00030.safetensors', 'model-00027-of-00030.safetensors', 'model-00028-of-00030.safetensors', 'model-00029-of-00030.safetensors', 'model-00030-of-00030.safetensors'], 'recipe_checkpoint': None, 'output_dir': '/tmp/Meta-Llama-3-70b', 'model_type': 'LLAMA3'}, 'resume_from_checkpoint': False, 'batch_size': 2, 'epochs': 2, 'optimizer': {'_component_': 'torch.optim.AdamW', 'lr': 2e-05, 'fused': True}, 'loss': {'_component_': 'torchtune.modules.loss.CEWithChunkedOutputLoss'}, 'max_steps_per_epoch': None, 'gradient_accumulation_steps': 1, 'device': 'cuda', 'enable_activation_checkpointing': True, 'enable_activation_offloading': False, 'custom_sharded_layers': ['tok_embeddings', 'output'], 'fsdp_cpu_offload': True, 'compile': False, 'optimizer_in_bwd': False, 'dtype': 'bf16', 'metric_logger': {'_component_': 'torchtune.training.metric_logging.WandBLogger', 'project': 'torchtune-llama3.1-70b-finetune'}, 'output_dir': '/tmp/full-llama3.1-finetune', 'log_every_n_steps': 1, 'log_peak_memory_stats': True, 'profiler': {'_component_': 'torchtune.training.setup_torch_profiler', 'enabled': False, 'output_dir': '/tmp/full-llama3.1-finetune/profiling_outputs', 'cpu': True, 'cuda': True, 'profile_memory': False, 'with_stack': False, 'record_shapes': True, 'with_flops': False, 'wait_steps': 5, 'warmup_steps': 3, 'active_steps': 2, 'num_cycles': 1}}
2024-11-13 19:04:27,591 INFO    MainThread:3952175 [wandb_run.py:_finish():2155] finishing run sahancpal-meta/torchtune-llama3.1-70b-finetune/bwxa71tn
2024-11-13 19:04:27,593 INFO    MainThread:3952175 [wandb_run.py:_atexit_cleanup():2420] got exitcode: 0
2024-11-13 19:04:27,593 INFO    MainThread:3952175 [wandb_run.py:_restore():2402] restore
2024-11-13 19:04:27,594 INFO    MainThread:3952175 [wandb_run.py:_restore():2408] restore done
2024-11-13 19:04:29,511 INFO    MainThread:3952175 [wandb_run.py:_footer_history_summary_info():3960] rendering history
2024-11-13 19:04:29,511 INFO    MainThread:3952175 [wandb_run.py:_footer_history_summary_info():3992] rendering summary
2024-11-13 19:04:29,514 INFO    MainThread:3952175 [wandb_run.py:_footer_sync_info():3921] logging synced files
