INFO:torchtune.utils._logging:Logging /tmp/Llama-3.2-3B/torchtune_config.yaml to W&B under Files
INFO:torchtune.utils._logging:Model is initialized with precision torch.bfloat16.
INFO:torchtune.utils._logging:Memory stats after model init:
	GPU peak memory allocation: 6.05 GiB
	GPU peak memory reserved: 6.09 GiB
	GPU peak memory active: 6.05 GiB
INFO:torchtune.utils._logging:Tokenizer is initialized from file.
INFO:torchtune.utils._logging:In-backward optimizers are set up.
INFO:torchtune.utils._logging:Loss is initialized.
Generating train split: 3153 examples [00:00, 80401.75 examples/s]
INFO:torchtune.utils._logging:Dataset and Sampler are initialized.
WARNING:torchtune.utils._logging: Profiling disabled.
INFO:torchtune.utils._logging: Profiler config after instantiation: {'enabled': False}
1|788|Loss: 0.035208605229854584: 100%|█████████████████████████████| 788/788 [07:48<00:00,  1.70it/s]INFO:torchtune.utils._logging:Model checkpoint of size 4.97 GB saved to /tmp/Llama-3.2-3B/hf_model_0001_0.pt
INFO:torchtune.utils._logging:Model checkpoint of size 1.46 GB saved to /tmp/Llama-3.2-3B/hf_model_0002_0.pt
INFO:torchtune.utils._logging:Saving final epoch checkpoint.
INFO:torchtune.utils._logging:The full model checkpoint, including all weights and configurations, has been saved successfully.You can now use this checkpoint for further training or inference.
1|788|Loss: 0.035208605229854584: 100%|█████████████████████████████| 788/788 [07:56<00:00,  1.65it/s]
