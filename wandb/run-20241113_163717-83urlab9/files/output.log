INFO:torchtune.utils._logging:Logging /tmp/Llama-3.2-3B/torchtune_config.yaml to W&B under Files
INFO:torchtune.utils._logging:Model is initialized with precision torch.bfloat16.
INFO:torchtune.utils._logging:Memory stats after model init:
	GPU peak memory allocation: 6.05 GiB
	GPU peak memory reserved: 6.09 GiB
	GPU peak memory active: 6.05 GiB
INFO:torchtune.utils._logging:Tokenizer is initialized from file.
INFO:torchtune.utils._logging:In-backward optimizers are set up.
INFO:torchtune.utils._logging:Loss is initialized.
INFO:torchtune.utils._logging:Dataset and Sampler are initialized.
WARNING:torchtune.utils._logging: Profiling disabled.
INFO:torchtune.utils._logging: Profiler config after instantiation: {'enabled': False}
1|384|Loss: 0.15410691499710083:  49%|██████████████▌               | 384/788 [03:51<03:46,  1.78it/s]Traceback (most recent call last):
  File "/home/sahanp/.conda/envs/parity-bench/bin/tune", line 8, in <module>
    sys.exit(main())
             ^^^^^^
  File "/home/sahanp/.conda/envs/parity-bench/lib/python3.11/site-packages/torchtune/_cli/tune.py", line 49, in main
    parser.run(args)
  File "/home/sahanp/.conda/envs/parity-bench/lib/python3.11/site-packages/torchtune/_cli/tune.py", line 43, in run
    args.func(args)
  File "/home/sahanp/.conda/envs/parity-bench/lib/python3.11/site-packages/torchtune/_cli/run.py", line 196, in _run_cmd
    self._run_single_device(args, is_builtin=is_builtin)
  File "/home/sahanp/.conda/envs/parity-bench/lib/python3.11/site-packages/torchtune/_cli/run.py", line 102, in _run_single_device
    runpy.run_path(str(args.recipe), run_name="__main__")
  File "<frozen runpy>", line 291, in run_path
  File "<frozen runpy>", line 98, in _run_module_code
  File "<frozen runpy>", line 88, in _run_code
  File "/home/sahanp/.conda/envs/parity-bench/lib/python3.11/site-packages/recipes/full_finetune_single_device.py", line 669, in <module>
    sys.exit(recipe_main())
             ^^^^^^^^^^^^^
  File "/home/sahanp/.conda/envs/parity-bench/lib/python3.11/site-packages/torchtune/config/_parse.py", line 99, in wrapper
    sys.exit(recipe_main(conf))
             ^^^^^^^^^^^^^^^^^
  File "/home/sahanp/.conda/envs/parity-bench/lib/python3.11/site-packages/recipes/full_finetune_single_device.py", line 664, in recipe_main
    recipe.train()
  File "/home/sahanp/.conda/envs/parity-bench/lib/python3.11/site-packages/recipes/full_finetune_single_device.py", line 576, in train
    loss.backward()
  File "/home/sahanp/.conda/envs/parity-bench/lib/python3.11/site-packages/torch/_tensor.py", line 581, in backward
    torch.autograd.backward(
  File "/home/sahanp/.conda/envs/parity-bench/lib/python3.11/site-packages/torch/autograd/__init__.py", line 347, in backward
    _engine_run_backward(
  File "/home/sahanp/.conda/envs/parity-bench/lib/python3.11/site-packages/torch/autograd/graph.py", line 825, in _engine_run_backward
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
KeyboardInterrupt
